@INPROCEEDINGS{Guo2017-zy,
  title     = "On Calibration of Modern Neural Networks",
  author    = "Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q",
  booktitle = "International Conference on Machine Learning",
  pages     = "1321--1330",
  abstract  = "Confidence calibration – the problem of predicting probability
               estimates representative of the true correctness likelihood – is
               important for classification models in many applications. We
               discover...",
  month     =  jul,
  year      =  2017,
  language  = "en"
}

@ARTICLE{Vaswani2017-ue,
  title    = "Attention Is All You Need",
  author   = "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit,
              Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and
              Polosukhin, Illia",
  journal  = "arXiv:1706.03762 [cs]",
  abstract = "The dominant sequence transduction models are based on complex
              recurrent or convolutional neural networks in an encoder-decoder
              configuration. The best performing models also connect the encoder
              and decoder through an attention mechanism. We propose a new
              simple network architecture, the Transformer, based solely on
              attention mechanisms, dispensing with recurrence and convolutions
              entirely. Experiments on two machine translation tasks show these
              models to be superior in quality while being more parallelizable
              and requiring significantly less time to train. Our model achieves
              28.4 BLEU on the WMT 2014 English-to-German translation task,
              improving over the existing best results, including ensembles by
              over 2 BLEU. On the WMT 2014 English-to-French translation task,
              our model establishes a new single-model state-of-the-art BLEU
              score of 41.8 after training for 3.5 days on eight GPUs, a small
              fraction of the training costs of the best models from the
              literature. We show that the Transformer generalizes well to other
              tasks by applying it successfully to English constituency parsing
              both with large and limited training data.",
  month    =  jun,
  year     =  2017
}

@INPROCEEDINGS{Kim2018-bu,
  title     = "Joint Learning of Domain Classification and Out-of-Domain
               Detection with Dynamic Class Weighting for Satisficing False
               Acceptance Rates",
  author    = "Kim, Joo-Kyung and Kim, Young-Bum",
  booktitle = "arXiv:1807.00072 [cs]",
  abstract  = "In domain classification for spoken dialog systems, correct
               detection of out-of-domain (OOD) utterances is crucial because it
               reduces confusion and unnecessary interaction costs between users
               and the systems. Previous work usually utilizes OOD detectors
               that are trained separately from in-domain (IND) classifiers, and
               confidence thresholding for OOD detection given target evaluation
               scores. In this paper, we introduce a neural joint learning model
               for domain classification and OOD detection, where dynamic class
               weighting is used during the model training to satisfice a given
               OOD false acceptance rate (FAR) while maximizing the domain
               classification accuracy. Evaluating on two domain classification
               tasks for the utterances from a large spoken dialogue system, we
               show that our approach significantly improves the domain
               classification performance with satisficing given target FARs.",
  month     =  jun,
  year      =  2018
}

@INPROCEEDINGS{Kim2018-bm,
  title     = "A Scalable Neural Shortlisting-Reranking Approach for Large-Scale
               Domain Classification in Natural Language Understanding",
  author    = "Kim, Young-Bum and Kim, Dongchan and Kim, Joo-Kyung and Sarikaya,
               Ruhi",
  booktitle = "Proceedings of the 2018 Conference of the North American Chapter
               of the Association for Computational Linguistics: Human Language
               Technologies, Volume 3 (Industry Papers)",
  volume    =  3,
  pages     = "16--24",
  year      =  2018,
  doi       = "10.18653/v1/N18-3003",
  language  = "en"
}

@ARTICLE{Artetxe2017-ju,
  title    = "Unsupervised Neural Machine Translation",
  author   = "Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko and Cho,
              Kyunghyun",
  journal  = "arXiv:1710.11041 [cs]",
  abstract = "In spite of the recent success of neural machine translation (NMT)
              in standard benchmarks, the lack of large parallel corpora poses a
              major practical problem for many language pairs. There have been
              several proposals to alleviate this issue with, for instance,
              triangulation and semi-supervised learning techniques, but they
              still require a strong cross-lingual signal. In this work, we
              completely remove the need of parallel data and propose a novel
              method to train an NMT system in a completely unsupervised manner,
              relying on nothing but monolingual corpora. Our model builds upon
              the recent work on unsupervised embedding mappings, and consists
              of a slightly modified attentional encoder-decoder model that can
              be trained on monolingual corpora alone using a combination of
              denoising and backtranslation. Despite the simplicity of the
              approach, our system obtains 15.56 and 10.21 BLEU points in WMT
              2014 French-to-English and German-to-English translation. The
              model can also profit from small parallel corpora, and attains
              21.81 and 15.24 points when combined with 100,000 parallel
              sentences, respectively. Our implementation is released as an open
              source project.",
  month    =  oct,
  year     =  2017
}

@ARTICLE{Niculae2017-zy,
  title    = "A Regularized Framework for Sparse and Structured Neural Attention",
  author   = "Niculae, Vlad and Blondel, Mathieu",
  journal  = "arXiv:1705.07704 [cs, stat]",
  abstract = "Modern neural networks are often augmented with an attention
              mechanism, which tells the network where to focus within the
              input. We propose in this paper a new framework for sparse and
              structured attention, building upon a smoothed max operator. We
              show that the gradient of this operator defines a mapping from
              real values to probabilities, suitable as an attention mechanism.
              Our framework includes softmax and a slight generalization of the
              recently-proposed sparsemax as special cases. However, we also
              show how our framework can incorporate modern structured
              penalties, resulting in more interpretable attention mechanisms,
              that focus on entire segments or groups of an input. We derive
              efficient algorithms to compute the forward and backward passes of
              our attention mechanisms, enabling their use in a neural network
              trained with backpropagation. To showcase their potential as a
              drop-in replacement for existing ones, we evaluate our attention
              mechanisms on three large-scale tasks: textual entailment, machine
              translation, and sentence summarization. Our attention mechanisms
              improve interpretability without sacrificing performance; notably,
              on textual entailment and summarization, we outperform the
              standard attention mechanisms based on softmax and sparsemax.",
  month    =  may,
  year     =  2017
}

@ARTICLE{Wu2018-fb,
  title    = "Group Normalization",
  author   = "Wu, Yuxin and He, Kaiming",
  journal  = "arXiv:1803.08494 [cs]",
  abstract = "Batch Normalization (BN) is a milestone technique in the
              development of deep learning, enabling various networks to train.
              However, normalizing along the batch dimension introduces problems
              --- BN's error increases rapidly when the batch size becomes
              smaller, caused by inaccurate batch statistics estimation. This
              limits BN's usage for training larger models and transferring
              features to computer vision tasks including detection,
              segmentation, and video, which require small batches constrained
              by memory consumption. In this paper, we present Group
              Normalization (GN) as a simple alternative to BN. GN divides the
              channels into groups and computes within each group the mean and
              variance for normalization. GN's computation is independent of
              batch sizes, and its accuracy is stable in a wide range of batch
              sizes. On ResNet-50 trained in ImageNet, GN has 10.6\% lower error
              than its BN counterpart when using a batch size of 2; when using
              typical batch sizes, GN is comparably good with BN and outperforms
              other normalization variants. Moreover, GN can be naturally
              transferred from pre-training to fine-tuning. GN can outperform
              its BN-based counterparts for object detection and segmentation in
              COCO, and for video classification in Kinetics, showing that GN
              can effectively replace the powerful BN in a variety of tasks. GN
              can be easily implemented by a few lines of code in modern
              libraries.",
  month    =  mar,
  year     =  2018
}

@ARTICLE{Conneau2018-hl,
  title    = "What you can cram into a single vector: Probing sentence
              embeddings for linguistic properties",
  author   = "Conneau, Alexis and Kruszewski, German and Lample, Guillaume and
              Barrault, Loïc and Baroni, Marco",
  journal  = "arXiv:1805.01070 [cs]",
  abstract = "Although much effort has recently been devoted to training
              high-quality sentence embeddings, we still have a poor
              understanding of what they are capturing. ``Downstream'' tasks,
              often based on sentence classification, are commonly used to
              evaluate the quality of sentence representations. The complexity
              of the tasks makes it however difficult to infer what kind of
              information is present in the representations. We introduce here
              10 probing tasks designed to capture simple linguistic features of
              sentences, and we use them to study embeddings generated by three
              different encoders trained in eight distinct ways, uncovering
              intriguing properties of both encoders and training methods.",
  month    =  may,
  year     =  2018
}

@ARTICLE{Choudhury2017-vb,
  title    = "The Language of Social Support in Social Media and its Effect on
              Suicidal Ideation Risk",
  author   = "Choudhury, Munmun De and Kiciman, Emre",
  journal  = "Microsoft Research",
  abstract = "Online social support is known to play a signiﬁcant role in mental
              well-being. However, current research is limited in its ability to
              quantify this link. Challenges exist due to the paucity of
              longitudinal, pre- and post-mental illness risk data, and reliable
              methods that can examine causality between past availability of
              support and future risk. In …",
  month    =  mar,
  year     =  2017,
  language = "en-US"
}

@ARTICLE{Choudhury2013-rp,
  title    = "Predicting Depression via Social Media",
  author   = "Choudhury, Munmun De and Gamon, Michael and Counts, Scott and
              Horvitz, Eric",
  journal  = "Microsoft Research",
  abstract = "Major depression constitutes a serious challenge in personal and
              public health. Tens of millions of people each year suffer from
              depression and only a fraction receives adequate treatment. We
              explore the potential to use social media to detect and diagnose
              major depressive disorder in individuals. We first employ
              crowdsourcing to compile a set of Twitter …",
  month    =  jul,
  year     =  2013,
  language = "en-US"
}

@INPROCEEDINGS{Coppersmith2017-wf,
  title     = "Scalable mental health analysis in the clinical whitespace via
               natural language processing",
  author    = "Coppersmith, G and Hilland, C and Frieder, O and Leary, R",
  booktitle = "2017 IEEE EMBS International Conference on Biomedical Health
               Informatics (BHI)",
  pages     = "393--396",
  abstract  = "Our increasingly digital life provides a wealth of data about our
               behavior, beliefs, mood, and well-being. This data provides some
               insight into the lives of patients outside the healthcare
               setting, and in aggregate can be insightful for the person's
               mental health and emotional crisis. Here, we introduce this
               community to some of the recent advancement in using natural
               language processing and machine learning to provide insight into
               mental health of both individuals and populations. We advocate
               using these linguistic signals as a supplement to those that are
               collected in the health care system, filling in some of the
               so-called “whitespace” between visits.",
  month     =  feb,
  year      =  2017,
  doi       = "10.1109/BHI.2017.7897288"
}

@INPROCEEDINGS{Soldaini2015-va,
  title     = "Retrieving Medical Literature for Clinical Decision Support",
  author    = "Soldaini, Luca and Cohan, Arman and Yates, Andrew and Goharian,
               Nazli and Frieder, Ophir",
  booktitle = "Advances in Information Retrieval",
  publisher = "Springer, Cham",
  pages     = "538--549",
  abstract  = "Keeping current given the vast volume of medical literature
               published yearly poses a serious challenge for medical
               professionals. Thus, interest in systems that aid physicians in
               making clinical decisions is intensifying. A task of Clinical
               Decision Support (CDS) systems is retrieving highly relevant
               medical literature that could help healthcare professionals in
               formulating diagnoses or determining treatments. This search task
               is atypical as the queries are medical case reports, which
               differs in terms of size and structure from queries in other,
               more common search tasks. We apply query reformulation techniques
               to address literature search based on case reports. The proposed
               system achieves a statistically significant improvement over the
               baseline (29\% – 32\%) and the state-of-the-art (12\% – 59\%).",
  series    = "Lecture Notes in Computer Science",
  month     =  mar,
  year      =  2015,
  doi       = "10.1007/978-3-319-16354-3\_59",
  isbn      = "9783319163536,9783319163543",
  language  = "en"
}

@INPROCEEDINGS{Cohan2015-cb,
  title     = "Matching Citation Text and Cited Spans in Biomedical Literature:
               a Search-Oriented Approach",
  author    = "Cohan, Arman and Soldaini, Luca and Goharian, Nazli",
  booktitle = "Proceedings of the 2015 Conference of the North American Chapter
               of the Association for Computational Linguistics: Human Language
               Technologies",
  publisher = "Association for Computational Linguistics",
  address   = "Denver, Colorado",
  pages     = "1042–1048",
  year      =  2015
}

@ARTICLE{Su2018-sc,
  title         = "A Re-ranker Scheme for Integrating Large Scale {NLU} models",
  author        = "Su, Chengwei and Gupta, Rahul and Ananthakrishnan, Shankar
                   and Matsoukas, Spyros",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large scale Natural Language Understanding (NLU) systems are
                   typically trained on large quantities of data, requiring a
                   fast and scalable training strategy. A typical design for NLU
                   systems consists of domain-level NLU modules (domain
                   classifier, intent classifier and named entity recognizer).
                   Hypotheses (NLU interpretations consisting of various
                   intent+slot combinations) from these domain specific modules
                   are typically aggregated with another downstream component.
                   The re-ranker integrates outputs from domain-level
                   recognizers, returning a scored list of cross domain
                   hypotheses. An ideal re-ranker will exhibit the following two
                   properties: (a) it should prefer the most relevant hypothesis
                   for the given input as the top hypothesis and, (b) the
                   interpretation scores corresponding to each hypothesis
                   produced by the re-ranker should be calibrated. Calibration
                   allows the final NLU interpretation score to be comparable
                   across domains. We propose a novel re-ranker strategy that
                   addresses these aspects, while also maintaining domain
                   specific modularity. We design optimization loss functions
                   for such a modularized re-ranker and present results on
                   decreasing the top hypothesis error rate as well as
                   maintaining the model calibration. We also experiment with an
                   extension involving training the domain specific re-rankers
                   on datasets curated independently by each domain to allow
                   further asynchronization. \%The proposed re-ranker design
                   showcases the following: (i) improved NLU performance over an
                   unweighted aggregation strategy, (ii) cross-domain calibrated
                   performance and, (iii) support for use cases involving
                   training each re-ranker on datasets curated by each domain
                   independently.",
  month         =  sep,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1809.09605",
  language      = "en"
}

@INPROCEEDINGS{Duh2008-lm,
  title     = "Learning to Rank with Partially-labeled Data",
  author    = "Duh, Kevin and Kirchhoff, Katrin",
  booktitle = "Proceedings of the 31st Annual International ACM SIGIR Conference
               on Research and Development in Information Retrieval",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "251--258",
  series    = "SIGIR '08",
  year      =  2008,
  keywords  = "boosting, information retrieval, kernel principal components
               analysis, learning to rank, transductive learning",
  doi       = "10.1145/1390334.1390379",
  isbn      =  9781605581644
}

@ARTICLE{Peters2018-be,
  title         = "Deep contextualized word representations",
  author        = "Peters, Matthew E and Neumann, Mark and Iyyer, Mohit and
                   Gardner, Matt and Clark, Christopher and Lee, Kenton and
                   Zettlemoyer, Luke",
  journal       = "arXiv:1802.05365 [cs]",
  abstract      = "We introduce a new type of deep contextualized word
                   representation that models both (1) complex characteristics
                   of word use (e.g., syntax and semantics), and (2) how these
                   uses vary across linguistic contexts (i.e., to model
                   polysemy). Our word vectors are learned functions of the
                   internal states of a deep bidirectional language model
                   (biLM), which is pre-trained on a large text corpus. We show
                   that these representations can be easily added to existing
                   models and significantly improve the state of the art across
                   six challenging NLP problems, including question answering,
                   textual entailment and sentiment analysis. We also present an
                   analysis showing that exposing the deep internals of the
                   pre-trained network is crucial, allowing downstream models to
                   mix different types of semi-supervision signals.",
  month         =  feb,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1802.05365"
}

@ARTICLE{Sennrich2015-qp,
  title         = "Neural Machine Translation of Rare Words with Subword Units",
  author        = "Sennrich, Rico and Haddow, Barry and Birch, Alexandra",
  journal       = "arXiv [cs.CL]",
  abstract      = "Neural machine translation (NMT) models typically operate
                   with a fixed vocabulary, but translation is an
                   open-vocabulary problem. Previous work addresses the
                   translation of out-of-vocabulary words by backing off to a
                   dictionary. In this paper, we introduce a simpler and more
                   effective approach, making the NMT model capable of
                   open-vocabulary translation by encoding rare and unknown
                   words as sequences of subword units. This is based on the
                   intuition that various word classes are translatable via
                   smaller units than words, for instance names (via character
                   copying or transliteration), compounds (via compositional
                   translation), and cognates and loanwords (via phonological
                   and morphological transformations). We discuss the
                   suitability of different word segmentation techniques,
                   including simple character n-gram models and a segmentation
                   based on the byte pair encoding compression algorithm, and
                   empirically show that subword models improve over a back-off
                   dictionary baseline for the WMT 15 translation tasks
                   English-German and English-Russian by 1.1 and 1.3 BLEU,
                   respectively.",
  month         =  aug,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1508.07909"
}

@ARTICLE{Tai2015-yt,
  title         = "Improved Semantic Representations From Tree-Structured Long
                   Short-Term Memory Networks",
  author        = "Tai, Kai Sheng and Socher, Richard and Manning, Christopher D",
  journal       = "arXiv [cs.CL]",
  abstract      = "Because of their superior ability to preserve sequence
                   information over time, Long Short-Term Memory (LSTM)
                   networks, a type of recurrent neural network with a more
                   complex computational unit, have obtained strong results on a
                   variety of sequence modeling tasks. The only underlying LSTM
                   structure that has been explored so far is a linear chain.
                   However, natural language exhibits syntactic properties that
                   would naturally combine words to phrases. We introduce the
                   Tree-LSTM, a generalization of LSTMs to tree-structured
                   network topologies. Tree-LSTMs outperform all existing
                   systems and strong LSTM baselines on two tasks: predicting
                   the semantic relatedness of two sentences (SemEval 2014, Task
                   1) and sentiment classification (Stanford Sentiment
                   Treebank).",
  month         =  feb,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1503.00075"
}

@ARTICLE{Luong2015-np,
  title    = "Effective Approaches to Attention-based Neural Machine Translation",
  author   = "Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D",
  journal  = "arXiv:1508.04025 [cs]",
  abstract = "An attentional mechanism has lately been used to improve neural
              machine translation (NMT) by selectively focusing on parts of the
              source sentence during translation. However, there has been little
              work exploring useful architectures for attention-based NMT. This
              paper examines two simple and effective classes of attentional
              mechanism: a global approach which always attends to all source
              words and a local one that only looks at a subset of source words
              at a time. We demonstrate the effectiveness of both approaches
              over the WMT translation tasks between English and German in both
              directions. With local attention, we achieve a significant gain of
              5.0 BLEU points over non-attentional systems which already
              incorporate known techniques such as dropout. Our ensemble model
              using different attention architectures has established a new
              state-of-the-art result in the WMT'15 English to German
              translation task with 25.9 BLEU points, an improvement of 1.0 BLEU
              points over the existing best system backed by NMT and an n-gram
              reranker.",
  month    =  aug,
  year     =  2015,
  language = "en"
}

@ARTICLE{Bahdanau2014-en,
  title    = "Neural Machine Translation by Jointly Learning to Align and
              Translate",
  author   = "Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua",
  journal  = "arXiv:1409.0473 [cs, stat]",
  abstract = "Neural machine translation is a recently proposed approach to
              machine translation. Unlike the traditional statistical machine
              translation, the neural machine translation aims at building a
              single neural network that can be jointly tuned to maximize the
              translation performance. The models proposed recently for neural
              machine translation often belong to a family of encoder–decoders
              and encode a source sentence into a ﬁxed-length vector from which
              a decoder generates a translation. In this paper, we conjecture
              that the use of a ﬁxed-length vector is a bottleneck in improving
              the performance of this basic encoder–decoder architecture, and
              propose to extend this by allowing a model to automatically
              (soft-)search for parts of a source sentence that are relevant to
              predicting a target word, without having to form these parts as a
              hard segment explicitly. With this new approach, we achieve a
              translation performance comparable to the existing
              state-of-the-art phrase-based system on the task of
              English-to-French translation. Furthermore, qualitative analysis
              reveals that the (soft-)alignments found by the model agree well
              with our intuition.",
  month    =  sep,
  year     =  2014,
  language = "en"
}

@ARTICLE{Conneau2017-fb,
  title         = "Word Translation Without Parallel Data",
  author        = "Conneau, Alexis and Lample, Guillaume and Ranzato,
                   Marc'aurelio and Denoyer, Ludovic and Jégou, Hervé",
  journal       = "arXiv [cs.CL]",
  abstract      = "State-of-the-art methods for learning cross-lingual word
                   embeddings have relied on bilingual dictionaries or parallel
                   corpora. Recent studies showed that the need for parallel
                   data supervision can be alleviated with character-level
                   information. While these methods showed encouraging results,
                   they are not on par with their supervised counterparts and
                   are limited to pairs of languages sharing a common alphabet.
                   In this work, we show that we can build a bilingual
                   dictionary between two languages without using any parallel
                   corpora, by aligning monolingual word embedding spaces in an
                   unsupervised way. Without using any character information,
                   our model even outperforms existing supervised methods on
                   cross-lingual tasks for some language pairs. Our experiments
                   demonstrate that our method works very well also for distant
                   language pairs, like English-Russian or English-Chinese. We
                   finally describe experiments on the English-Esperanto
                   low-resource language pair, on which there only exists a
                   limited amount of parallel data, to show the potential impact
                   of our method in fully unsupervised machine translation. Our
                   code, embeddings and dictionaries are publicly available.",
  month         =  oct,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1710.04087"
}

@INPROCEEDINGS{Metzler2005-ql,
  title     = "A Markov random field model for term dependencies",
  author    = "Metzler, Donald and Croft, W Bruce",
  booktitle = "Proceedings of the 28th annual international ACM SIGIR conference
               on Research and development in information retrieval",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "472--479",
  series    = "SIGIR '05",
  month     =  aug,
  year      =  2005,
  keywords  = "term dependence, phrases, information retrieval, Markov random
               fields",
  doi       = "10.1145/1076034.1076115",
  isbn      =  9781595930347
}

@ARTICLE{Mu2023-ti,
  title         = "Learning to compress prompts with gist tokens",
  author        = "Mu, Jesse and Li, Xiang Lisa and Goodman, Noah",
  journal       = "arXiv [cs.CL]",
  abstract      = "Prompting is the primary way to utilize the multitask
                   capabilities of language models (LMs), but prompts occupy
                   valuable space in the input context window, and repeatedly
                   encoding the same prompt is computationally inefficient.
                   Finetuning and distillation methods allow for specialization
                   of LMs without prompting, but require retraining the model
                   for each task. To avoid this trade-off entirely, we present
                   gisting, which trains an LM to compress prompts into smaller
                   sets of ``gist'' tokens which can be cached and reused for
                   compute efficiency. Gist models can be trained with no
                   additional cost over standard instruction finetuning by
                   simply modifying Transformer attention masks to encourage
                   prompt compression. On decoder (LLaMA-7B) and encoder-decoder
                   (FLAN-T5-XXL) LMs, gisting enables up to 26x compression of
                   prompts, resulting in up to 40\% FLOPs reductions, 4.2\% wall
                   time speedups, and storage savings, all with minimal loss in
                   output quality.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2304.08467"
}

@ARTICLE{Yao2022-kh,
  title         = "{ReAct}: Synergizing Reasoning and Acting in Language Models",
  author        = "Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and
                   Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan",
  journal       = "arXiv [cs.CL]",
  abstract      = "While large language models (LLMs) have demonstrated
                   impressive capabilities across tasks in language
                   understanding and interactive decision making, their
                   abilities for reasoning (e.g. chain-of-thought prompting) and
                   acting (e.g. action plan generation) have primarily been
                   studied as separate topics. In this paper, we explore the use
                   of LLMs to generate both reasoning traces and task-specific
                   actions in an interleaved manner, allowing for greater
                   synergy between the two: reasoning traces help the model
                   induce, track, and update action plans as well as handle
                   exceptions, while actions allow it to interface with external
                   sources, such as knowledge bases or environments, to gather
                   additional information. We apply our approach, named ReAct,
                   to a diverse set of language and decision making tasks and
                   demonstrate its effectiveness over state-of-the-art
                   baselines, as well as improved human interpretability and
                   trustworthiness over methods without reasoning or acting
                   components. Concretely, on question answering (HotpotQA) and
                   fact verification (Fever), ReAct overcomes issues of
                   hallucination and error propagation prevalent in
                   chain-of-thought reasoning by interacting with a simple
                   Wikipedia API, and generates human-like task-solving
                   trajectories that are more interpretable than baselines
                   without reasoning traces. On two interactive decision making
                   benchmarks (ALFWorld and WebShop), ReAct outperforms
                   imitation and reinforcement learning methods by an absolute
                   success rate of 34\% and 10\% respectively, while being
                   prompted with only one or two in-context examples. Project
                   site with code: https://react-lm.github.io",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2210.03629"
}

@ARTICLE{Rosa2022-sv,
  title     = "No parameter left behind: How distillation and model size affect
               zero-shot retrieval",
  author    = "Rosa, Guilherme Moraes and Bonifacio, Luiz and Jeronymo, Vitor
               and Abonizio, Hugo and Fadaee, Marzieh and Lotufo, Roberto and
               Nogueira, Rodrigo",
  journal   = "arXiv.org",
  publisher = "arXiv",
  abstract  = "Recent work has shown that small distilled language models are
               strong competitors to models that are orders of magnitude larger
               and slower in a wide range of information retrieval tasks. This
               has made distilled and dense models, due to latency constraints,
               the go-to choice for deployment in real-world retrieval
               applications. In this work, we question this practice by showing
               that the number of parameters and early query-document
               interaction play a significant role in the generalization ability
               of retrieval models. Our experiments show that increasing model
               size results in marginal gains on in-domain test sets, but much
               larger gains in new domains never seen during fine-tuning.
               Furthermore, we show that rerankers largely outperform dense ones
               of similar size in several tasks. Our largest reranker reaches
               the state of the art in 12 of the 18 datasets of the Benchmark-IR
               (BEIR) and surpasses the previous state of the art by 3 average
               points. Finally, we confirm that in-domain effectiveness is not a
               good indicator of zero-shot effectiveness. Code is available at
               https://github.com/guilhermemr04/scaling-zero-shot-retrieval.git",
  year      =  2022,
  eprint    = "2206.02873",
  doi       = "10.48550/ARXIV.2206.02873",
  language  = "en"
}

@ARTICLE{Nie2015-of,
  title    = "Disease Inference from Health-Related Questions via Sparse Deep
              Learning",
  author   = "Nie, L and Wang, M and Zhang, L and Yan, S and Zhang, B and Chua,
              T",
  journal  = "IEEE transactions on knowledge and data engineering",
  volume   =  27,
  number   =  8,
  pages    = "2107--2119",
  abstract = "Automatic disease inference is of importance to bridge the gap
              between what online health seekers with unusual symptoms need and
              what busy human doctors with biased expertise can offer. However,
              accurately and efficiently inferring diseases is non-trivial,
              especially for community-based health services due to the
              vocabulary gap, incomplete information, correlated medical
              concepts, and limited high quality training samples. In this
              paper, we first report a user study on the information needs of
              health seekers in terms of questions and then select those that
              ask for possible diseases of their manifested symptoms for further
              analytic. We next propose a novel deep learning scheme to infer
              the possible diseases given the questions of health seekers. The
              proposed scheme is comprised of two key components. The first
              globally mines the discriminant medical signatures from raw
              features. The second deems the raw features and their signatures
              as input nodes in one layer and hidden nodes in the subsequent
              layer, respectively. Meanwhile, it learns the inter-relations
              between these two layers via pre-training with pseudo-labeled
              data. Following that, the hidden nodes serve as raw features for
              the more abstract signature mining. With incremental and
              alternative repeating of these two components, our scheme builds a
              sparsely connected deep architecture with three hidden layers.
              Overall, it well fits specific tasks with fine-tuning. Extensive
              experiments on a real-world dataset labeled by online doctors show
              the significant performance gains of our scheme.",
  month    =  aug,
  year     =  2015,
  keywords = "data mining;diseases;health care;inference mechanisms;information
              needs;learning (artificial intelligence);medical information
              systems;pseudolabeled data;abstract signature mining;sparsely
              connected deep architecture;discriminant medical
              signatures;information needs;community-based health
              services;online health seekers;automatic disease inference;sparse
              deep learning;health-related questions;Diseases;Medical diagnostic
              imaging;Cancer;Educational institutions;Training;Community-based
              Health Services;Question Answering;Disease Inference;Deep
              Learning;Community-based health services;question
              answering;disease inference;deep learning",
  doi      = "10.1109/TKDE.2015.2399298",
  issn     = "1041-4347"
}

@INPROCEEDINGS{Soldaini2015-sm,
  title     = "Retrieving medical literature for clinical decision support",
  author    = "Soldaini, Luca and Cohan, Arman and Yates, Andrew and Goharian,
               Nazli and Frieder, Ophir",
  booktitle = "European Conference on Information Retrieval (ECIR)",
  publisher = "Springer",
  pages     = "538–549",
  year      =  2015
}

@ARTICLE{Frenay2014-fj,
  title    = "Classification in the presence of label noise: a survey",
  author   = "Frénay, Benoît and Verleysen, Michel",
  journal  = "IEEE transactions on neural networks and learning systems",
  volume   =  25,
  number   =  5,
  pages    = "845--869",
  abstract = "Label noise is an important issue in classification, with many
              potential negative consequences. For example, the accuracy of
              predictions may decrease, whereas the complexity of inferred
              models and the number of necessary training samples may increase.
              Many works in the literature have been devoted to the study of
              label noise and the development of techniques to deal with label
              noise. However, the field lacks a comprehensive survey on the
              different types of label noise, their consequences and the
              algorithms that consider label noise. This paper proposes to fill
              this gap. First, the definitions and sources of label noise are
              considered and a taxonomy of the types of label noise is proposed.
              Second, the potential consequences of label noise are discussed.
              Third, label noise-robust, label noise cleansing, and label
              noise-tolerant algorithms are reviewed. For each category of
              approaches, a short discussion is proposed to help the
              practitioner to choose the most suitable technique in its own
              particular field of application. Eventually, the design of
              experiments is also discussed, what may interest the researchers
              who would like to test their own algorithms. In this paper, label
              noise consists of mislabeled instances: no additional information
              is assumed to be available like e.g., confidences on labels.",
  month    =  may,
  year     =  2014,
  doi      = "10.1109/TNNLS.2013.2292894",
  pmid     =  24808033,
  issn     = "2162-2388,2162-237X",
  language = "en"
}

@INPROCEEDINGS{Divita2014-sm,
  title     = "Sophia: a expedient {UMLS} concept extraction annotator",
  author    = "Divita, Guy and Zeng, Qing T and Gundlapalli, Adi V and Duvall,
               Scott and Nebeker, Jonathan and Samore, Matthew H",
  booktitle = "AMIA Annual Symposium Proceedings",
  volume    =  2014,
  year      =  2014
}

@INPROCEEDINGS{Pennington2014-dp,
  title     = "Glove: Global vectors for word representation",
  author    = "Pennington, Jeffrey and Socher, Richard and Manning, Christopher",
  booktitle = "Proceedings of the 2014 conference on empirical methods in
               natural language processing (EMNLP)",
  pages     = "1532--1543",
  year      =  2014
}

@INPROCEEDINGS{Roberts2017-kf,
  title     = "Overview of the {TREC} 2016 Clinical Decision Support Track",
  author    = "Roberts, Kirk and Demner-Fushman, Dina and Voorhees, Ellen M and
               Hersh, William R",
  booktitle = "TREC",
  year      =  2017
}

@INPROCEEDINGS{Soldaini2015-lg,
  title     = "Retrieving medical literature for clinical decision support",
  author    = "Soldaini, Luca and Cohan, Arman and Yates, Andrew and Goharian,
               Nazli and Frieder, Ophir",
  booktitle = "ECIR",
  year      =  2015
}

@INPROCEEDINGS{Cohan2014-jv,
  title     = "Towards citation-based summarization of biomedical literature",
  author    = "Cohan, Arman and Soldaini, Luca and Goharian, Nazli",
  booktitle = "Proceedings of the Text Analysis Conference (TAC'14)",
  year      =  2014
}

@ARTICLE{Severyn2015-ai,
  title     = "Learning to rank short text pairs with convolutional deep neural
               networks",
  author    = "Severyn, A and Moschitti, A",
  journal   = "Proceedings of the 38th international ACM SIGIR",
  publisher = "dl.acm.org",
  abstract  = "Learning a similarity function between pairs of objects is at the
               core of learning to rank approaches. In information retrieval
               tasks we typically deal with query-document pairs, in question
               answering--question-answer pairs. However, before learning can
               take place, such …",
  year      =  2015
}

@MISC{UnknownUnknown-as,
  title        = "Common Crawl - Kagi Search",
  abstract     = "Better search results with no ads. Welcome to Kagi (pronounced
                  kah-gee), a paid search engine that gives power back to the
                  user.",
  howpublished = "\url{https://kagi.com/search?q=Common+Crawl}",
  note         = "Accessed: 2023-11-29"
}

@ARTICLE{Min2023-ty,
  title         = "{SILO} language models: Isolating legal risk in a
                   nonparametric datastore",
  author        = "Min, Sewon and Gururangan, Suchin and Wallace, Eric and
                   Hajishirzi, Hannaneh and Smith, Noah A and Zettlemoyer, Luke",
  journal       = "arXiv [cs.CL]",
  abstract      = "The legality of training language models (LMs) on copyrighted
                   or otherwise restricted data is under intense debate.
                   However, as we show, model performance significantly degrades
                   if trained only on low-risk text (e.g., out-of-copyright
                   books or government documents), due to its limited size and
                   domain coverage. We present SILO, a new language model that
                   manages this risk-performance tradeoff during inference. SILO
                   is built by (1) training a parametric LM on Open License
                   Corpus (OLC), a new corpus we curate with 228B tokens of
                   public domain and permissively licensed text and (2)
                   augmenting it with a more general and easily modifiable
                   nonparametric datastore (e.g., containing copyrighted books
                   or news) that is only queried during inference. The datastore
                   allows use of high-risk data without training on it, supports
                   sentence-level data attribution, and enables data producers
                   to opt out from the model by removing content from the store.
                   These capabilities can foster compliance with data-use
                   regulations such as the fair use doctrine in the United
                   States and the GDPR in the European Union. Our experiments
                   show that the parametric LM struggles on domains not covered
                   by OLC. However, access to the datastore greatly improves out
                   of domain performance, closing 90\% of the performance gap
                   with an LM trained on the Pile, a more diverse corpus with
                   mostly high-risk text. We also analyze which nonparametric
                   approach works best, where the remaining errors lie, and how
                   performance scales with datastore size. Our results suggest
                   that it is possible to build high quality language models
                   while mitigating their legal risk.",
  month         =  aug,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2308.04430"
}

@ARTICLE{Ram2023-rl,
  title         = "In-Context retrieval-augmented language models",
  author        = "Ram, Ori and Levine, Yoav and Dalmedigos, Itay and Muhlgay,
                   Dor and Shashua, Amnon and Leyton-Brown, Kevin and Shoham,
                   Yoav",
  journal       = "arXiv [cs.CL]",
  abstract      = "Retrieval-Augmented Language Modeling (RALM) methods, which
                   condition a language model (LM) on relevant documents from a
                   grounding corpus during generation, were shown to
                   significantly improve language modeling performance. In
                   addition, they can mitigate the problem of factually
                   inaccurate text generation and provide natural source
                   attribution mechanism. Existing RALM approaches focus on
                   modifying the LM architecture in order to facilitate the
                   incorporation of external information, significantly
                   complicating deployment. This paper considers a simple
                   alternative, which we dub In-Context RALM: leaving the LM
                   architecture unchanged and prepending grounding documents to
                   the input, without any further training of the LM. We show
                   that In-Context RALM that builds on off-the-shelf general
                   purpose retrievers provides surprisingly large LM gains
                   across model sizes and diverse corpora. We also demonstrate
                   that the document retrieval and ranking mechanism can be
                   specialized to the RALM setting to further boost performance.
                   We conclude that In-Context RALM has considerable potential
                   to increase the prevalence of LM grounding, particularly in
                   settings where a pretrained LM must be used without
                   modification or even via API access.",
  month         =  jan,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2302.00083"
}

@ARTICLE{Shi2023-jl,
  title         = "In-Context Pretraining: Language modeling beyond document
                   boundaries",
  author        = "Shi, Weijia and Min, Sewon and Lomeli, Maria and Zhou,
                   Chunting and Li, Margaret and Lin, Xi Victoria and Smith,
                   Noah A and Zettlemoyer, Luke and Yih, Scott and Lewis, Mike",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large language models (LMs) are currently trained to predict
                   tokens given document prefixes, enabling them to directly
                   perform long-form generation and prompting-style tasks which
                   can be reduced to document completion. Existing pretraining
                   pipelines train LMs by concatenating random sets of short
                   documents to create input contexts but the prior documents
                   provide no signal for predicting the next document. We
                   instead present In-Context Pretraining, a new approach where
                   language models are pretrained on a sequence of related
                   documents, thereby explicitly encouraging them to read and
                   reason across document boundaries. We can do In-Context
                   Pretraining by simply changing the document ordering so that
                   each context contains related documents, and directly
                   applying existing pretraining pipelines. However, this
                   document sorting problem is challenging. There are billions
                   of documents and we would like the sort to maximize
                   contextual similarity for every document without repeating
                   any data. To do this, we introduce approximate algorithms for
                   finding related documents with efficient nearest neighbor
                   search and constructing coherent input contexts with a graph
                   traversal algorithm. Our experiments show In-Context
                   Pretraining offers a simple and scalable approach to
                   significantly enhance LMs'performance: we see notable
                   improvements in tasks that require more complex contextual
                   reasoning, including in-context learning (+8\%), reading
                   comprehension (+15\%), faithfulness to previous contexts
                   (+16\%), long-context reasoning (+5\%), and retrieval
                   augmentation (+9\%).",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2310.10638"
}

@ARTICLE{Gu2023-ds,
  title         = "Pre-training to learn in context",
  author        = "Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie",
  journal       = "arXiv [cs.CL]",
  abstract      = "In-context learning, where pre-trained language models learn
                   to perform tasks from task examples and instructions in their
                   contexts, has attracted much attention in the NLP community.
                   However, the ability of in-context learning is not fully
                   exploited because language models are not explicitly trained
                   to learn in context. To this end, we propose PICL
                   (Pre-training for In-Context Learning), a framework to
                   enhance the language models' in-context learning ability by
                   pre-training the model on a large collection of ``intrinsic
                   tasks'' in the general plain-text corpus using the simple
                   language modeling objective. PICL encourages the model to
                   infer and perform tasks by conditioning on the contexts while
                   maintaining task generalization of pre-trained models. We
                   evaluate the in-context learning performance of the model
                   trained with PICL on seven widely-used text classification
                   datasets and the Super-NaturalInstrctions benchmark, which
                   contains 100+ NLP tasks formulated to text generation. Our
                   experiments show that PICL is more effective and
                   task-generalizable than a range of baselines, outperforming
                   larger language models with nearly 4x parameters. The code is
                   publicly available at https://github.com/thu-coai/PICL.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.09137"
}

@ARTICLE{Liu2023-oq,
  title         = "Lost in the middle: How language models use long contexts",
  author        = "Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape,
                   Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang,
                   Percy",
  journal       = "arXiv [cs.CL]",
  abstract      = "While recent language models have the ability to take long
                   contexts as input, relatively little is known about how well
                   they use longer context. We analyze language model
                   performance on two tasks that require identifying relevant
                   information within their input contexts: multi-document
                   question answering and key-value retrieval. We find that
                   performance is often highest when relevant information occurs
                   at the beginning or end of the input context, and
                   significantly degrades when models must access relevant
                   information in the middle of long contexts. Furthermore,
                   performance substantially decreases as the input context
                   grows longer, even for explicitly long-context models. Our
                   analysis provides a better understanding of how language
                   models use their input context and provides new evaluation
                   protocols for future long-context models.",
  month         =  jul,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2307.03172"
}

@ARTICLE{Borgeaud2021-uv,
  title         = "Improving language models by retrieving from trillions of
                   tokens",
  author        = "Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan
                   and Cai, Trevor and Rutherford, Eliza and Millican, Katie and
                   van den Driessche, George and Lespiau, Jean-Baptiste and
                   Damoc, Bogdan and Clark, Aidan and Casas, Diego de Las and
                   Guy, Aurelia and Menick, Jacob and Ring, Roman and Hennigan,
                   Tom and Huang, Saffron and Maggiore, Loren and Jones, Chris
                   and Cassirer, Albin and Brock, Andy and Paganini, Michela and
                   Irving, Geoffrey and Vinyals, Oriol and Osindero, Simon and
                   Simonyan, Karen and Rae, Jack W and Elsen, Erich and Sifre,
                   Laurent",
  journal       = "arXiv [cs.CL]",
  abstract      = "We enhance auto-regressive language models by conditioning on
                   document chunks retrieved from a large corpus, based on local
                   similarity with preceding tokens. With a $2$ trillion token
                   database, our Retrieval-Enhanced Transformer (RETRO) obtains
                   comparable performance to GPT-3 and Jurassic-1 on the Pile,
                   despite using 25$\times$ fewer parameters. After fine-tuning,
                   RETRO performance translates to downstream
                   knowledge-intensive tasks such as question answering. RETRO
                   combines a frozen Bert retriever, a differentiable encoder
                   and a chunked cross-attention mechanism to predict tokens
                   based on an order of magnitude more data than what is
                   typically consumed during training. We typically train RETRO
                   from scratch, yet can also rapidly RETROfit pre-trained
                   transformers with retrieval and still achieve good
                   performance. Our work opens up new avenues for improving
                   language models through explicit memory at unprecedented
                   scale.",
  month         =  dec,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2112.04426"
}

@ARTICLE{Sun2023-oo,
  title     = "Instruction distillation makes Large Language Models efficient
               zero-shot rankers",
  author    = "Sun, Weiwei and Chen, Zheng and Ma, Xinyu and Yan, Lingyong and
               Wang, Shuaiqiang and Ren, Pengjie and Chen, Zhumin and Yin, Dawei
               and Ren, Zhaochun",
  journal   = "arXiv.org",
  publisher = "arXiv",
  abstract  = "Recent studies have demonstrated the great potential of Large
               Language Models (LLMs) serving as zero-shot relevance rankers.
               The typical approach involves making comparisons between pairs or
               lists of documents. Although effective, these listwise and
               pairwise methods are not efficient and also heavily rely on
               intricate prompt engineering. To tackle this problem, we
               introduce a novel instruction distillation method. The key idea
               is to distill the pairwise ranking ability of open-sourced LLMs
               to a simpler but more efficient pointwise ranking. Specifically,
               given the same LLM, we first rank documents using the effective
               pairwise approach with complex instructions, and then distill the
               teacher predictions to the pointwise approach with simpler
               instructions. Evaluation results on the BEIR, TREC, and ReDial
               datasets demonstrate that instruction distillation can improve
               efficiency by 10 to 100x and also enhance the ranking performance
               of LLMs. Furthermore, our approach surpasses the performance of
               existing supervised methods like monoT5 and is on par with the
               state-of-the-art zero-shot methods. The code to reproduce our
               results is available at www.github.com/sunnweiwei/RankGPT.",
  year      =  2023,
  eprint    = "2311.01555",
  doi       = "10.48550/ARXIV.2311.01555",
  language  = "en"
}

@MISC{UnknownUnknown-ck,
  title        = "Allow installing `provides\_extras` from local wheel file ·
                  Issue \#9694 · pypa/pip",
  booktitle    = "GitHub",
  abstract     = "What's the problem this feature will solve? Would like pip
                  install mypackage-0.1.0-py3-none-any.whl to be able to
                  optionally install the provides\_extras metadata present
                  within the wheel. Describe the solution you'd like Consider a
                  packa...",
  howpublished = "\url{https://github.com/pypa/pip/issues/9694}",
  note         = "Accessed: 2023-11-26"
}

@ARTICLE{Corry2021-pv,
  title   = "The problem of zombie datasets: A framework for deprecating
             datasets",
  author  = "Corry, Frances and Sridharan, H and Luccioni, A and Ananny, Mike
             and Schultz, J and Crawford, Kate",
  journal = "arXiv.org",
  year    =  2021
}

@INCOLLECTION{Alby2022-xx,
  title     = "Analyzing the web: Are top websites lists a good choice for
               research?",
  author    = "Alby, Tom and Jäschke, Robert",
  booktitle = "Linking Theory and Practice of Digital Libraries",
  publisher = "Springer International Publishing",
  address   = "Cham",
  pages     = "11--25",
  series    = "Lecture notes in computer science",
  year      =  2022,
  doi       = "10.1007/978-3-031-16802-4\_2",
  isbn      = "9783031168017,9783031168024",
  issn      = "0302-9743,1611-3349"
}

@MISC{UnknownUnknown-no,
  title       = "presidio: Context aware, pluggable and customizable data
                 protection and de-identification {SDK} for text and images",
  institution = "Github",
  abstract    = "Context aware, pluggable and customizable data protection and
                 de-identification SDK for text and images - GitHub -
                 microsoft/presidio: Context aware, pluggable and customizable
                 data protection and de-identification SDK for text and images",
  language    = "en"
}

@INPROCEEDINGS{Broder2002-ra,
  title     = "On the resemblance and containment of documents",
  author    = "Broder, A Z",
  booktitle = "Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat.
               No.97TB100171)",
  publisher = "IEEE Comput. Soc",
  pages     = "21--29",
  abstract  = "Given two documents A and B we define two mathematical notions:
               their resemblance r(A, B) and their containment c(A, B) that seem
               to capture well the informal notions of ``roughly the same'' and
               ``roughly contained.'' The basic idea is to reduce these issues
               to set intersection problems that can be easily evaluated by a
               process of random sampling that can be done independently for
               each document. Furthermore, the resemblance can be evaluated
               using a fixed size sample for each document. This paper discusses
               the mathematical properties of these measures and the efficient
               implementation of the sampling process using Rabin (1981)
               fingerprints.",
  year      =  2002,
  doi       = "10.1109/sequen.1997.666900",
  isbn      =  9780818681325
}

@MISC{PetersonUnknown-dv,
  title       = "openwebtext: Open clone of {OpenAI}'s unreleased {WebText}
                 dataset scraper. This version uses pushshift.io files instead
                 of the {API} for speed",
  author      = "Peterson, Joshua",
  institution = "Github",
  abstract    = "Open clone of OpenAI's unreleased WebText dataset scraper. This
                 version uses pushshift.io files instead of the API for speed. -
                 GitHub - jcpeterson/openwebtext: Open clone of OpenAI's
                 unreleased WebText dataset scraper. This version uses
                 pushshift.io files instead of the API for speed.",
  language    = "en"
}

@ARTICLE{Radford2019-vq,
  title    = "Language Models are Unsupervised Multitask Learners",
  author   = "Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and
              Amodei, Dario and Sutskever, Ilya",
  abstract = "🏆 SOTA for Language Modelling on enwik8 (Bit per Character (BPC)
              metric)",
  month    =  feb,
  year     =  2019
}

@MISC{Lucy1991-uf,
  title   = "The Ernst \& Young approach to facilities management",
  author  = "Lucy, J de and de Lucy, J",
  journal = "Property Management",
  volume  =  9,
  number  =  1,
  pages   = "24--31",
  year    =  1991,
  doi     = "10.1108/02637479110029793"
}

@ARTICLE{Ushio2022-kq,
  title         = "Generative Language Models for Paragraph-Level Question
                   Generation",
  author        = "Ushio, Asahi and Alva-Manchego, Fernando and
                   Camacho-Collados, Jose",
  journal       = "arXiv [cs.CL]",
  abstract      = "Powerful generative models have led to recent progress in
                   question generation (QG). However, it is difficult to measure
                   advances in QG research since there are no standardized
                   resources that allow a uniform comparison among approaches.
                   In this paper, we introduce QG-Bench, a multilingual and
                   multidomain benchmark for QG that unifies existing question
                   answering datasets by converting them to a standard QG
                   setting. It includes general-purpose datasets such as SQuAD
                   for English, datasets from ten domains and two styles, as
                   well as datasets in eight different languages. Using QG-Bench
                   as a reference, we perform an extensive analysis of the
                   capabilities of language models for the task. First, we
                   propose robust QG baselines based on fine-tuning generative
                   language models. Then, we complement automatic evaluation
                   based on standard metrics with an extensive manual
                   evaluation, which in turn sheds light on the difficulty of
                   evaluating QG models. Finally, we analyse both the domain
                   adaptability of these models as well as the effectiveness of
                   multilingual models in languages other than English. QG-Bench
                   is released along with the fine-tuned models presented in the
                   paper https://github.com/asahi417/lm-question-generation,
                   which are also available as a demo https://autoqg.net/.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2210.03992"
}

@ARTICLE{Krishna2023-do,
  title         = "{USB}: A unified summarization benchmark across tasks and
                   domains",
  author        = "Krishna, Kundan and Gupta, Prakhar and Ramprasad, Sanjana and
                   Wallace, Byron C and Bigham, Jeffrey P and Lipton, Zachary C",
  journal       = "arXiv [cs.CL]",
  abstract      = "An abundance of datasets exist for training and evaluating
                   models on the task of summary generation.However, these
                   datasets are often derived heuristically, and lack sufficient
                   annotations to support research into all aspects of
                   summarization, such as evidence extraction and controllable
                   summarization. We introduce a benchmark comprising 8 tasks
                   that require multi-dimensional understanding of
                   summarization, e.g., surfacing evidence for a summary,
                   assessing its correctness, and gauging its relevance to
                   different topics. We compare various methods on this
                   benchmark and discover that on multiple tasks,
                   moderately-sized fine-tuned models consistently outperform
                   much larger few-shot prompted language models. For factuality
                   related tasks, we also evaluate existing heuristics to create
                   training data and find that training on them performs worse
                   than training on $20\times$ less human-labeled data. Our
                   benchmark consists of data from 6 different domains, allowing
                   us to study cross-domain performance of trained models. We
                   find that for some tasks, the amount of training data matters
                   more than the domain where it comes from, while for other
                   tasks training specifically on data from the target domain,
                   even if limited, is more beneficial. Our work fulfills the
                   need for a well-annotated summarization benchmark with
                   diverse tasks, and provides useful insights about the impact
                   of the quality, size and domain of training data.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.14296"
}

@MISC{McCoy2023-iz,
  title        = "When language models produce text, is the text novel or copied
                  from the training set?For answers, come to our poster today at
                  \#acl2023nlp! Session 1 posters, 11:00 - 12:30 {todayCritics*}
                  are calling the work ``monumental''Link to paper:
                  https://t.co/{s6kDlGA3IH1}/2 pic.twitter.com/{qw1cKUMZvk}",
  author       = "McCoy, Tom",
  booktitle    = "Twitter",
  month        =  jul,
  year         =  2023,
  howpublished = "\url{https://twitter.com/rtommccoy/status/1678378978925506562?s=12\&t=jr8t7woBh6WnXaXmdrUbIw}",
  note         = "Accessed: 2023-7-11",
  language     = "en"
}

@INPROCEEDINGS{Kamalloo2023-qj,
  title     = "Limitations of open-domain question answering benchmarks for
               document-level reasoning",
  author    = "Kamalloo, Ehsan and Clarke, Charles L A and Rafiei, Davood",
  booktitle = "Proceedings of the 46th International ACM SIGIR Conference on
               Research and Development in Information Retrieval",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  jul,
  year      =  2023,
  doi       = "10.1145/3539618.3592011"
}

@ARTICLE{Zhan2021-mp,
  title         = "Optimizing Dense Retrieval model training with hard negatives",
  author        = "Zhan, Jingtao and Mao, Jiaxin and Liu, Yiqun and Guo, Jiafeng
                   and Zhang, Min and Ma, Shaoping",
  journal       = "arXiv [cs.IR]",
  abstract      = "Ranking has always been one of the top concerns in
                   information retrieval researches. For decades, the lexical
                   matching signal has dominated the ad-hoc retrieval process,
                   but solely using this signal in retrieval may cause the
                   vocabulary mismatch problem. In recent years, with the
                   development of representation learning techniques, many
                   researchers turn to Dense Retrieval (DR) models for better
                   ranking performance. Although several existing DR models have
                   already obtained promising results, their performance
                   improvement heavily relies on the sampling of training
                   examples. Many effective sampling strategies are not
                   efficient enough for practical usage, and for most of them,
                   there still lacks theoretical analysis in how and why
                   performance improvement happens. To shed light on these
                   research questions, we theoretically investigate different
                   training strategies for DR models and try to explain why hard
                   negative sampling performs better than random sampling.
                   Through the analysis, we also find that there are many
                   potential risks in static hard negative sampling, which is
                   employed by many existing training methods. Therefore, we
                   propose two training strategies named a Stable Training
                   Algorithm for dense Retrieval (STAR) and a query-side
                   training Algorithm for Directly Optimizing Ranking
                   pErformance (ADORE), respectively. STAR improves the
                   stability of DR training process by introducing random
                   negatives. ADORE replaces the widely-adopted static hard
                   negative sampling method with a dynamic one to directly
                   optimize the ranking performance. Experimental results on two
                   publicly available retrieval benchmark datasets show that
                   either strategy gains significant improvements over existing
                   competitive baselines and a combination of them leads to the
                   best performance.",
  month         =  apr,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "2104.08051"
}

@INPROCEEDINGS{Kandpal2023-rv,
  title     = "Large language models struggle to learn long-tail knowledge",
  author    = "Kandpal, Nikhil and Deng, Haikang and Roberts, Adam and Wallace,
               Eric and Raffel, Colin",
  editor    = "Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and
               Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan",
  booktitle = "Proceedings of the 40th International Conference on Machine
               Learning",
  publisher = "PMLR",
  volume    =  202,
  pages     = "15696--15707",
  abstract  = "The Internet contains a wealth of knowledge—from the birthdays of
               historical figures to tutorials on how to code—all of which may
               be learned by language models. However, while certain pieces of
               information are ubiquitous on the web, others appear extremely
               rarely. In this paper, we study the relationship between the
               knowledge memorized by large language models and the information
               in pre-training datasets scraped from the web. In particular, we
               show that a language model’s ability to answer a fact-based
               question relates to how many documents associated with that
               question were seen during pre-training. We identify these
               relevant documents by entity linking pre-training datasets and
               counting documents that contain the same entities as a given
               question-answer pair. Our results demonstrate strong
               correlational and causal relationships between accuracy and
               relevant document count for numerous question answering datasets
               (e.g., TriviaQA), pre-training corpora (e.g., ROOTS), and model
               sizes (e.g., 176B parameters). Moreover, while larger models are
               better at learning long-tail knowledge, we estimate that today’s
               models must be scaled by many orders of magnitude to reach
               competitive QA performance on questions with little support in
               the pre-training data. Finally, we show that
               retrieval-augmentation can reduce the dependence on relevant
               pre-training information, presenting a promising approach for
               capturing the long-tail.",
  series    = "Proceedings of Machine Learning Research",
  year      =  2023
}

@ARTICLE{Chen2021-rp,
  title         = "Evaluating large language models trained on code",
  author        = "Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming
                   and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and
                   Edwards, Harri and Burda, Yuri and Joseph, Nicholas and
                   Brockman, Greg and Ray, Alex and Puri, Raul and Krueger,
                   Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry,
                   Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott
                   and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and
                   Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and
                   Tillet, Philippe and Such, Felipe Petroski and Cummings, Dave
                   and Plappert, Matthias and Chantzis, Fotios and Barnes,
                   Elizabeth and Herbert-Voss, Ariel and Guss, William Hebgen
                   and Nichol, Alex and Paino, Alex and Tezak, Nikolas and Tang,
                   Jie and Babuschkin, Igor and Balaji, Suchir and Jain,
                   Shantanu and Saunders, William and Hesse, Christopher and
                   Carr, Andrew N and Leike, Jan and Achiam, Josh and Misra,
                   Vedant and Morikawa, Evan and Radford, Alec and Knight,
                   Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie
                   and Welinder, Peter and McGrew, Bob and Amodei, Dario and
                   McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech",
  journal       = "arXiv [cs.LG]",
  abstract      = "We introduce Codex, a GPT language model fine-tuned on
                   publicly available code from GitHub, and study its Python
                   code-writing capabilities. A distinct production version of
                   Codex powers GitHub Copilot. On HumanEval, a new evaluation
                   set we release to measure functional correctness for
                   synthesizing programs from docstrings, our model solves
                   28.8\% of the problems, while GPT-3 solves 0\% and GPT-J
                   solves 11.4\%. Furthermore, we find that repeated sampling
                   from the model is a surprisingly effective strategy for
                   producing working solutions to difficult prompts. Using this
                   method, we solve 70.2\% of our problems with 100 samples per
                   problem. Careful investigation of our model reveals its
                   limitations, including difficulty with docstrings describing
                   long chains of operations and with binding operations to
                   variables. Finally, we discuss the potential broader impacts
                   of deploying powerful code generation technologies, covering
                   safety, security, and economics.",
  month         =  jul,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2107.03374"
}

@ARTICLE{MacAvaney2022-tn,
  title     = "{ABNIRML}: Analyzing the Behavior of neural {IR} models",
  author    = "MacAvaney, Sean and Feldman, Sergey and Goharian, Nazli and
               Downey, Doug and Cohan, Arman",
  journal   = "Transactions of the Association for Computational Linguistics",
  publisher = "MIT Press - Journals",
  volume    =  10,
  pages     = "224--239",
  abstract  = "Abstract Pretrained contextualized language models such as BERT
               and T5 have established a new state-of-the-art for ad-hoc search.
               However, it is not yet well understood why these methods are so
               effective, what makes some variants more effective than others,
               and what pitfalls they may have. We present a new comprehensive
               framework for Analyzing the Behavior of Neural IR ModeLs
               (ABNIRML), which includes new types of diagnostic probes that
               allow us to test several characteristics—such as writing styles,
               factuality, sensitivity to paraphrasing and word order—that are
               not addressed by previous techniques. To demonstrate the value of
               the framework, we conduct an extensive empirical study that
               yields insights into the factors that contribute to the neural
               model’s gains, and identify potential unintended biases the
               models exhibit. Some of our results confirm conventional wisdom,
               for example, that recent neural ranking models rely less on exact
               term overlap with the query, and instead leverage richer
               linguistic information, evidenced by their higher sensitivity to
               word and sentence order. Other results are more surprising, such
               as that some models (e.g., T5 and ColBERT) are biased towards
               factually correct (rather than simply relevant) texts. Further,
               some characteristics vary even for the same base language model,
               and other characteristics can appear due to random variations
               during model training.1",
  month     =  mar,
  year      =  2022,
  doi       = "10.1162/tacl\_a\_00457",
  issn      = "2307-387X",
  language  = "en"
}

@ARTICLE{Lo2023-td,
  title         = "The Semantic Reader Project: Augmenting Scholarly Documents
                   through {AI}-Powered Interactive Reading Interfaces",
  author        = "Lo, Kyle and Chang, Joseph Chee and Head, Andrew and Bragg,
                   Jonathan and Zhang, Amy X and Trier, Cassidy and
                   Anastasiades, Chloe and August, Tal and Authur, Russell and
                   Bragg, Danielle and Bransom, Erin and Cachola, Isabel and
                   Candra, Stefan and Chandrasekhar, Yoganand and Chen, Yen-Sung
                   and Cheng, Evie Yu-Yen and Chou, Yvonne and Downey, Doug and
                   Evans, Rob and Fok, Raymond and Hu, Fangzhou and Huff, Regan
                   and Kang, Dongyeop and Kim, Tae Soo and Kinney, Rodney and
                   Kittur, Aniket and Kang, Hyeonsu and Klevak, Egor and Kuehl,
                   Bailey and Langan, Michael and Latzke, Matt and Lochner,
                   Jaron and MacMillan, Kelsey and Marsh, Eric and Murray, Tyler
                   and Naik, Aakanksha and Nguyen, Ngoc-Uyen and Palani, Srishti
                   and Park, Soya and Paulic, Caroline and Rachatasumrit, Napol
                   and Rao, Smita and Sayre, Paul and Shen, Zejiang and
                   Siangliulue, Pao and Soldaini, Luca and Tran, Huy and van
                   Zuylen, Madeleine and Wang, Lucy Lu and Wilhelm, Christopher
                   and Wu, Caroline and Yang, Jiangjiang and Zamarron, Angele
                   and Hearst, Marti A and Weld, Daniel S",
  journal       = "arXiv [cs.HC]",
  abstract      = "Scholarly publications are key to the transfer of knowledge
                   from scholars to others. However, research papers are
                   information-dense, and as the volume of the scientific
                   literature grows, the need for new technology to support the
                   reading process grows. In contrast to the process of finding
                   papers, which has been transformed by Internet technology,
                   the experience of reading research papers has changed little
                   in decades. The PDF format for sharing research papers is
                   widely used due to its portability, but it has significant
                   downsides including: static content, poor accessibility for
                   low-vision readers, and difficulty reading on mobile devices.
                   This paper explores the question ``Can recent advances in AI
                   and HCI power intelligent, interactive, and accessible
                   reading interfaces -- even for legacy PDFs?'' We describe the
                   Semantic Reader Project, a collaborative effort across
                   multiple institutions to explore automatic creation of
                   dynamic reading interfaces for research papers. Through this
                   project, we've developed ten research prototype interfaces
                   and conducted usability studies with more than 300
                   participants and real-world users showing improved reading
                   experiences for scholars. We've also released a production
                   reading interface for research papers that will incorporate
                   the best features as they mature. We structure this paper
                   around challenges scholars and the public face when reading
                   research papers -- Discovery, Efficiency, Comprehension,
                   Synthesis, and Accessibility -- and present an overview of
                   our progress and remaining open challenges.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC",
  eprint        = "2303.14334"
}

@INPROCEEDINGS{MacAvaney2021-ex,
  title     = "Simplified Data Wrangling with ir\_datasets",
  author    = "MacAvaney, Sean and Yates, Andrew and Feldman, Sergey and Downey,
               Doug and Cohan, Arman and Goharian, Nazli",
  booktitle = "Proceedings of the 44th International ACM SIGIR Conference on
               Research and Development in Information Retrieval",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "2429--2436",
  abstract  = "Managing the data for Information Retrieval (IR) experiments can
               be challenging. Dataset documentation is scattered across the
               Internet and once one obtains a copy of the data, there are
               numerous different data formats to work with. Even basic formats
               can have subtle dataset-specific nuances that need to be
               considered for proper use. To help mitigate these challenges, we
               introduce a new robust and lightweight tool (ir\_datasets) for
               acquiring, managing, and performing typical operations over
               datasets used in IR. We primarily focus on textual datasets used
               for ad-hoc search. This tool provides both a Python and command
               line interface to numerous IR datasets and benchmarks. To our
               knowledge, this is the most extensive tool of its kind.
               Integrations with popular IR indexing and experimentation
               toolkits demonstrate the tool's utility. We also provide
               documentation of these datasets through the \sys catalog:
               https://ir-datasets.com/. The catalog acts as a hub for
               information on datasets used in IR, providing core information
               about what data each benchmark provides as well as links to more
               detailed information. We welcome community contributions and
               intend to continue to maintain and grow this tool.",
  series    = "SIGIR '21",
  month     =  jul,
  year      =  2021,
  keywords  = "information retrieval, benchmarks, datasets",
  doi       = "10.1145/3404835.3463254",
  isbn      =  9781450380379
}

@ARTICLE{MacAvaney2023-da,
  title         = "One-Shot Labeling for Automatic Relevance Estimation",
  author        = "MacAvaney, Sean and Soldaini, Luca",
  journal       = "arXiv [cs.IR]",
  abstract      = "Dealing with unjudged documents (``holes'') in relevance
                   assessments is a perennial problem when evaluating search
                   systems with offline experiments. Holes can reduce the
                   apparent effectiveness of retrieval systems during evaluation
                   and introduce biases in models trained with incomplete data.
                   In this work, we explore whether large language models can
                   help us fill such holes to improve offline evaluations. We
                   examine an extreme, albeit common, evaluation setting wherein
                   only a single known relevant document per query is available
                   for evaluation. We then explore various approaches for
                   predicting the relevance of unjudged documents with respect
                   to a query and the known relevant document, including nearest
                   neighbor, supervised, and prompting techniques. We find that
                   although the predictions of these One-Shot Labelers (1SL)
                   frequently disagree with human assessments, the labels they
                   produce yield a far more reliable ranking of systems than the
                   single labels do alone. Specifically, the strongest
                   approaches can consistently reach system ranking correlations
                   of over 0.86 with the full rankings over a variety of
                   measures. Meanwhile, the approach substantially increases the
                   reliability of t-tests due to filling holes in relevance
                   assessments, giving researchers more confidence in results
                   they find to be significant. Alongside this work, we release
                   an easy-to-use software package to enable the use of 1SL for
                   evaluation of other ad-hoc collections or systems.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "2302.11266"
}

@INPROCEEDINGS{Nguyen2023-ty,
  title     = "A Unified Framework for Learned Sparse Retrieval",
  author    = "Nguyen, Thong and MacAvaney, Sean and Yates, Andrew",
  booktitle = "Advances in Information Retrieval",
  publisher = "Springer Nature Switzerland",
  pages     = "101--116",
  abstract  = "Learned sparse retrieval (LSR) is a family of first-stage
               retrieval methods that are trained to generate sparse lexical
               representations of queries and documents for use with an inverted
               index. Many LSR methods have been recently introduced, with
               Splade models achieving state-of-the-art performance on MSMarco.
               Despite similarities in their model architectures, many LSR
               methods show substantial differences in effectiveness and
               efficiency. Differences in the experimental setups and
               configurations used make it difficult to compare the methods and
               derive insights. In this work, we analyze existing LSR methods
               and identify key components to establish an LSR framework that
               unifies all LSR methods under the same perspective. We then
               reproduce all prominent methods using a common codebase and
               re-train them in the same environment, which allows us to
               quantify how components of the framework affect effectiveness and
               efficiency. We find that (1) including document term weighting is
               most important for a method’s effectiveness, (2) including query
               weighting has a small positive impact, and (3) document expansion
               and query expansion have a cancellation effect. As a result, we
               show how removing query expansion from a state-of-the-art model
               can reduce latency significantly while maintaining effectiveness
               on MSMarco and TripClick benchmarks. Our code is publicly
               available (Code:
               https://github.com/thongnt99/learned-sparse-retrieval).",
  year      =  2023,
  doi       = "10.1007/978-3-031-28241-6\_7"
}

@ARTICLE{Nguyen2023-yh,
  title         = "Adapting Learned Sparse Retrieval for Long Documents",
  author        = "Nguyen, Thong and MacAvaney, Sean and Yates, Andrew",
  journal       = "arXiv [cs.IR]",
  abstract      = "Learned sparse retrieval (LSR) is a family of neural
                   retrieval methods that transform queries and documents into
                   sparse weight vectors aligned with a vocabulary. While LSR
                   approaches like Splade work well for short passages, it is
                   unclear how well they handle longer documents. We investigate
                   existing aggregation approaches for adapting LSR to longer
                   documents and find that proximal scoring is crucial for LSR
                   to handle long documents. To leverage this property, we
                   proposed two adaptations of the Sequential Dependence Model
                   (SDM) to LSR: ExactSDM and SoftSDM. ExactSDM assumes only
                   exact query term dependence, while SoftSDM uses potential
                   functions that model the dependence of query terms and their
                   expansion terms (i.e., terms identified using a transformer's
                   masked language modeling head). Experiments on the MSMARCO
                   Document and TREC Robust04 datasets demonstrate that both
                   ExactSDM and SoftSDM outperform existing LSR aggregation
                   approaches for different document length constraints.
                   Surprisingly, SoftSDM does not provide any performance
                   benefits over ExactSDM. This suggests that soft proximity
                   matching is not necessary for modeling term dependence in
                   LSR. Overall, this study provides insights into handling long
                   documents with LSR, proposing adaptations that improve its
                   performance.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "2305.18494"
}

@INPROCEEDINGS{Gospodinov2023-sn,
  title     = "{Doc2Query–}: When Less is More",
  author    = "Gospodinov, Mitko and MacAvaney, Sean and Macdonald, Craig",
  booktitle = "Advances in Information Retrieval",
  publisher = "Springer Nature Switzerland",
  pages     = "414--422",
  abstract  = "Doc2Query—the process of expanding the content of a document
               before indexing using a sequence-to-sequence model—has emerged as
               a prominent technique for improving the first-stage retrieval
               effectiveness of search engines. However, sequence-to-sequence
               models are known to be prone to “hallucinating” content that is
               not present in the source text. We argue that Doc2Query is indeed
               prone to hallucination, which ultimately harms retrieval
               effectiveness and inflates the index size. In this work, we
               explore techniques for filtering out these harmful queries prior
               to indexing. We find that using a relevance model to remove
               poor-quality queries can improve the retrieval effectiveness of
               Doc2Query by up to 16\%, while simultaneously reducing mean query
               execution time by 30\% and cutting the index size by 48\%. We
               release the code, data, and a live demonstration to facilitate
               reproduction and further exploration
               (https://github.com/terrierteam/pyterrier\_doc2query).",
  year      =  2023,
  doi       = "10.1007/978-3-031-28238-6\_31"
}

@INPROCEEDINGS{MacAvaney2023-be,
  title     = "One-Shot Labeling for Automatic Relevance Estimation",
  author    = "MacAvaney, Sean and Soldaini, Luca",
  booktitle = "Proceedings of the 46th International ACM SIGIR Conference on
               Research and Development in Information Retrieval",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "2230--2235",
  abstract  = "Dealing with unjudged documents (``holes'') in relevance
               assessments is a perennial problem when evaluating search systems
               with offline experiments. Holes can reduce the apparent
               effectiveness of retrieval systems during evaluation and
               introduce biases in models trained with incomplete data. In this
               work, we explore whether large language models can help us fill
               such holes to improve offline evaluations. We examine an extreme,
               albeit common, evaluation setting wherein only a single known
               relevant document per query is available for evaluation. We then
               explore various approaches for predicting the relevance of
               unjudged documents with respect to a query and the known relevant
               document, including nearest neighbor, supervised, and prompting
               techniques. We find that although the predictions of these
               One-Shot Labelers (1SL) frequently disagree with human
               assessments, the labels they produce yield a far more reliable
               ranking of systems than the single labels do alone. Specifically,
               the strongest approaches can consistently reach system ranking
               correlations of over 0.86 with the full rankings over a variety
               of measures. Meanwhile, the approach substantially increases the
               reliability of t-tests due to filling holes in relevance
               assessments, giving researchers more confidence in results they
               find to be significant. Alongside this work, we release an
               easy-to-use software package to enable the use of 1SL for
               evaluation of other ad-hoc collections or systems.",
  series    = "SIGIR '23",
  month     =  jul,
  year      =  2023,
  keywords  = "few-shot learning, relevance assessments, neural networks",
  doi       = "10.1145/3539618.3592032",
  isbn      =  9781450394086
}

@INPROCEEDINGS{Nguyen2023-hw,
  title     = "Adapting Learned Sparse Retrieval for Long Documents",
  author    = "Nguyen, Thong and MacAvaney, Sean and Yates, Andrew",
  booktitle = "Proceedings of the 46th International ACM SIGIR Conference on
               Research and Development in Information Retrieval",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1781--1785",
  abstract  = "Learned sparse retrieval (LSR) is a family of neural retrieval
               methods that transform queries and documents into sparse weight
               vectors aligned with a vocabulary. While LSR approaches like
               Splade work well for short passages, it is unclear how well they
               handle longer documents. We investigate existing aggregation
               approaches for adapting LSR to longer documents and find that
               proximal scoring is crucial for LSR to handle long documents. To
               leverage this property, we proposed two adaptations of the
               Sequential Dependence Model (SDM) to LSR: ExactSDM and SoftSDM.
               ExactSDM assumes only exact query term dependence, while SoftSDM
               uses potential functions that model the dependence of query terms
               and their expansion terms (i.e., terms identified using a
               transformer's masked language modeling head).Experiments on the
               MSMARCO Document and TREC Robust04 datasets demonstrate that both
               ExactSDM and SoftSDM outperform existing LSR aggregation
               approaches for different document length constraints.
               Surprisingly, SoftSDM does not provide any performance benefits
               over ExactSDM. This suggests that soft proximity matching is not
               necessary for modeling term dependence in LSR. Overall, this
               study provides insights into handling long documents with LSR,
               proposing adaptations that improve its performance.",
  series    = "SIGIR '23",
  month     =  jul,
  year      =  2023,
  keywords  = "learned sparse retrieval, long documents, term proximity",
  doi       = "10.1145/3539618.3591943",
  isbn      =  9781450394086
}

@ARTICLE{Li2023-ix,
  title         = "{FLM}-{101B}: An open {LLM} and how to train it with \${100K}
                   budget",
  author        = "Li, Xiang and Yao, Yiqun and Jiang, Xin and Fang, Xuezhi and
                   Meng, Xuying and Fan, Siqi and Han, Peng and Li, Jing and Du,
                   Li and Qin, Bowen and Zhang, Zheng and Sun, Aixin and Wang,
                   Yequan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large language models (LLMs) have achieved remarkable success
                   in NLP and multimodal tasks. Despite these successes, their
                   development faces two main challenges: (i) high computational
                   cost; and (ii) difficulty in conducting fair and objective
                   evaluations. LLMs are prohibitively expensive, making it
                   feasible for only a few major players to undertake their
                   training, thereby constraining both research and application
                   opportunities. This underscores the importance of
                   cost-effective LLM training. In this paper, we utilize a
                   growth strategy to significantly reduce LLM training cost. We
                   demonstrate that an LLM with 101B parameters and 0.31TB
                   tokens can be trained on a $100K budget. We also adopt a
                   systematic evaluation paradigm for the IQ evaluation of LLMs,
                   in complement to existing evaluations that focus more on
                   knowledge-oriented abilities. We introduce our benchmark
                   including evaluations on important aspects of intelligence
                   including symbolic mapping, itrule understanding, pattern
                   mining, and anti-interference. Such evaluations minimize the
                   potential impact of memorization. Experimental results show
                   that our model FLM-101B, trained with a budget of $100K,
                   achieves comparable performance to powerful and well-known
                   models, eg GPT-3 and GLM-130B, especially in the IQ benchmark
                   evaluations with contexts unseen in training data. The
                   checkpoint of FLM-101B will be open-sourced at
                   https://huggingface.co/CofeAI/FLM-101B.",
  month         =  sep,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2309.03852"
}

@ARTICLE{Zhu2023-zl,
  title     = "Large language models for information retrieval: A survey",
  author    = "Zhu, Yutao and Yuan, Huaying and Wang, Shuting and Liu, Jiongnan
               and Liu, Wenhan and Deng, Chenlong and Dou, Zhicheng and Wen,
               Ji-Rong",
  journal   = "arXiv.org",
  publisher = "arXiv",
  abstract  = "As a primary means of information acquisition, information
               retrieval (IR) systems, such as search engines, have integrated
               themselves into our daily lives. These systems also serve as
               components of dialogue, question-answering, and recommender
               systems. The trajectory of IR has evolved dynamically from its
               origins in term-based methods to its integration with advanced
               neural models. While the neural models excel at capturing complex
               contextual signals and semantic nuances, thereby reshaping the IR
               landscape, they still face challenges such as data scarcity,
               interpretability, and the generation of contextually plausible
               yet potentially inaccurate responses. This evolution requires a
               combination of both traditional methods (such as term-based
               sparse retrieval methods with rapid response) and modern neural
               architectures (such as language models with powerful language
               understanding capacity). Meanwhile, the emergence of large
               language models (LLMs), typified by ChatGPT and GPT-4, has
               revolutionized natural language processing due to their
               remarkable language understanding, generation, generalization,
               and reasoning abilities. Consequently, recent research has sought
               to leverage LLMs to improve IR systems. Given the rapid evolution
               of this research trajectory, it is necessary to consolidate
               existing methodologies and provide nuanced insights through a
               comprehensive overview. In this survey, we delve into the
               confluence of LLMs and IR systems, including crucial aspects such
               as query rewriters, retrievers, rerankers, and readers.
               Additionally, we explore promising directions within this
               expanding field.",
  year      =  2023,
  eprint    = "2308.07107",
  doi       = "10.48550/ARXIV.2308.07107",
  language  = "en"
}

@ARTICLE{Izacard2020-zn,
  title         = "Leveraging Passage Retrieval with Generative Models for Open
                   Domain Question Answering",
  author        = "Izacard, Gautier and Grave, Edouard",
  journal       = "arXiv [cs.CL]",
  abstract      = "Generative models for open domain question answering have
                   proven to be competitive, without resorting to external
                   knowledge. While promising, this approach requires to use
                   models with billions of parameters, which are expensive to
                   train and query. In this paper, we investigate how much these
                   models can benefit from retrieving text passages, potentially
                   containing evidence. We obtain state-of-the-art results on
                   the Natural Questions and TriviaQA open benchmarks.
                   Interestingly, we observe that the performance of this method
                   significantly improves when increasing the number of
                   retrieved passages. This is evidence that generative models
                   are good at aggregating and combining evidence from multiple
                   passages.",
  month         =  jul,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2007.01282"
}

@MISC{Dror2019-ko,
  title   = "Deep Dominance - How to Properly Compare Deep Neural Models",
  author  = "Dror, Rotem and Shlomov, Segev and Reichart, Roi",
  journal = "Proceedings of the 57th Annual Meeting of the Association for
             Computational Linguistics",
  year    =  2019,
  doi     = "10.18653/v1/p19-1266"
}

@ARTICLE{Haddad2019-lc,
  title         = "Learning More From Less: Towards Strengthening Weak
                   Supervision for Ad-Hoc Retrieval",
  author        = "Haddad, Dany and Ghosh, Joydeep",
  journal       = "arXiv [cs.IR]",
  abstract      = "The limited availability of ground truth relevance labels has
                   been a major impediment to the application of supervised
                   methods to ad-hoc retrieval. As a result, unsupervised
                   scoring methods, such as BM25, remain strong competitors to
                   deep learning techniques which have brought on dramatic
                   improvements in other domains, such as computer vision and
                   natural language processing. Recent works have shown that it
                   is possible to take advantage of the performance of these
                   unsupervised methods to generate training data for
                   learning-to-rank models. The key limitation to this line of
                   work is the size of the training set required to surpass the
                   performance of the original unsupervised method, which can be
                   as large as $10^{13}$ training examples. Building on these
                   insights, we propose two methods to reduce the amount of
                   training data required. The first method takes inspiration
                   from crowdsourcing, and leverages multiple unsupervised
                   rankers to generate soft, or noise-aware, training labels.
                   The second identifies harmful, or mislabeled, training
                   examples and removes them from the training set. We show that
                   our methods allow us to surpass the performance of the
                   unsupervised baseline with far fewer training examples than
                   previous works.",
  month         =  jul,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "1907.08657",
  doi           = "10.1145/3331184.3331272"
}

@ARTICLE{Chen2020-vj,
  title         = "{AdaBERT}: Task-Adaptive {BERT} Compression with
                   Differentiable Neural Architecture Search",
  author        = "Chen, Daoyuan and Li, Yaliang and Qiu, Minghui and Wang, Zhen
                   and Li, Bofang and Ding, Bolin and Deng, Hongbo and Huang,
                   Jun and Lin, Wei and Zhou, Jingren",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large pre-trained language models such as BERT have shown
                   their effectiveness in various natural language processing
                   tasks. However, the huge parameter size makes them difficult
                   to be deployed in real-time applications that require quick
                   inference with limited resources. Existing methods compress
                   BERT into small models while such compression is
                   task-independent, i.e., the same compressed BERT for all
                   different downstream tasks. Motivated by the necessity and
                   benefits of task-oriented BERT compression, we propose a
                   novel compression method, AdaBERT, that leverages
                   differentiable Neural Architecture Search to automatically
                   compress BERT into task-adaptive small models for specific
                   tasks. We incorporate a task-oriented knowledge distillation
                   loss to provide search hints and an efficiency-aware loss as
                   search constraints, which enables a good trade-off between
                   efficiency and effectiveness for task-adaptive BERT
                   compression. We evaluate AdaBERT on several NLP tasks, and
                   the results demonstrate that those task-adaptive compressed
                   models are 12.7x to 29.3x faster than BERT in inference time
                   and 11.5x to 17.0x smaller in terms of parameter size, while
                   comparable performance is maintained.",
  month         =  jan,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2001.04246"
}

@ARTICLE{Conneau2019-ft,
  title         = "Unsupervised Cross-lingual Representation Learning at Scale",
  author        = "Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and
                   Chaudhary, Vishrav and Wenzek, Guillaume and Guzmán,
                   Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer,
                   Luke and Stoyanov, Veselin",
  journal       = "arXiv [cs.CL]",
  abstract      = "This paper shows that pretraining multilingual language
                   models at scale leads to significant performance gains for a
                   wide range of cross-lingual transfer tasks. We train a
                   Transformer-based masked language model on one hundred
                   languages, using more than two terabytes of filtered
                   CommonCrawl data. Our model, dubbed XLM-R, significantly
                   outperforms multilingual BERT (mBERT) on a variety of
                   cross-lingual benchmarks, including +13.8\% average accuracy
                   on XNLI, +12.3\% average F1 score on MLQA, and +2.1\% average
                   F1 score on NER. XLM-R performs particularly well on
                   low-resource languages, improving 11.8\% in XNLI accuracy for
                   Swahili and 9.2\% for Urdu over the previous XLM model. We
                   also present a detailed empirical evaluation of the key
                   factors that are required to achieve these gains, including
                   the trade-offs between (1) positive transfer and capacity
                   dilution and (2) the performance of high and low resource
                   languages at scale. Finally, we show, for the first time, the
                   possibility of multilingual modeling without sacrificing
                   per-language performance; XLM-Ris very competitive with
                   strong monolingual models on the GLUE and XNLI benchmarks. We
                   will make XLM-R code, data, and models publicly available.",
  month         =  nov,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1911.02116"
}

@ARTICLE{Jiao2019-gl,
  title         = "{TinyBERT}: Distilling {BERT} for Natural Language
                   Understanding",
  author        = "Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin
                   and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun",
  journal       = "arXiv [cs.CL]",
  abstract      = "Language model pre-training, such as BERT, has significantly
                   improved the performances of many natural language processing
                   tasks. However, pre-trained language models are usually
                   computationally expensive and memory intensive, so it is
                   difficult to effectively execute them on some
                   resource-restricted devices. To accelerate inference and
                   reduce model size while maintaining accuracy, we firstly
                   propose a novel transformer distillation method that is a
                   specially designed knowledge distillation (KD) method for
                   transformer-based models. By leveraging this new KD method,
                   the plenty of knowledge encoded in a large teacher BERT can
                   be well transferred to a small student TinyBERT. Moreover, we
                   introduce a new two-stage learning framework for TinyBERT,
                   which performs transformer distillation at both the
                   pre-training and task-specific learning stages. This
                   framework ensures that TinyBERT can capture both the
                   general-domain and task-specific knowledge of the teacher
                   BERT. TinyBERT is empirically effective and achieves
                   comparable results with BERT in GLUE datasets, while being
                   7.5x smaller and 9.4x faster on inference. TinyBERT is also
                   significantly better than state-of-the-art baselines, even
                   with only about 28\% parameters and 31\% inference time of
                   baselines.",
  month         =  sep,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1909.10351"
}

@ARTICLE{Lan2019-on,
  title         = "{ALBERT}: A Lite {BERT} for Self-supervised Learning of
                   Language Representations",
  author        = "Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and
                   Gimpel, Kevin and Sharma, Piyush and Soricut, Radu",
  journal       = "arXiv [cs.CL]",
  abstract      = "Increasing model size when pretraining natural language
                   representations often results in improved performance on
                   downstream tasks. However, at some point further model
                   increases become harder due to GPU/TPU memory limitations,
                   longer training times, and unexpected model degradation. To
                   address these problems, we present two parameter-reduction
                   techniques to lower memory consumption and increase the
                   training speed of BERT. Comprehensive empirical evidence
                   shows that our proposed methods lead to models that scale
                   much better compared to the original BERT. We also use a
                   self-supervised loss that focuses on modeling inter-sentence
                   coherence, and show it consistently helps downstream tasks
                   with multi-sentence inputs. As a result, our best model
                   establishes new state-of-the-art results on the GLUE, RACE,
                   and SQuAD benchmarks while having fewer parameters compared
                   to BERT-large.The code and the pretrained models are
                   available at
                   https://github.com/google-research/google-research/tree/master/albert.",
  month         =  sep,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1909.11942"
}

@ARTICLE{Li2019-rs,
  title         = "Dice Loss for Data-imbalanced {NLP} Tasks",
  author        = "Li, Xiaoya and Sun, Xiaofei and Meng, Yuxian and Liang,
                   Junjun and Wu, Fei and Li, Jiwei",
  journal       = "arXiv [cs.CL]",
  abstract      = "Many NLP tasks such as tagging and machine reading
                   comprehension are faced with the severe data imbalance issue:
                   negative examples significantly outnumber positive examples,
                   and the huge number of background examples (or easy-negative
                   examples) overwhelms the training. The most commonly used
                   cross entropy (CE) criteria is actually an accuracy-oriented
                   objective, and thus creates a discrepancy between training
                   and test: at training time, each training instance
                   contributes equally to the objective function, while at test
                   time F1 score concerns more about positive examples. In this
                   paper, we propose to use dice loss in replacement of the
                   standard cross-entropy objective for data-imbalanced NLP
                   tasks. Dice loss is based on the Sorensen-Dice coefficient or
                   Tversky index, which attaches similar importance to false
                   positives and false negatives, and is more immune to the
                   data-imbalance issue. To further alleviate the dominating
                   influence from easy-negative examples in training, we propose
                   to associate training examples with dynamically adjusted
                   weights to deemphasize easy-negative examples.Theoretical
                   analysis shows that this strategy narrows down the gap
                   between the F1 score in evaluation and the dice loss in
                   training. With the proposed training objective, we observe
                   significant performance boost on a wide range of data
                   imbalanced NLP tasks. Notably, we are able to achieve SOTA
                   results on CTB5, CTB6 and UD1.4 for the part of speech
                   tagging task; SOTA results on CoNLL03, OntoNotes5.0, MSRA and
                   OntoNotes4.0 for the named entity recognition task; along
                   with competitive results on the tasks of machine reading
                   comprehension and paraphrase identification.",
  month         =  nov,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1911.02855"
}

@ARTICLE{Sil2017-ii,
  title         = "One for All: Towards Language Independent Named Entity
                   Linking",
  author        = "Sil, Avirup and Florian, Radu",
  journal       = "arXiv [cs.CL]",
  abstract      = "Entity linking (EL) is the task of disambiguating mentions in
                   text by associating them with entries in a predefined
                   database of mentions (persons, organizations, etc). Most
                   previous EL research has focused mainly on one language,
                   English, with less attention being paid to other languages,
                   such as Spanish or Chinese. In this paper, we introduce LIEL,
                   a Language Independent Entity Linking system, which provides
                   an EL framework which, once trained on one language, works
                   remarkably well on a number of different languages without
                   change. LIEL makes a joint global prediction over the entire
                   document, employing a discriminative reranking framework with
                   many domain and language-independent feature functions.
                   Experiments on numerous benchmark datasets, show that the
                   proposed system, once trained on one language, English,
                   outperforms several state-of-the-art systems in English (by 4
                   points) and the trained model also works very well on Spanish
                   (14 points better than a competitor system), demonstrating
                   the viability of the approach.",
  month         =  dec,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1712.01797"
}

@ARTICLE{Chauhan2019-ko,
  title         = "{REflex}: Flexible Framework for Relation Extraction in
                   Multiple Domains",
  author        = "Chauhan, Geeticka and McDermott, Matthew B A and Szolovits,
                   Peter",
  journal       = "arXiv [cs.CL]",
  abstract      = "Systematic comparison of methods for relation extraction (RE)
                   is difficult because many experiments in the field are not
                   described precisely enough to be completely reproducible and
                   many papers fail to report ablation studies that would
                   highlight the relative contributions of their various
                   combined techniques. In this work, we build a unifying
                   framework for RE, applying this on three highly used datasets
                   (from the general, biomedical and clinical domains) with the
                   ability to be extendable to new datasets. By performing a
                   systematic exploration of modeling, pre-processing and
                   training methodologies, we find that choices of
                   pre-processing are a large contributor performance and that
                   omission of such information can further hinder fair
                   comparison. Other insights from our exploration allow us to
                   provide recommendations for future research in this area.",
  month         =  jun,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1906.08318"
}

@ARTICLE{Collobert2019-zt,
  title         = "A Fully Differentiable Beam Search Decoder",
  author        = "Collobert, Ronan and Hannun, Awni and Synnaeve, Gabriel",
  journal       = "arXiv [cs.CL]",
  abstract      = "We introduce a new beam search decoder that is fully
                   differentiable, making it possible to optimize at training
                   time through the inference procedure. Our decoder allows us
                   to combine models which operate at different granularities
                   (e.g. acoustic and language models). It can be used when
                   target sequences are not aligned to input sequences by
                   considering all possible alignments between the two. We
                   demonstrate our approach scales by applying it to speech
                   recognition, jointly training acoustic and word-level
                   language models. The system is end-to-end, with gradients
                   flowing through the whole architecture from the word-level
                   transcriptions. Recent research efforts have shown that deep
                   neural networks with attention-based mechanisms are powerful
                   enough to successfully train an acoustic model from the final
                   transcription, while implicitly learning a language model.
                   Instead, we show that it is possible to discriminatively
                   train an acoustic model jointly with an explicit and possibly
                   pre-trained language model.",
  month         =  feb,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1902.06022"
}

@ARTICLE{Talmor2018-tb,
  title         = "{CommonsenseQA}: A Question Answering Challenge Targeting
                   Commonsense Knowledge",
  author        = "Talmor, Alon and Herzig, Jonathan and Lourie, Nicholas and
                   Berant, Jonathan",
  journal       = "arXiv [cs.CL]",
  abstract      = "When answering a question, people often draw upon their rich
                   world knowledge in addition to the particular context. Recent
                   work has focused primarily on answering questions given some
                   relevant document or context, and required very little
                   general background. To investigate question answering with
                   prior knowledge, we present CommonsenseQA: a challenging new
                   dataset for commonsense question answering. To capture common
                   sense beyond associations, we extract from ConceptNet (Speer
                   et al., 2017) multiple target concepts that have the same
                   semantic relation to a single source concept. Crowd-workers
                   are asked to author multiple-choice questions that mention
                   the source concept and discriminate in turn between each of
                   the target concepts. This encourages workers to create
                   questions with complex semantics that often require prior
                   knowledge. We create 12,247 questions through this procedure
                   and demonstrate the difficulty of our task with a large
                   number of strong baselines. Our best baseline is based on
                   BERT-large (Devlin et al., 2018) and obtains 56\% accuracy,
                   well below human performance, which is 89\%.",
  month         =  nov,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1811.00937"
}

@ARTICLE{Feng2018-ci,
  title    = "Pathologies of Neural Models Make Interpretation Difficult",
  author   = "Feng, Shi and Wallace, Eric and Grissom, Alvin and Iyyer, Mohit
              and Rodriguez, Pedro and Boyd-Graber, Jordan L",
  journal  = "undefined",
  abstract = "One way to interpret neural model predictions is to highlight the
              most important input features—for example, a heatmap visualization
              over the words in an input sentence. In existing interpretation
              methods for NLP, a word’s importance is determined by either input
              perturbation—measuring the decrease in model confidence when that
              word is removed—or by the gradient with respect to that word. To
              understand the limitations of these methods, we use input
              reduction, which iteratively removes the least important word from
              the input. This exposes pathological behaviors of neural models:
              the remaining words appear nonsensical to humans and do not match
              the words interpretation methods deem important. As we confirm
              with human experiments, the reduced examples lack information to
              support the prediction of any label, but models still make the
              same predictions with high confidence. To explain these
              counterintuitive results, we draw connections to adversarial
              examples and confidence calibration: pathological behaviors reveal
              difficulties in interpreting neural models trained with maximum
              likelihood. To mitigate their deficiencies, we fine-tune the
              models by encouraging high entropy outputs on reduced examples.
              Fine-tuned models become more interpretable under input reduction
              without accuracy loss on regular examples.",
  year     =  2018,
  eprint   = "1804.07781"
}

@ARTICLE{Gonen2019-yy,
  title         = "Lipstick on a Pig: Debiasing Methods Cover up Systematic
                   Gender Biases in Word Embeddings But do not Remove Them",
  author        = "Gonen, Hila and Goldberg, Yoav",
  journal       = "arXiv [cs.CL]",
  abstract      = "Word embeddings are widely used in NLP for a vast range of
                   tasks. It was shown that word embeddings derived from text
                   corpora reflect gender biases in society. This phenomenon is
                   pervasive and consistent across different word embedding
                   models, causing serious concern. Several recent works tackle
                   this problem, and propose methods for significantly reducing
                   this gender bias in word embeddings, demonstrating convincing
                   results. However, we argue that this removal is superficial.
                   While the bias is indeed substantially reduced according to
                   the provided bias definition, the actual effect is mostly
                   hiding the bias, not removing it. The gender bias information
                   is still reflected in the distances between
                   ``gender-neutralized'' words in the debiased embeddings, and
                   can be recovered from them. We present a series of
                   experiments to support this claim, for two debiasing methods.
                   We conclude that existing bias removal techniques are
                   insufficient, and should not be trusted for providing
                   gender-neutral modeling.",
  month         =  mar,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1903.03862"
}

@ARTICLE{Jimmy2018-bl,
  title    = "Payoffs and pitfalls in using knowledge-bases for consumer health
              search",
  author   = "{Jimmy} and Zuccon, Guido and Koopman, Bevan",
  journal  = "Information Retrieval Journal",
  abstract = "Consumer health search (CHS) is a challenging domain with
              vocabulary mismatch and considerable domain expertise hampering
              peoples’ ability to formulate effective queries. We posit that
              using knowledge bases for query reformulation may help alleviate
              this problem. How to exploit knowledge bases for effective CHS is
              nontrivial, involving a swathe of key choices and design decisions
              (many of which are not explored in the literature). Here we
              rigorously empirically evaluate the impact these different choices
              have on retrieval effectiveness.A state-of-the-art knowledge-base
              retrieval model—the Entity Query Feature Expansion model—was used
              to evaluate these choices, which include: which knowledge base to
              use (specialised vs. general purpose), how to construct the
              knowledge base, how to extract entities from queries and map them
              to entities in the knowledge base, what part of the knowledge base
              to use for query expansion, and if to augment the knowledge base
              search process with relevance feedback.While knowledge base
              retrieval has been proposed as a solution for CHS, this paper
              delves into the finer details of doing this effectively,
              highlighting both payoffs and pitfalls. It aims to provide some
              lessons to others in advancing the state-of-the-art in CHS.",
  month    =  nov,
  year     =  2018,
  doi      = "10.1007/s10791-018-9344-z",
  issn     = "1573-7659"
}

@INPROCEEDINGS{Zamani2017-px,
  title     = "Relevance-based Word Embedding",
  author    = "Zamani, Hamed and Croft, W Bruce",
  booktitle = "Proceedings of the 40th International ACM SIGIR Conference on
               Research and Development in Information Retrieval",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "505--514",
  abstract  = "Learning a high-dimensional dense representation for vocabulary
               terms, also known as a word embedding, has recently attracted
               much attention in natural language processing and information
               retrieval tasks. The embedding vectors are typically learned
               based on term proximity in a large corpus. This means that the
               objective in well-known word embedding algorithms, e.g.,
               word2vec, is to accurately predict adjacent word(s) for a given
               word or context. However, this objective is not necessarily
               equivalent to the goal of many information retrieval (IR) tasks.
               The primary objective in various IR tasks is to capture relevance
               instead of term proximity, syntactic, or even semantic
               similarity. This is the motivation for developing unsupervised
               relevance-based word embedding models that learn word
               representations based on query-document relevance information. In
               this paper, we propose two learning models with different
               objective functions; one learns a relevance distribution over the
               vocabulary set for each query, and the other classifies each term
               as belonging to the relevant or non-relevant class for each
               query. To train our models, we used over six million unique
               queries and the top ranked documents retrieved in response to
               each query, which are assumed to be relevant to the query. We
               extrinsically evaluate our learned word representation models
               using two IR tasks: query expansion and query classification.
               Both query expansion experiments on four TREC collections and
               query classification experiments on the KDD Cup 2005 dataset
               suggest that the relevance-based word embedding models
               significantly outperform state-of-the-art proximity-based
               embedding models, such as word2vec and GloVe.",
  series    = "SIGIR '17",
  year      =  2017,
  keywords  = "embedding vector, neural network, query classification, query
               expansion, word representation",
  doi       = "10.1145/3077136.3080831",
  isbn      =  9781450350228
}

@INPROCEEDINGS{Goodwin2016-ga,
  title     = "Medical Question Answering for Clinical Decision Support",
  author    = "Goodwin, Travis R and Harabagiu, Sanda M",
  booktitle = "Proceedings of the 25th ACM International on Conference on
               Information and Knowledge Management",
  publisher = "ACM",
  pages     = "297–306",
  month     =  oct,
  year      =  2016
}

@INPROCEEDINGS{Paul2015-qy,
  title     = "Diagnoses, Decisions, and Outcomes: Web Search as Decision
               Support for Cancer",
  author    = "Paul, Michael J and White, Ryen W and Horvitz, Eric",
  booktitle = "Proceedings of the 24th International Conference on World Wide
               Web",
  publisher = "International World Wide Web Conferences Steering Committee",
  pages     = "831–841",
  month     =  may,
  year      =  2015
}

@INPROCEEDINGS{Alsulmi2016-eo,
  title     = "Learning to predict the performance of clinical queries using an
               integrated approach",
  author    = "Alsulmi, M and Carterette, B",
  booktitle = "2016 IEEE International Conference on Bioinformatics and
               Biomedicine (BIBM)",
  pages     = "930–937",
  abstract  = "Several query performance prediction approaches have been
               proposed to estimate the retrieval effectiveness of user queries.
               One limitation in these approaches is that they estimate query
               performance without any consideration of the type of features
               implemented in the retrieval systems used to answer those
               queries. In this work, aiming to address this challenge, we use a
               learning based approach that combines several query predictors as
               well as some system features to predict the performance of a
               given query that is submitted to a certain retrieval system. We
               apply the result of cross-validated training to several retrieval
               systems submitted to TREC Clinical Decision Support (CDS) track,
               and show that our approach can estimate retrieval effectiveness
               values with high accuracy.",
  month     =  dec,
  year      =  2016
}

@INCOLLECTION{Whitehill2009-ka,
  title     = "Whose Vote Should Count More: Optimal Integration of Labels from
               Labelers of Unknown Expertise",
  author    = "Whitehill, Jacob and Wu, Ting-Fan and Bergsma, Jacob and
               Movellan, Javier R and Ruvolo, Paul L",
  editor    = "Bengio, Y and Schuurmans, D and Lafferty, J D and Williams, C K I
               and Culotta, A",
  booktitle = "Advances in Neural Information Processing Systems 22",
  publisher = "Curran Associates, Inc.",
  pages     = "2035–2043",
  year      =  2009
}

@INCOLLECTION{Soldaini2015-hs,
  title     = "Retrieving Medical Literature for Clinical Decision Support",
  author    = "Soldaini, Luca and Cohan, Arman and Yates, Andrew and Goharian,
               Nazli and Frieder, Ophir",
  booktitle = "Lecture Notes in Computer Science",
  pages     = "538–549",
  year      =  2015
}

@INPROCEEDINGS{Yu2016-qk,
  title     = "Retrofitting Word Vectors of {MeSH} Terms to Improve Semantic
               Similarity Measures",
  author    = "Yu, Zhiguo and Cohen, Trevor and Wallace, Byron and Bernstam,
               Elmer and Johnson, Todd",
  booktitle = "Proceedings of the Seventh International Workshop on Health Text
               Mining and Information Analysis",
  year      =  2016
}

@INPROCEEDINGS{Schoenherr2014-lp,
  title     = "Interactions between health searchers and search engines",
  author    = "Schoenherr, Georg P and White, Ryen W",
  booktitle = "Proceedings of the 37th international ACM SIGIR conference on
               Research \& development in information retrieval",
  publisher = "ACM",
  pages     = "143–152",
  month     =  jul,
  year      =  2014
}

@INPROCEEDINGS{Soldaini2014-tr,
  title     = "Query reformulation for clinical decision support search",
  author    = "Soldaini, Luca and Cohan, Arman and Yates, Andrew and Goharian,
               Nazli and Frieder, Ophir",
  booktitle = "23rd Text REtrieval Conference (TREC)",
  year      =  2014
}

@INPROCEEDINGS{Druck2008-yv,
  title     = "Learning from labeled features using generalized expectation
               criteria",
  author    = "Druck, Gregory and Mann, Gideon and McCallum, Andrew",
  booktitle = "SIGIR",
  pages     = "595–602",
  year      =  2008
}

@INPROCEEDINGS{Cartright2011-ri,
  title     = "Intentions and attention in exploratory health search",
  author    = "Cartright, Marc-Allen and White, Ryen W and Horvitz, Eric",
  booktitle = "Proceedings of the 34th international ACM SIGIR conference on
               Research and development in Information Retrieval",
  publisher = "ACM",
  pages     = "65–74",
  month     =  jul,
  year      =  2011
}

@ARTICLE{Uzuner2011-ul,
  title    = "2010 {i2b2}/{VA} challenge on concepts, assertions, and relations
              in clinical text",
  author   = "Uzuner, Özlem and South, Brett R and Shen, Shuying and DuVall,
              Scott L",
  journal  = "Journal of the American Medical Informatics Association: JAMIA",
  volume   =  18,
  number   =  5,
  pages    = "552–556",
  abstract = "The 2010 i2b2/VA Workshop on Natural Language Processing
              Challenges for Clinical Records presented three tasks: a concept
              extraction task focused on the extraction of medical concepts from
              patient reports; an assertion classification task focused on
              assigning assertion types for medical problem concepts; and a
              relation classification task focused on assigning relation types
              that hold between medical problems, tests, and treatments. i2b2
              and the VA provided an annotated reference standard corpus for the
              three tasks. Using this reference standard, 22 systems were
              developed for concept extraction, 21 for assertion classification,
              and 16 for relation classification. These systems showed that
              machine learning approaches could be augmented with rule-based
              systems to determine concepts, assertions, and relations.
              Depending on the task, the rule-based systems can either provide
              input for machine learning or post-process the output of machine
              learning. Ensembles of classifiers, information from unlabeled
              data, and external knowledge sources can help when the training
              data are inadequate.",
  month    =  sep,
  year     =  2011,
  issn     = "1067-5027",
  language = "en"
}

@ARTICLE{Roberts2016-xw,
  title   = "State-of-the-art in biomedical literature retrieval for clinical
             cases: a survey of the {TREC} 2014 {CDS} track",
  author  = "Roberts, Kirk and Simpson, Matthew and Demner-Fushman, Dina and
             Voorhees, Ellen and Hersh, William",
  journal = "Information Retrieval Journal",
  volume  =  19,
  number  = "1-2",
  pages   = "113–148",
  year    =  2016
}

@INPROCEEDINGS{Severyn2015-ty,
  title     = "Learning to Rank Short Text Pairs with Convolutional Deep Neural
               Networks",
  author    = "Severyn, Aliaksei and Moschitti, Alessandro",
  booktitle = "Proceedings of the 38th International ACM SIGIR Conference on
               Research and Development in Information Retrieval",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "373–382",
  series    = "SIGIR '15",
  month     =  aug,
  year      =  2015
}

@INPROCEEDINGS{Okazaki2010-el,
  title     = "Simple and Efficient Algorithm for Approximate Dictionary
               Matching",
  author    = "Okazaki, Naoaki and Tsujii, Jun'ichi",
  booktitle = "Proceedings of the 23rd International Conference on Computational
               Linguistics",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "851--859",
  series    = "COLING '10",
  year      =  2010
}

@INPROCEEDINGS{Liu2007-wf,
  title     = "Letor: Benchmark dataset for research on learning to rank for
               information retrieval",
  author    = "Liu, Tie-Yan and Xu, Jun and Qin, Tao and Xiong, Wenying and Li,
               Hang",
  booktitle = "Proceedings of SIGIR 2007 workshop on learning to rank for
               information retrieval",
  pages     = "3–10",
  year      =  2007
}

@TECHREPORT{Choi2014-dd,
  title       = "{SNUMedinfo} at {TREC} {CDS} track 2014: Medical case-based
                 retrieval task",
  author      = "Choi, Sungbin and Choi, Jinwook",
  institution = "DTIC Document",
  year        =  2014
}

@INPROCEEDINGS{Bendersky2008-jg,
  title     = "Discovering Key Concepts in Verbose Queries",
  author    = "Bendersky, Michael and Croft, W Bruce",
  booktitle = "Proceedings of the 31st Annual International ACM SIGIR Conference
               on Research and Development in Information Retrieval",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "491–498",
  series    = "SIGIR '08",
  year      =  2008
}

@INPROCEEDINGS{Bendersky2010-yj,
  title     = "Learning Concept Importance Using a Weighted Dependence Model",
  author    = "Bendersky, Michael and Metzler, Donald and Croft, W Bruce",
  booktitle = "Proceedings of the Third ACM International Conference on Web
               Search and Data Mining",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "31–40",
  series    = "WSDM '10",
  year      =  2010
}

@ARTICLE{Tax2015-qk,
  title   = "A cross-benchmark comparison of 87 learning to rank methods",
  author  = "Tax, Niek and Bockting, Sander and Hiemstra, Djoerd",
  journal = "Information processing \& management",
  year    =  2015,
  issn    = "0306-4573"
}

@INPROCEEDINGS{Stanton2014-qi,
  title     = "Circumlocution in diagnostic medical queries",
  author    = "Stanton, Isabelle and Ieong, Samuel and Mishra, Nina",
  booktitle = "Proceedings of the 37th international ACM SIGIR conference on
               Research \& development in information retrieval",
  publisher = "ACM",
  pages     = "133–142",
  month     =  jul,
  year      =  2014
}

@INPROCEEDINGS{Soldaini2016-ay,
  title     = "Team {GU}-{IRLAB} at {CLEF} {eHealth} 2016: Task 3",
  author    = "Soldaini, Luca and Edman, Will and Goharian, Nazli",
  booktitle = "clef",
  year      =  2016
}

@INPROCEEDINGS{Bendersky2010-gv,
  title     = "Learning Concept Importance Using a Weighted Dependence Model",
  author    = "Bendersky, Michael and Metzler, Donald and Croft, W Bruce",
  booktitle = "WSDM",
  year      =  2010
}

@TECHREPORT{Soldaini2014-jz,
  title       = "Query reformulation for clinical decision support search",
  author      = "Soldaini, Luca and Cohan, Arman and Yates, Andrew and Goharian,
                 Nazli and Frieder, Ophir",
  institution = "DTIC Document",
  year        =  2014
}

@INPROCEEDINGS{Cohan2014-kb,
  title     = "On clinical decision support",
  author    = "Cohan, Arman and Soldaini, Luca and Yates, Andrew and Goharian,
               Nazli and Frieder, Ophir",
  booktitle = "Proceedings of the 5th ACM Conference on Bioinformatics,
               Computational Biology, and Health Informatics",
  publisher = "ACM",
  pages     = "651–652",
  year      =  2014
}

@ARTICLE{Cohan2015-ly,
  title   = "Matching Citation Text and Cited Spans in Biomedical Literature: a
             Search-Oriented Approach",
  author  = "Cohan, Arman and Soldaini, Luca and Goharian, Nazli",
  journal = "North American Chapter of the Association for Computational
             Linguistics–Human Language Technologies (NAACL HLT 2015)",
  year    =  2015
}

@INPROCEEDINGS{Bendersky2011-ov,
  title     = "Parameterized concept weighting in verbose queries",
  author    = "Bendersky, Michael and Metzler, Donald and Croft, W Bruce",
  booktitle = "Proceedings of the 34th international ACM SIGIR conference on
               Research and development in Information Retrieval",
  publisher = "ACM",
  pages     = "605–614",
  month     =  jul,
  year      =  2011
}

@INPROCEEDINGS{Soldaini2015-vj,
  title     = "Retrieving medical literature for clinical decision support",
  author    = "Soldaini, Luca and Cohan, Arman and Yates, Andrew and Goharian,
               Nazli and Frieder, Ophir",
  booktitle = "European Conference on Information Retrieval",
  publisher = "Springer, Cham",
  pages     = "538–549",
  year      =  2015
}

@INPROCEEDINGS{Cohan2015-zi,
  title     = "Identifying Significance of Discrepancies in Radiology Reports",
  author    = "Cohan, Arman and Soldaini, Luca and Goharian, Nazli and Fong,
               Allan and Filice, Ross and Ratwani, Raj",
  booktitle = "5th Workshop on Data Mining for Medicine and Healthcare",
  pages     =  41,
  year      =  2015
}

@ARTICLE{Soldaini2016-aa,
  title   = "Team {GU}-{IRLAB} at {CLEF} {eHealth} 2016: Task 3",
  author  = "Soldaini, Luca and Edman, Will and Goharian, Nazli",
  journal = "Proceedings of CLEF eHealth 2016",
  year    =  2016
}

@INPROCEEDINGS{Soldaini2017-bm,
  title     = "Inferring individual attributes from search engine queries and
               auxiliary information",
  author    = "Soldaini, Luca and Yom-Tov, Elad",
  booktitle = "Proceedings of the 26th International Conference on World Wide
               Web",
  publisher = "International World Wide Web Conferences Steering Committee",
  pages     = "293–301",
  year      =  2017
}

@INPROCEEDINGS{Diaz2016-ja,
  title     = "Query Expansion with Locally-Trained Word Embeddings",
  author    = "Diaz, Fernando and Mitra, Bhaskar and Craswell, Nick",
  booktitle = "Proceedings of the 54th Annual Meeting of the Association for
               Computational Linguistics",
  pages     = "367–377",
  abstract  = "Continuous space word embeddings have received a great deal of
               attention in the natural language processing and machine learning
               communities for their ability to model term similarity and other
               relationships. We study the use of term relatedness in the
               context of query expansion for ad hoc information retrieval. We
               demonstrate that word embeddings such as word2vec and GloVe, when
               trained globally, underperform corpus and query specific
               embeddings for retrieval tasks. These results suggest that other
               tasks benefiting from global embeddings may also benefit from
               local embeddings.",
  month     =  may,
  year      =  2016
}

@ARTICLE{Rekabsaz2017-pf,
  title         = "Toward Incorporation of Relevant Documents in {word2vec}",
  author        = "Rekabsaz, Navid and Mitra, Bhaskar and Lupu, Mihai and
                   Hanbury, Allan",
  journal       = "arXiv [cs.IR]",
  abstract      = "Recent advances in neural word embedding provide significant
                   benefit to various information retrieval tasks. However as
                   shown by recent studies, adapting the embedding models for
                   the needs of IR tasks can bring considerable further
                   improvements. The embedding models in general define the term
                   relatedness by exploiting the terms' co-occurrences in
                   short-window contexts. An alternative (and well-studied)
                   approach in IR for related terms to a query is using local
                   information i.e. a set of top-retrieved documents. In view of
                   these two methods of term relatedness, in this work, we
                   report our study on incorporating the local information of
                   the query in the word embeddings. One main challenge in this
                   direction is that the dense vectors of word embeddings and
                   their estimation of term-to-term relatedness remain difficult
                   to interpret and hard to analyze. As an alternative, explicit
                   word representations propose vectors whose dimensions are
                   easily interpretable, and recent methods show competitive
                   performance to the dense vectors. We introduce a neural-based
                   explicit representation, rooted in the conceptual ideas of
                   the word2vec Skip-Gram model. The method provides
                   interpretable explicit vectors while keeping the
                   effectiveness of the Skip-Gram model. The evaluation of
                   various explicit representations on word association
                   collections shows that the newly proposed method out-
                   performs the state-of-the-art explicit representations when
                   tasked with ranking highly similar terms. Based on the
                   introduced ex- plicit representation, we discuss our
                   approaches on integrating local documents in globally-trained
                   embedding models and discuss the preliminary results.",
  month         =  jul,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "1707.06598"
}

@INPROCEEDINGS{Nguyen2017-mm,
  title     = "Learning Concept-Driven Document Embeddings for Medical
               Information Search",
  author    = "Nguyen, Gia-Hung and Tamine, Lynda and Soulier, Laure and Souf,
               Nathalie",
  booktitle = "Artificial Intelligence in Medicine",
  publisher = "Springer, Cham",
  pages     = "160–170",
  abstract  = "Many medical tasks such as self-diagnosis, health-care
               assessment, and clinical trial patient recruitment involve the
               usage of information access tools. A key underlying step to
               achieve such tasks is the document-to-document matching which
               mostly fails to bridge the gap identified between raw level
               representations of information in documents and high-level human
               interpretation. In this paper, we study how to optimize the
               document representation by leveraging neural-based approaches to
               capture latent representations built upon both validated medical
               concepts specified in an external resource as well as the used
               words. We experimentally show the effectiveness of our proposed
               model used as a support of two different medical search tasks,
               namely health search and clinical search for cohorts.",
  series    = "Lecture Notes in Computer Science",
  month     =  jun,
  year      =  2017,
  language  = "en"
}

@ARTICLE{Ammar2016-xz,
  title         = "Massively Multilingual Word Embeddings",
  author        = "Ammar, Waleed and Mulcaire, George and Tsvetkov, Yulia and
                   Lample, Guillaume and Dyer, Chris and Smith, Noah A",
  journal       = "arXiv [cs.CL]",
  abstract      = "We introduce new methods for estimating and evaluating
                   embeddings of words in more than fifty languages in a single
                   shared embedding space. Our estimation methods, multiCluster
                   and multiCCA, use dictionaries and monolingual data; they do
                   not require parallel data. Our new evaluation method,
                   multiQVEC-CCA, is shown to correlate better than previous
                   ones with two downstream tasks (text categorization and
                   parsing). We also describe a web portal for evaluation that
                   will facilitate further research in this area, along with
                   open-source releases of all our methods.",
  month         =  feb,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1602.01925"
}

@ARTICLE{Luong2015-ck,
  title    = "Bilingual Word Representations with Monolingual Quality in Mind",
  author   = "Luong, T and Pham, H and Manning, C D",
  journal  = "VS@ HLT-NAACL",
  abstract = "Abstract Recent work in learning bilingual representations tend to
              tailor towards achieving good performance on bilingual tasks, most
              often the crosslingual document classification (CLDC) evaluation,
              but to the detriment of preserving clustering structures of word
              representations monolingually. In this work, we propose a joint
              model to learn word representations from scratch that utilizes
              both the context coocurrence information through ...",
  year     =  2015
}

@INPROCEEDINGS{Nguyen2017-mw,
  title     = "{DSRIM}: A Deep Neural Information Retrieval Model Enhanced by a
               Knowledge Resource Driven Representation of Documents",
  author    = "Nguyen, Gia-Hung and Soulier, Laure and Tamine, Lynda and
               Bricon-Souf, Nathalie",
  booktitle = "Proceedings of the ACM SIGIR International Conference on Theory
               of Information Retrieval",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "19--26",
  abstract  = "The state-of-the-art solutions to the vocabulary mismatch in
               information retrieval (IR) mainly aim at leveraging either the
               relational semantics provided by external resources or the
               distributional semantics, recently investigated by deep neural
               approaches. Guided by the intuition that the relational semantics
               might improve the effectiveness of deep neural approaches, we
               propose the Deep Semantic Resource Inference Model (DSRIM) that
               relies on: 1) a representation of raw-data that models the
               relational semantics of text by jointly considering objects and
               relations expressed in a knowledge resource, and 2) an end-to-end
               neural architecture that learns the query-document relevance by
               leveraging the distributional and relational semantics of
               documents and queries. The experimental evaluation carried out on
               two TREC datasets from TREC Terabyte and TREC CDS tracks relying
               respectively on WordNet and MeSH resources, indicates that our
               model outperforms state-of-the-art semantic and deep neural IR
               models.",
  series    = "ICTIR '17",
  year      =  2017,
  keywords  = "ad-hoc ir, deep neural architecture, knowledge resource, semantic
               document representation",
  doi       = "10.1145/3121050.3121063",
  isbn      =  9781450344906
}

@INPROCEEDINGS{Riezler2007-fo,
  title     = "Statistical machine translation for query expansion in answer
               retrieval",
  author    = "Riezler, Stefan and Vasserman, Alexander and Tsochantaridis,
               Ioannis and Mittal, Vibhu and Liu, Yi",
  booktitle = "Annual Meeting-Association For Computational Linguistics",
  volume    =  45,
  pages     =  464,
  year      =  2007
}

@INPROCEEDINGS{Siencnik2015-hb,
  title     = "Adapting {word2vec} to named entity recognition",
  author    = "Sienčnik, Scharolta Katharina",
  booktitle = "Proceedings of the 20th Nordic Conference of Computational
               Linguistics, NODALIDA 2015, May 11-13, 2015, Vilnius, Lithuania",
  publisher = "Linköping University Electronic Press",
  pages     = "239–243",
  year      =  2015
}

@ARTICLE{Wick2016-us,
  title    = "Minimally-Constrained Multilingual Embeddings via Artificial
              Code-Switching",
  author   = "Wick, M and Kanani, P and Pocock, A C",
  journal  = "AAAI",
  abstract = "Abstract We present a method that consumes a large corpus of
              multilingual text and produces a single, unified word embedding in
              which the word vectors generalize across languages. In contrast to
              current approaches that require language identification, our
              method is agnostic about the languages with which the documents in
              the corpus are expressed, and does not rely on parallel corpora to
              constrain the spaces. Instead we ...",
  year     =  2016
}

@ARTICLE{Pedersen2007-gb,
  title    = "Measures of semantic similarity and relatedness in the biomedical
              domain",
  author   = "Pedersen, Ted and Pakhomov, Serguei V S and Patwardhan, Siddharth
              and Chute, Christopher G",
  journal  = "Journal of biomedical informatics",
  volume   =  40,
  number   =  3,
  pages    = "288–299",
  abstract = "Measures of semantic similarity between concepts are widely used
              in Natural Language Processing. In this article, we show how six
              existing domain-independent measures can be adapted to the
              biomedical domain. These measures were originally based on
              WordNet, an English lexical database of concepts and relations. In
              this research, we adapt these measures to the SNOMED-CT ontology
              of medical concepts. The measures include two path-based measures,
              and three measures that augment path-based measures with
              information content statistics from corpora. We also derive a
              context vector measure based on medical corpora that can be used
              as a measure of semantic relatedness. These six measures are
              evaluated against a newly created test bed of 30 medical concept
              pairs scored by three physicians and nine medical coders. We find
              that the medical coders and physicians differ in their ratings,
              and that the context vector measure correlates most closely with
              the physicians, while the path-based measures and one of the
              information content measures correlates most closely with the
              medical coders. We conclude that there is a role both for more
              flexible measures of relatedness based on information derived from
              corpora, as well as for measures that rely on existing ontological
              structures.",
  month    =  jun,
  year     =  2007,
  issn     = "1532-0464",
  language = "en"
}

@ARTICLE{Faruqui2014-gl,
  title         = "Retrofitting Word Vectors to Semantic Lexicons",
  author        = "Faruqui, Manaal and Dodge, Jesse and Jauhar, Sujay K and
                   Dyer, Chris and Hovy, Eduard and Smith, Noah A",
  journal       = "arXiv [cs.CL]",
  abstract      = "Vector space word representations are learned from
                   distributional information of words in large corpora.
                   Although such statistics are semantically informative, they
                   disregard the valuable information that is contained in
                   semantic lexicons such as WordNet, FrameNet, and the
                   Paraphrase Database. This paper proposes a method for
                   refining vector space representations using relational
                   information from semantic lexicons by encouraging linked
                   words to have similar vector representations, and it makes no
                   assumptions about how the input vectors were constructed.
                   Evaluated on a battery of standard lexical semantic
                   evaluation tasks in several languages, we obtain substantial
                   improvements starting with a variety of word vector models.
                   Our refinement method outperforms prior techniques for
                   incorporating semantic lexicons into the word vector training
                   algorithms.",
  month         =  nov,
  year          =  2014,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1411.4166"
}

@INPROCEEDINGS{Kilicoglu2016-uq,
  title     = "Annotating Named Entities in Consumer Health Questions",
  author    = "Kilicoglu, Halil and Abacha, Asma Ben and Mrabet, Yassine and
               Roberts, Kirk and Rodriguez, Laritza and Shooshan, Sonya E and
               Demner-Fushman, Dina",
  booktitle = "LREC",
  year      =  2016
}

@INPROCEEDINGS{Ghosh2016-pa,
  title     = "Characterizing Diseases from Unstructured Text: A Vocabulary
               Driven {Word2Vec} Approach",
  author    = "Ghosh, Saurav and Chakraborty, Prithwish and Cohn, Emily and
               Brownstein, John S and Ramakrishnan, Naren",
  booktitle = "Proceedings of the 25th ACM International on Conference on
               Information and Knowledge Management",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "1129--1138",
  abstract  = "Traditional disease surveillance can be augmented with a wide
               variety of real-time sources such as, news and social media.
               However, these sources are in general unstructured and,
               construction of surveillance tools such as taxonomical
               correlations and trace mapping involves considerable human
               supervision. In this paper, we motivate a disease vocabulary
               driven word2vec model (Dis2Vec) to model diseases and constituent
               attributes as word embeddings from the HealthMap news corpus. We
               use these word embeddings to automatically create disease
               taxonomies and evaluate our model against corresponding human
               annotated taxonomies. We compare our model accuracies against
               several state-of-the art word2vec methods. Our results
               demonstrate that Dis2Vec outperforms traditional distributed
               vector representations in its ability to faithfully capture
               taxonomical attributes across different class of diseases such as
               endemic, emerging and rare.",
  series    = "CIKM '16",
  year      =  2016,
  keywords  = "application-specific word embeddings, disease characterization,
               emerging diseases, healthmap, rare diseases",
  doi       = "10.1145/2983323.2983362",
  isbn      =  9781450340731
}

@INPROCEEDINGS{Lin2017-yf,
  title     = "Neural Relation Extraction with Multi-lingual Attention",
  author    = "Lin, Yankai and Liu, Zhiyuan and Sun, Maosong",
  booktitle = "Proceedings of the 55th Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "34–43",
  year      =  2017
}

@ARTICLE{Pakhomov2010-wy,
  title    = "Semantic Similarity and Relatedness between Clinical Terms: An
              Experimental Study",
  author   = "Pakhomov, Serguei and McInnes, Bridget and Adam, Terrence and Liu,
              Ying and Pedersen, Ted and Melton, Genevieve B",
  journal  = "AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA
              Symposium",
  volume   =  2010,
  pages    = "572–576",
  abstract = "Automated approaches to measuring semantic similarity and
              relatedness can provide necessary semantic context information for
              information retrieval applications and a number of fundamental
              natural language processing tasks including word sense
              disambiguation. Challenges for the development of these approaches
              include the limited availability of validated reference standards
              and the need for better understanding of the notions of semantic
              relatedness and similarity in medical vocabulary. We present
              results of a study in which eight medical residents were asked to
              judge 724 pairs of medical terms for semantic similarity and
              relatedness. The results of the study confirm the existence of a
              measurable mental representation of semantic relatedness between
              medical terms that is distinct from similarity and independent of
              the context in which the terms occur. This study produced a
              validated publicly available dataset for developing automated
              approaches to measuring semantic relatedness and similarity.",
  month    =  nov,
  year     =  2010,
  language = "en"
}

@INPROCEEDINGS{Ammar2017-kg,
  title     = "The {AI2} system at {SemEval}-2017 Task 10 ({ScienceIE}):
               semi-supervised end-to-end entity and relation extraction",
  author    = "Ammar, Waleed and Peters, Matthew E and Bhagavatula, Chandra and
               Power, Russell",
  booktitle = "Proceedings of SemEval 2017, Task 10, ScienceIE",
  year      =  2017
}

@INPROCEEDINGS{Mintz2009-ub,
  title     = "Distant Supervision for Relation Extraction Without Labeled Data",
  author    = "Mintz, Mike and Bills, Steven and Snow, Rion and Jurafsky, Dan",
  booktitle = "Proceedings of the Joint Conference of the 47th Annual Meeting of
               the ACL and the 4th International Joint Conference on Natural
               Language Processing of the AFNLP: Volume 2 - Volume 2",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "1003–1011",
  series    = "ACL '09",
  year      =  2009
}

@INPROCEEDINGS{Artetxe2017-zu,
  title     = "Learning bilingual word embeddings with (almost) no bilingual
               data",
  author    = "Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko",
  booktitle = "Proceedings of the 55th Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "451–462",
  year      =  2017
}

@ARTICLE{Nguyen2017-mi,
  title         = "Hierarchical Embeddings for Hypernymy Detection and
                   Directionality",
  author        = "Nguyen, Kim Anh and Köper, Maximilian and Walde, Sabine
                   Schulte im and Vu, Ngoc Thang",
  journal       = "arXiv [cs.CL]",
  abstract      = "We present a novel neural model HyperVec to learn
                   hierarchical embeddings for hypernymy detection and
                   directionality. While previous embeddings have shown
                   limitations on prototypical hypernyms, HyperVec represents an
                   unsupervised measure where embeddings are learned in a
                   specific order and capture the hypernym$-$hyponym
                   distributional hierarchy. Moreover, our model is able to
                   generalize over unseen hypernymy pairs, when using only small
                   sets of training data, and by mapping to other languages.
                   Results on benchmark datasets show that HyperVec outperforms
                   both state$-$of$-$the$-$art unsupervised measures and
                   embedding models on hypernymy detection and directionality,
                   and on predicting graded lexical entailment.",
  month         =  jul,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1707.07273"
}

@INPROCEEDINGS{Yao2010-sb,
  title     = "Collective Cross-document Relation Extraction Without Labelled
               Data",
  author    = "Yao, Limin and Riedel, Sebastian and McCallum, Andrew",
  booktitle = "Proceedings of the 2010 Conference on Empirical Methods in
               Natural Language Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "1013–1023",
  series    = "EMNLP '10",
  year      =  2010
}

@ARTICLE{Johnson2016-jg,
  title    = "{MIMIC}-{III}, a freely accessible critical care database",
  author   = "Johnson, Alistair E W and Pollard, Tom J and Shen, Lu and Lehman,
              Li-Wei H and Feng, Mengling and Ghassemi, Mohammad and Moody,
              Benjamin and Szolovits, Peter and Celi, Leo Anthony and Mark,
              Roger G",
  journal  = "Sci Data",
  volume   =  3,
  pages    =  160035,
  abstract = "MIMIC-III ('Medical Information Mart for Intensive Care') is a
              large, single-center database comprising information relating to
              patients admitted to critical care units at a large tertiary care
              hospital. Data includes vital signs, medications, laboratory
              measurements, observations and notes charted by care providers,
              fluid balance, procedure codes, diagnostic codes, imaging reports,
              hospital length of stay, survival data, and more. The database
              supports applications including academic and industrial research,
              quality improvement initiatives, and higher education coursework.",
  month    =  may,
  year     =  2016,
  language = "en"
}

@INPROCEEDINGS{Burl1994-ce,
  title     = "Automated analysis of radar imagery of Venus: handling lack of
               ground truth",
  author    = "Burl, M C and Fayyad, U M and Perona, P and Smyth, P",
  booktitle = "Proceedings of 1st International Conference on Image Processing",
  volume    =  3,
  pages     = "236–240 vol.3",
  abstract  = "Lack of verifiable ground truth is a common problem in remote
               sensing image analysis. For example, consider the synthetic
               aperture radar (SAR) image data of Venus obtained by the Magellan
               spacecraft. Planetary scientists are interested in automatically
               cataloging the locations of all the small volcanoes in this data
               set; however, the problem is very difficult and cannot be
               performed with perfect reliability even by human experts. Thus,
               training and evaluating the performance of an automatic algorithm
               on this data set must be handled carefully. We discuss the use of
               weighted free-response receiver-operating characteristics
               (wFROCs) for evaluating detection performance when the “ground
               truth” is subjective. In particular, we evaluate the relative
               detection performance of humans and automatic algorithms. Our
               experimental results indicate that proper assessment of the
               uncertainty in “ground truth” is essential in applications of
               this nature",
  month     =  nov,
  year      =  1994
}

@ARTICLE{Jin2018-on,
  title    = "Approximately optimizing {NDCG} using pair-wise loss",
  author   = "Jin, Xiao-Bo and Geng, Guang-Gang and Xie, Guo-Sen and Huang,
              Kaizhu",
  journal  = "Information sciences",
  volume   =  453,
  pages    = "50–65",
  abstract = "The Normalized Discounted Cumulative Gain (NDCG) is used to
              measure the performance of ranking algorithms. Much of the work on
              learning to rank by optimizing NDCG directly or indirectly is
              based on list-wise approaches. In our work, we approximately
              optimize a variant of NDCG called NDCGβ using pair-wise
              approaches. NDCGβ utilizes the linear discounting function. We
              first prove that the DCG error of NDCGβ is equal to the weighted
              pair-wise loss; then, on that basis, RankBoostndcg and RankSVMndcg
              are proposed to optimize the upper bound of the pair-wise 0–1 loss
              function. The experimental results from applying our approaches
              and ten other state-of-the-art methods to five public datasets
              show the superiority of the proposed methods, especially
              RankSVMndcg. In addition, RankBoostndcg are less influenced by the
              initial weight distribution.",
  month    =  jul,
  year     =  2018,
  issn     = "0020-0255"
}

@INPROCEEDINGS{Zeng2015-hw,
  title     = "Distant supervision for relation extraction via piecewise
               convolutional neural networks",
  author    = "Zeng, Daojian and Liu, Kang and Chen, Yubo and Zhao, Jun",
  booktitle = "Proceedings of the 2015 Conference on Empirical Methods in
               Natural Language Processing",
  pages     = "1753--1762",
  year      =  2015
}

@INPROCEEDINGS{Balaneshin-kordan2016-na,
  title     = "Optimization Method for Weighting Explicit and Latent Concepts in
               Clinical Decision Support Queries",
  author    = "Balaneshin-kordan, Saeid and Kotov, Alexander",
  booktitle = "Proceedings of the 2016 ACM International Conference on the
               Theory of Information Retrieval",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "241–250",
  abstract  = "Abstract Accurately answering verbose queries that describe a
               clinical case and aim at finding articles in a collection of
               medical literature requires capturing many explicit and latent
               aspects of complex information needs underlying such queries.
               Proper representation of",
  series    = "ICTIR '16",
  year      =  2016
}

@ARTICLE{Lipton2018-mi,
  title    = "Detecting and Correcting for Label Shift with Black Box Predictors",
  author   = "Lipton, Zachary C and Wang, Yu-Xiang and Smola, Alex",
  journal  = "arXiv [cs.LG]",
  abstract = "Faced with distribution shift between training and test set, we
              wish to detect and quantify the shift, and to correct our
              classifiers without test set labels. Motivated by medical
              diagnosis, where diseases (targets), cause symptoms
              (observations), we focus on label shift, where the label marginal
              $p(y)$ changes but the conditional $p(x|y)$ does not. We propose
              Black Box Shift Estimation (BBSE) to estimate the test
              distribution $p(y)$. BBSE exploits arbitrary black box predictors
              to reduce dimensionality prior to shift correction. While better
              predictors give tighter estimates, BBSE works even when predictors
              are biased, inaccurate, or uncalibrated, so long as their
              confusion matrices are invertible. We prove BBSE's consistency,
              bound its error, and introduce a statistical test that uses BBSE
              to detect shift. We also leverage BBSE to correct classifiers.
              Experiments demonstrate accurate estimates and improved
              prediction, even on high-dimensional datasets of natural images",
  month    =  feb,
  year     =  2018
}

@ARTICLE{Duchi2011-on,
  title   = "Adaptive Subgradient Methods for Online Learning and Stochastic
             Optimization",
  author  = "Duchi, John and Hazan, Elad and Singer, Yoram",
  journal = "Journal of machine learning research: JMLR",
  volume  =  12,
  pages   = "2121–2159",
  month   =  jul,
  year    =  2011,
  issn    = "1532-4435"
}

@INPROCEEDINGS{Ai2018-zh,
  title     = "Learning a Deep Listwise Context Model for Ranking Refinement",
  author    = "Ai, Qingyao and Bi, Keping and Guo, Jiafeng and Croft, W Bruce",
  publisher = "ACM Press",
  pages     = "135--144",
  abstract  = "Learning to rank has been intensively studied and widely applied
               in information retrieval. Typically, a global ranking function is
               learned from a set of labeled data, which can achieve good
               performance on average but may be suboptimal for individual
               queries by ignoring the fact that relevant documents for
               different queries may have different distributions in the feature
               space. Inspired by the idea of pseudo relevance feedback where
               top ranked documents, which we refer as the local ranking
               context, can provide important information about the query’s
               characteristics, we propose to use the inherent feature
               distributions of the top results to learn a Deep Listwise Context
               Model that helps us fine tune the initial ranked list.
               Specifically, we employ a recurrent neural network to
               sequentially encode the top results using their feature vectors,
               learn a local context model and use it to re-rank the top
               results. There are three merits with our model: (1) Our model can
               capture the local ranking context based on the complex
               interactions between top results using a deep neural network; (2)
               Our model can be built upon existing learning-to-rank methods by
               directly using their extracted feature vectors; (3) Our model is
               trained with an attention-based loss function, which is more
               effective and efficient than many existing listwise methods.
               Experimental results show that the proposed model can
               significantly improve the state-of-the-art learning to rank
               methods on benchmark retrieval corpora.",
  year      =  2018,
  doi       = "10.1145/3209978.3209985",
  isbn      =  9781450356572,
  language  = "en"
}

@INPROCEEDINGS{Jagerman2017-ot,
  title     = "Modeling Label Ambiguity for Neural List-Wise Learning to Rank",
  author    = "Jagerman, Rolf and Kiseleva, Julia and de Rijke, Maarten",
  booktitle = "SIGIR 2017 Workshop on Neural Information Retrieval (Neu-IR’17),",
  abstract  = "List-wise learning to rank methods are considered to be the
               stateof-the-art. One of the major problems with these methods is
               that the ambiguous nature of relevance labels in learning to rank
               data is ignored. Ambiguity of relevance labels refers to the
               phenomenon that multiple documents may be assigned the same
               relevance label for a given query, so that no preference order
               should be learned for those documents. In this paper we propose a
               novel sampling technique for computing a list-wise loss that can
               take into account this ambiguity. We show the e ectiveness of the
               proposed method by training a 3-layer deep neural network. We
               compare our new loss function to two strong baselines: ListNet
               and ListMLE. We show that our method generalizes better and signi
               cantly outperforms other methods on the validation and test sets.",
  month     =  aug,
  year      =  2017,
  language  = "en"
}

@INPROCEEDINGS{Jiang2018-fw,
  title     = "Learning Word Embeddings for Low-Resource Languages by {PU}
               Learning",
  author    = "Jiang, Chao and Yu, Hsiang-Fu and Hsieh, Cho-Jui and Chang,
               Kai-Wei",
  booktitle = "Proceedings of the 2018 Conference of the North American Chapter
               of the Association for Computational Linguistics: Human Language
               Technologies, Volume 1 (Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "New Orleans, Louisiana",
  pages     = "1024–1034",
  year      =  2018
}

@INPROCEEDINGS{Kim2018-xh,
  title     = "Efﬁcient Large-Scale Neural Domain Classiﬁcation with
               Personalized Attention",
  author    = "Kim, Young-Bum and Kim, Dongchan and Kumar, Anijishnu and
               Sarikaya, Ruhi",
  booktitle = "Proceedings of the 56th Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  pages     =  11,
  abstract  = "In this paper, we explore the task of mapping spoken language
               utterances to one of thousands of natural language understanding
               domains in intelligent personal digital assistants (IPDAs). This
               scenario is observed in mainstream IPDAs in industry that allow
               third parties to develop thousands of new domains to augment
               builtin ﬁrst party domains to rapidly increase domain coverage
               and overall IPDA capabilities. We propose a scalable neural model
               architecture with a shared encoder, a novel attention mechanism
               that incorporates personalization information and domain-speciﬁc
               classiﬁers that solves the problem efﬁciently. Our architecture
               is designed to efﬁciently accommodate incremental domain
               additions achieving two orders of magnitude speed up compared to
               full model retraining. We consider the practical constraints of
               real-time production systems, and design to minimize memory
               footprint and runtime latency. We demonstrate that incorporating
               personalization signiﬁcantly improves domain classiﬁcation
               accuracy in a setting with thousands of overlapping domains.",
  year      =  2018,
  language  = "en"
}

@INPROCEEDINGS{Resnik2015-mo,
  title     = "Beyond {LDA}: Exploring Supervised Topic Modeling for
               Depression-Related Language in Twitter",
  author    = "Resnik, Philip and Armstrong, William and Claudino, Leonardo and
               Nguyen, Thang and Nguyen, Viet-An and Boyd-Graber, Jordan",
  publisher = "Association for Computational Linguistics",
  pages     = "99--107",
  abstract  = "Topic models can yield insight into how depressed and
               non-depressed individuals use language differently. In this
               paper, we explore the use of supervised topic models in the
               analysis of linguistic signal for detecting depression, providing
               promising results using several models.",
  year      =  2015,
  doi       = "10.3115/v1/W15-1212",
  language  = "en"
}

@TECHREPORT{Pennebaker2015-be,
  title    = "The Development and Psychometric Properties of {LIWC2015}",
  author   = "Pennebaker, James W and Boyd, Ryan L and Jordan, Kayla and
              Blackburn, Kate",
  pages    =  26,
  year     =  2015,
  language = "en"
}

@INPROCEEDINGS{Perez-Rosas2017-kq,
  title     = "Understanding and Predicting Empathic Behavior in Counseling
               Therapy",
  author    = "Pérez-Rosas, Verónica and Mihalcea, Rada and Resnicow, Kenneth
               and Singh, Satinder and An, Lawrence",
  publisher = "Association for Computational Linguistics",
  pages     = "1426--1435",
  abstract  = "Counselor empathy is associated with better outcomes in
               psychology and behavioral counseling. In this paper, we explore
               several aspects pertaining to counseling interaction dynamics and
               their relation to counselor empathy during motivational
               interviewing encounters. Particularly, we analyze aspects such as
               participants’ engagement, participants’ verbal and nonverbal
               accommodation, as well as topics being discussed during the
               conversation, with the ﬁnal goal of identifying linguistic and
               acoustic markers of counselor empathy. We also show how we can
               use these ﬁndings alongside other raw linguistic and acoustic
               features to build accurate counselor empathy classiﬁers with
               accuracies of up to 80\%.",
  year      =  2017,
  doi       = "10.18653/v1/P17-1131",
  language  = "en"
}

@INPROCEEDINGS{Kumar2015-kp,
  title     = "Detecting Changes in Suicide Content Manifested in Social Media
               Following Celebrity Suicides",
  author    = "Kumar, Mrinal and Dredze, Mark and Coppersmith, Glen and De
               Choudhury, Munmun",
  publisher = "ACM Press",
  pages     = "85--94",
  abstract  = "The Werther effect describes the increased rate of completed or
               attempted suicides following the depiction of an individual’s
               suicide in the media, typically a celebrity. We present ﬁndings
               on the prevalence of this effect in an online platform:
               r/SuicideWatch on Reddit. We examine both the posting activity
               and post content after the death of ten high-proﬁle suicides.
               Posting activity increases following reports of celebrity
               suicides, and post content exhibits considerable changes that
               indicate increased suicidal ideation. Specifically, we observe
               that post-celebrity suicide content is more likely to be inward
               focused, manifest decreased social concerns, and laden with
               greater anxiety, anger, and negative emotion. Topic model
               analysis further reveals content in this period to switch to a
               more derogatory tone that bears evidence of self-harm and
               suicidal tendencies. We discuss the implications of our ﬁndings
               in enabling better community support to psychologically
               vulnerable populations, and the potential of building suicide
               prevention interventions following high-proﬁle suicides.",
  year      =  2015,
  doi       = "10.1145/2700171.2791026",
  isbn      =  9781450333955,
  language  = "en"
}

@INPROCEEDINGS{Milne2016-ss,
  title     = "{CLPsych} 2016 Shared Task: Triaging content in online
               peer-support forums",
  author    = "Milne, David N and Pink, Glen and Hachey, Ben and Calvo, Rafael A",
  booktitle = "Proceedings of the Third Workshop on Computational Linguistics
               and Clinical Psychology",
  publisher = "Association for Computational Linguistics",
  address   = "San Diego, CA, USA",
  pages     = "118–127",
  month     =  jun,
  year      =  2016
}

@INPROCEEDINGS{Khanpour2017-ms,
  title     = "Identifying Empathetic Messages in Online Health Communities",
  author    = "Khanpour, Hamed and Caragea, Cornelia and Biyani, Prakhar",
  booktitle = "Proceedings of the Eighth International Joint Conference on
               Natural Language Processing (Volume 2: Short Papers)",
  publisher = "Asian Federation of Natural Language Processing",
  address   = "Taipei, Taiwan",
  pages     = "246–251",
  abstract  = "Empathy captures one’s ability to correlate with and understand
               others’ emotional states and experiences. Messages with
               empathetic content are considered as one of the main advantages
               for joining online health communities due to their potential to
               improve people’s moods. Unfortunately, to this date, no
               computational studies exist that automatically identify
               empathetic messages in online health communities. We propose a
               combination of Convolutional Neural Networks (CNN) and Long Short
               Term Memory (LSTM) networks, and show that the proposed model
               outperforms each individual model (CNN and LSTM) as well as
               several baselines.",
  month     =  nov,
  year      =  2017
}

@INPROCEEDINGS{De_Choudhury2016-ag,
  title     = "Discovering Shifts to Suicidal Ideation from Mental Health
               Content in Social Media",
  author    = "De Choudhury, Munmun and Kiciman, Emre and Dredze, Mark and
               Coppersmith, Glen and Kumar, Mrinal",
  publisher = "ACM Press",
  pages     = "2098--2110",
  abstract  = "History of mental illness is a major factor behind suicide risk
               and ideation. However research efforts toward characterizing and
               forecasting this risk is limited due to the paucity of
               information regarding suicide ideation, exacerbated by the stigma
               of mental illness. This paper ﬁlls gaps in the literature by
               developing a statistical methodology to infer which individuals
               could undergo transitions from mental health discourse to
               suicidal ideation. We utilize semi-anonymous support communities
               on Reddit as unobtrusive data sources to infer the likelihood of
               these shifts. We develop language and interactional measures for
               this purpose, as well as a propensity score matching based
               statistical approach. Our approach allows us to derive distinct
               markers of shifts to suicidal ideation. These markers can be
               modeled in a prediction framework to identify individuals likely
               to engage in suicidal ideation in the future. We discuss societal
               and ethical implications of this research.",
  year      =  2016,
  doi       = "10.1145/2858036.2858207",
  isbn      =  9781450333627,
  language  = "en"
}

@INPROCEEDINGS{Coppersmith2016-lf,
  title     = "Exploratory Analysis of Social Media Prior to a Suicide Attempt",
  author    = "Coppersmith, Glen and Ngo, Kim and Leary, Ryan and Wood, Anthony",
  booktitle = "Proceedings of the Third Workshop on Computational Linguistics
               and Clinical Psychology",
  publisher = "Association for Computational Linguistics",
  address   = "San Diego, CA, USA",
  pages     = "106–117",
  month     =  jun,
  year      =  2016
}

@INPROCEEDINGS{Coppersmith2015-qg,
  title     = "Quantifying suicidal ideation via language usage on social media",
  author    = "Coppersmith, Glen and Leary, Ryan and Whyne, Eric and Wood, Tony",
  booktitle = "Joint Statistics Meetings Proceedings, Statistical Computing
               Section, JSM",
  year      =  2015
}

@INPROCEEDINGS{Yates2017-ku,
  title     = "Depression and Self-Harm Risk Assessment in Online Forums",
  author    = "Yates, Andrew and Cohan, Arman and Goharian, Nazli",
  booktitle = "Proceedings of the 2017 Conference on Empirical Methods in
               Natural Language Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Copenhagen, Denmark",
  pages     = "2968–2978",
  abstract  = "Users suffering from mental health conditions often turn to
               online resources for support, including specialized online
               support communities or general communities such as Twitter and
               Reddit. In this work, we present a framework for supporting and
               studying users in both types of communities. We propose methods
               for identifying posts in support communities that may indicate a
               risk of self-harm, and demonstrate that our approach outperforms
               strong previously proposed methods for identifying such posts.
               Self-harm is closely related to depression, which makes
               identifying depressed users on general forums a crucial related
               task. We introduce a large-scale general forum dataset consisting
               of users with self-reported depression diagnoses matched with
               control users. We show how our method can be applied to
               effectively identify depressed users from their use of language
               alone. We demonstrate that our method outperforms strong
               baselines on this general forum dataset.",
  month     =  sep,
  year      =  2017
}

@ARTICLE{Cohan2017-xc,
  title    = "Triaging content severity in online mental health forums",
  author   = "Cohan, Arman and Young, Sydney and Yates, Andrew and Goharian,
              Nazli",
  journal  = "Journal of the Association for Information Science and Technology",
  volume   =  68,
  number   =  11,
  pages    = "2675--2689",
  year     =  2017,
  doi      = "10.1002/asi.23865",
  issn     = "2330-1635",
  language = "en"
}

@INPROCEEDINGS{Soldaini2015-fi,
  title     = "Query Reformulation for Clinical Decision Support Search",
  author    = "Soldaini, Luca and Cohan, Arman and Yates, Andrew and Goharian,
               Nazli and Frieder, Ophir",
  booktitle = "The Twenty-Third Text REtrieval Conference Proceedings (TREC
               2014)",
  year      =  2015
}

@INPROCEEDINGS{Cohan2014-gp,
  title     = "On clinical decision support",
  author    = "Cohan, Arman and Soldaini, Luca and Yates, Andrew and Goharian,
               Nazli and Frieder, Ophir",
  publisher = "ACM Press",
  pages     = "651--652",
  abstract  = "Recent interest in search tools for Clinical Decision Support
               (CDS) has dramatically increased. These tools help clinicians
               assess a medical situation by providing actionable information in
               the form of a select few highly relevant recent medical papers.
               Unlike traditional search, which is designed to deal with short
               queries, queries in CDS are long and narrative. We investigate
               the utility of applying pseudo-relevance feedback (PRF), a query
               expansion method that performs well in keyword-based medical
               literature search to CDS search. Using the optimum combination of
               PRF parameters we obtained statistically significant retrieval
               efficiency improvement in terms of nDCG, over the baseline.",
  year      =  2014,
  doi       = "10.1145/2649387.2660820",
  isbn      =  9781450328944,
  language  = "en"
}

@INPROCEEDINGS{Cohan2014-kb,
  title     = "Towards Citation-Based Summarization of Biomedical Literature",
  author    = "Cohan, Arman and Soldaini, Luca and Mengle, Saket S R and
               Goharian, Nazli",
  booktitle = "Text Analysis Conference Proceedings (TAC 2014)",
  pages     =  8,
  abstract  = "Citation-based summarization is a form of technical summarization
               that uses citations to an article to form its summary. In
               biomedical literature, citations by themselves are not reliable
               to be used for summary as they fail to consider the context of
               the ﬁndings in the referenced article. One way to remedy such
               problem is to link citations to the related text spans in the
               reference article. The ultimate goal in TAC1 biomedical
               summarization track is to generate a citation-based summary,
               using both the citations and the context information. This paper
               describes our approach for ﬁnding the context information related
               to each citation and determining their discourse facet (Task 1 of
               the track). We approach this task as a search task, applying
               different query reformulation techniques for retrieving the
               relevant text spans. After ﬁnding the relevant spans, we classify
               each citation to a set of discourse facets to capture the
               structure of the referenced paper. While our results show 20\%
               improvement over the baseline, the efﬁciency of the system still
               leaves much room for improvement.",
  year      =  2014,
  language  = "en"
}

@ARTICLE{Soldaini2016-dl,
  title    = "Enhancing web search in the medical domain via query clarification",
  author   = "Soldaini, Luca and Yates, Andrew and Yom-Tov, Elad and Frieder,
              Ophir and Goharian, Nazli",
  journal  = "Information Retrieval Journal",
  volume   =  19,
  number   = "1-2",
  pages    = "149--173",
  abstract = "The majority of Internet users search for medical information
              online; however, many do not have an adequate medical vocabulary.
              Users might have diﬃculties ﬁnding the most authoritative and
              useful information because they are unfamiliar with the
              appropriate medical expressions describing their condition;
              consequently, they are unable to adequately satisfy their
              information need. We investigate the utility of bridging the gap
              between layperson and expert vocabularies; our approach adds the
              most appropriate expert expression to queries submitted by users,
              a task we call query clariﬁcation. We evaluated the impact of
              query clariﬁcation. Using three diﬀerent synonym mappings and
              conducting two task-based retrieval studies, users were asked to
              answer medically-related questions using interleaved results from
              a major search engine. Our results show that the proposed system
              was preferred by users and helped them answer medical concerns
              correctly more often, with up to a 7\% increase in correct answers
              over an unmodiﬁed query. Finally, we introduce a supervised
              classiﬁer to select the most appropriate synonym mapping for each
              query, which further increased the fraction of correct answers
              (12\%).",
  year     =  2016,
  doi      = "10.1007/s10791-015-9258-y",
  issn     = "1386-4564,1573-7659",
  language = "en"
}

@INPROCEEDINGS{Cohan2016-gm,
  title     = "Identifying Signiﬁcance of Discrepancies in Radiology Reports",
  author    = "Cohan, Arman and Soldaini, Luca and Fong, Allan and Filice, Ross
               and Goharian, Nazli and Ratwani, Raj",
  booktitle = "Workshop on Data Mining for Medicine and Healthcare",
  pages     =  8,
  abstract  = "At many teaching hospitals, it is common practice for on-call
               radiology residents to interpret radiology examinations; such
               reports are later reviewed and revised by an attending physician
               before being used for any decision making. In case there are
               substantial problems in the resident’s initial report, the
               resident is called and the problems are reviewed to prevent
               similar future reporting errors. However, due to the large volume
               of reports produced, attending physicians rarely discuss the
               problems side by side with residents, thus missing an educational
               opportunity. In this work, we introduce a pipeline to
               discriminate between reports with signiﬁcant discrepancies and
               those with non-signiﬁcant discrepancies. The former contain
               severe errors or mis-interpretations, thus representing a great
               learning opportunity for the resident; the latter presents only
               minor diﬀerences (often stylistic) and have a minor role in the
               education of a resident. By discriminating between the two, the
               proposed system could ﬂag those reports that an attending
               radiology should deﬁnitely review with residents under their
               supervision. We evaluated our approach on 350 manually annotated
               radiology reports sampled from a collection of tens of thousands.
               The proposed classiﬁer achieves an Area Under the Curve (AUC) of
               0.837, which represent a 14\% improvement over the baselines.
               Furthermore, the classiﬁer reduces the False Negative Rate (FNR)
               by 52\%, a desirable performance metric for any recall-oriented
               task such as the one studied in this work.",
  year      =  2016,
  language  = "en"
}

@INPROCEEDINGS{Soldaini2017-xx,
  title     = "Inferring Individual Attributes from Search Engine Queries and
               Auxiliary Information",
  author    = "Soldaini, Luca and Yom-Tov, Elad",
  publisher = "ACM Press",
  pages     = "293--301",
  abstract  = "Internet data has surfaced as a primary source for investigation
               of diﬀerent aspects of human behavior. A crucial step in such
               studies is ﬁnding a suitable cohort (i.e., a set of users) that
               shares a common trait of interest to researchers. However, direct
               identiﬁcation of users sharing this trait is often impossible, as
               the data available to researchers is usually anonymized to
               preserve user privacy. To facilitate research on speciﬁc topics
               of interest, especially in medicine, we introduce an algorithm
               for identifying a trait of interest in anonymous users. We
               illustrate how a small set of labeled examples, together with
               statistical information about the entire population, can be
               aggregated to obtain labels on unseen examples. We validate our
               approach using labeled data from the political domain.",
  year      =  2017,
  doi       = "10.1145/3038912.3052629",
  isbn      =  9781450349130,
  language  = "en"
}

@INPROCEEDINGS{Soldaini2016-bf,
  title     = "{QuickUMLS}: a fast, unsupervised approach for medical concept
               extraction",
  author    = "Soldaini, Luca and Goharian, Nazli",
  booktitle = "Proceedings of the MedIR Workshop",
  pages     =  4,
  abstract  = "Entity extraction is a fundamental step in many health
               informatics systems. In recent years, tools such as MetaMap and
               cTAKES have been widely used for medical concept extraction on
               medical literature and clinical notes; however, relatively little
               interest has been placed on their scalability to large datasets.
               In this work, we present QuickUMLS: a fast, unsupervised,
               approximate dictionary matching algorithm for medical concept
               extraction. The proposed method achieves similar precision and
               recall of state-of-the-art systems on two clinical notes corpora,
               and outperforms MetaMap and cTAKES on a dataset of consumer drug
               reviews. More importantly, it is up to 135 times faster than both
               systems.",
  year      =  2016,
  language  = "en"
}

@ARTICLE{Soldaini2017-rl,
  title    = "Learning to reformulate long queries for clinical decision support",
  author   = "Soldaini, Luca and Yates, Andrew and Goharian, Nazli",
  journal  = "Journal of the Association for Information Science and Technology",
  volume   =  68,
  number   =  11,
  pages    = "2602--2619",
  abstract = "The large volume of biomedical literature poses a serious problem
              for medical professionals, who are often struggling to keep
              current with it. At the same time, many health providers consider
              knowledge of the latest literature in their ﬁeld a key component
              for successful clinical practice. In this work, we introduce two
              systems designed to help retrieving medical literature. Both
              receive a long, discursive clinical note as input query, and
              return highly relevant literature that could be used in support of
              clinical practice. The ﬁrst system is an improved version of a
              method previously proposed by the authors; it combines pseudo
              relevance feedback and a domain speciﬁc term ﬁlter to reformulate
              the query. The second is an approach that uses a deep neural
              network to reformulate a clinical note. Both approaches were
              evaluated on the 2014 and 2015 TREC CDS datasets; in our tests,
              they outperform the previously proposed method by up to 28\% in
              inferred NDCG; furthermore, they are competitive with the state of
              the art, achieving up to 8\% improvement in inferred NDCG.",
  year     =  2017,
  doi      = "10.1002/asi.23924",
  issn     = "2330-1635",
  language = "en"
}

@INPROCEEDINGS{Soldaini2017-ty,
  title     = "Denoising Clinical Notes for Medical Literature Retrieval with
               Convolutional Neural Model",
  author    = "Soldaini, Luca and Yates, Andrew and Goharian, Nazli",
  publisher = "ACM Press",
  pages     = "2307--2310",
  year      =  2017,
  doi       = "10.1145/3132847.3133149",
  isbn      =  9781450349185,
  language  = "en"
}

@INPROCEEDINGS{Wang2018-fw,
  title     = "Key Terms Guided Expansion for Verbose Queries in Medical Domain",
  author    = "Wang, Yue and Fang, Hui",
  booktitle = "Information Retrieval Technology",
  publisher = "Springer International Publishing",
  pages     = "143--156",
  abstract  = "Due to the complex nature of medical concepts and information
               need, the queries tend to be verbose in medical domain. Verbose
               queries lead to sub-optimal performance since the current search
               engine promotes the results covering every query term, but not
               the truly important ones. Key term extraction has been studied to
               solve this problem, but another problem, i.e., vocabulary gap
               between query and documents, need to be discussed. Although
               various query expansion techniques have been well studied for the
               vocabulary gap problem, existing methods suffer different
               drawbacks such as inefficiency and expansion term mismatch. In
               this work, we propose to solve this problem by following the
               intuition that the surrounding contexts of the important terms in
               the original query should also be essential for retrieval.
               Specifically, we first identify the key terms from the verbose
               query and then locate the contexts of these key terms in the
               original document collection. The terms in the contexts are
               weighted and aggregated to select the expansion terms. We conduct
               experiments with five TREC data collections using the proposed
               methods. The results show that the improvement of the retrieval
               performance of proposed method is statistically significant
               comparing with the baseline methods.",
  year      =  2018,
  doi       = "10.1007/978-3-030-03520-4\_14"
}

@ARTICLE{Mikolov2017-uj,
  title         = "Advances in Pre-Training Distributed Word Representations",
  author        = "Mikolov, Tomas and Grave, Edouard and Bojanowski, Piotr and
                   Puhrsch, Christian and Joulin, Armand",
  journal       = "arXiv [cs.CL]",
  abstract      = "Many Natural Language Processing applications nowadays rely
                   on pre-trained word representations estimated from large text
                   corpora such as news collections, Wikipedia and Web Crawl. In
                   this paper, we show how to train high-quality word vector
                   representations by using a combination of known tricks that
                   are however rarely used together. The main result of our work
                   is the new set of publicly available pre-trained models that
                   outperform the current state of the art by a large margin on
                   a number of tasks.",
  month         =  dec,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1712.09405"
}

@ARTICLE{Johnson2016-bw,
  title         = "Google's Multilingual Neural Machine Translation System:
                   Enabling Zero-Shot Translation",
  author        = "Johnson, Melvin and Schuster, Mike and Le, Quoc V and Krikun,
                   Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat, Nikhil
                   and Viégas, Fernanda and Wattenberg, Martin and Corrado, Greg
                   and Hughes, Macduff and Dean, Jeffrey",
  journal       = "arXiv [cs.CL]",
  abstract      = "We propose a simple solution to use a single Neural Machine
                   Translation (NMT) model to translate between multiple
                   languages. Our solution requires no change in the model
                   architecture from our base system but instead introduces an
                   artificial token at the beginning of the input sentence to
                   specify the required target language. The rest of the model,
                   which includes encoder, decoder and attention, remains
                   unchanged and is shared across all languages. Using a shared
                   wordpiece vocabulary, our approach enables Multilingual NMT
                   using a single model without any increase in parameters,
                   which is significantly simpler than previous proposals for
                   Multilingual NMT. Our method often improves the translation
                   quality of all involved language pairs, even while keeping
                   the total number of model parameters constant. On the WMT'14
                   benchmarks, a single multilingual model achieves comparable
                   performance for English$\rightarrow$French and surpasses
                   state-of-the-art results for English$\rightarrow$German.
                   Similarly, a single multilingual model surpasses
                   state-of-the-art results for French$\rightarrow$English and
                   German$\rightarrow$English on WMT'14 and WMT'15 benchmarks
                   respectively. On production corpora, multilingual models of
                   up to twelve language pairs allow for better translation of
                   many individual pairs. In addition to improving the
                   translation quality of language pairs that the model was
                   trained with, our models can also learn to perform implicit
                   bridging between language pairs never seen explicitly during
                   training, showing that transfer learning and zero-shot
                   translation is possible for neural translation. Finally, we
                   show analyses that hints at a universal interlingua
                   representation in our models and show some interesting
                   examples when mixing languages.",
  month         =  nov,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1611.04558"
}

@ARTICLE{Devlin2018-hk,
  title         = "{BERT}: Pre-training of Deep Bidirectional Transformers for
                   Language Understanding",
  author        = "Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and
                   Toutanova, Kristina",
  journal       = "arXiv [cs.CL]",
  abstract      = "We introduce a new language representation model called BERT,
                   which stands for Bidirectional Encoder Representations from
                   Transformers. Unlike recent language representation models,
                   BERT is designed to pre-train deep bidirectional
                   representations by jointly conditioning on both left and
                   right context in all layers. As a result, the pre-trained
                   BERT representations can be fine-tuned with just one
                   additional output layer to create state-of-the-art models for
                   a wide range of tasks, such as question answering and
                   language inference, without substantial task-specific
                   architecture modifications. BERT is conceptually simple and
                   empirically powerful. It obtains new state-of-the-art results
                   on eleven natural language processing tasks, including
                   pushing the GLUE benchmark to 80.4\% (7.6\% absolute
                   improvement), MultiNLI accuracy to 86.7 (5.6\% absolute
                   improvement) and the SQuAD v1.1 question answering Test F1 to
                   93.2 (1.5\% absolute improvement), outperforming human
                   performance by 2.0\%.",
  month         =  oct,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1810.04805"
}

@ARTICLE{Radford2018-hz,
  title   = "Improving language understanding by generative pre-training",
  author  = "Radford, Alec and Narasimhan, Karthik and Salimans, Tim and
             Sutskever, Ilya",
  journal = "URL https://s3-us-west-2. amazonaws.
             com/openai-assets/research-covers/language-unsupervised/language\_
             understanding\_paper. pdf",
  year    =  2018
}

@ARTICLE{Hui2017-fu,
  title         = "{PACRR}: A Position-Aware Neural {IR} Model for Relevance
                   Matching",
  author        = "Hui, Kai and Yates, Andrew and Berberich, Klaus and de Melo,
                   Gerard",
  journal       = "arXiv [cs.IR]",
  abstract      = "In order to adopt deep learning for information retrieval,
                   models are needed that can capture all relevant information
                   required to assess the relevance of a document to a given
                   user query. While previous works have successfully captured
                   unigram term matches, how to fully employ position-dependent
                   information such as proximity and term dependencies has been
                   insufficiently explored. In this work, we propose a novel
                   neural IR model named PACRR aiming at better modeling
                   position-dependent interactions between a query and a
                   document. Extensive experiments on six years' TREC Web Track
                   data confirm that the proposed model yields better results
                   under multiple benchmarks.",
  month         =  apr,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "1704.03940"
}

@ARTICLE{Hui2017-de,
  title         = "Co-{PACRR}: A Context-Aware Neural {IR} Model for Ad-hoc
                   Retrieval",
  author        = "Hui, Kai and Yates, Andrew and Berberich, Klaus and de Melo,
                   Gerard",
  journal       = "arXiv [cs.IR]",
  abstract      = "Neural IR models, such as DRMM and PACRR, have achieved
                   strong results by successfully capturing relevance matching
                   signals. We argue that the context of these matching signals
                   is also important. Intuitively, when extracting, modeling,
                   and combining matching signals, one would like to consider
                   the surrounding text (local context) as well as other signals
                   from the same document that can contribute to the overall
                   relevance score. In this work, we highlight three potential
                   shortcomings caused by not considering context information
                   and propose three neural ingredients to address them: a
                   disambiguation component, cascade k-max pooling, and a
                   shuffling combination layer. Incorporating these components
                   into the PACRR model yields Co-PACRR, a novel context-aware
                   neural IR model. Extensive comparisons with established
                   models on Trec Web Track data confirm that the proposed model
                   can achieve superior search results. In addition, an ablation
                   analysis is conducted to gain insights into the impact of and
                   interactions between different components. We release our
                   code to enable future comparisons.",
  month         =  jun,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "1706.10192",
  doi           = "10.1145/nnnnnnn.nnnnnnn"
}

@INPROCEEDINGS{Fan2018-lp,
  title     = "Relation Extraction for Protein-protein Interactions Affected by
               Mutations",
  author    = "Fan, Ziling and Soldaini, Luca and Cohan, Arman and Goharian,
               Nazli",
  booktitle = "Proceedings of the 2018 ACM International Conference on
               Bioinformatics, Computational Biology, and Health Informatics",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "506--507",
  series    = "BCB '18",
  year      =  2018,
  keywords  = "information extraction, mutation detection, precision medicine,
               protein-protein interaction",
  doi       = "10.1145/3233547.3233617",
  isbn      =  9781450357944
}

@ARTICLE{Cohan2018-tv,
  title   = "{SMHD}: a Large-Scale Resource for Exploring Online Language Usage
             for Multiple Mental Health Conditions",
  author  = "Cohan, Arman and Desmet, Bart and Yates, Andrew and Soldaini, Luca
             and MacAvaney, Sean and Goharian, Nazli",
  journal = "Proceedings of the 27th International Conference on Computational
             Linguistics",
  pages   = "1485--1497",
  year    =  2018
}

@ARTICLE{MacAvaney2018-dn,
  title         = "{RSDD}-Time: Temporal Annotation of Self-Reported Mental
                   Health Diagnoses",
  author        = "MacAvaney, Sean and Desmet, Bart and Cohan, Arman and
                   Soldaini, Luca and Yates, Andrew and Zirikly, Ayah and
                   Goharian, Nazli",
  journal       = "arXiv [cs.CL]",
  abstract      = "Self-reported diagnosis statements have been widely employed
                   in studying language related to mental health in social
                   media. However, existing research has largely ignored the
                   temporality of mental health diagnoses. In this work, we
                   introduce RSDD-Time: a new dataset of 598 manually annotated
                   self-reported depression diagnosis posts from Reddit that
                   include temporal information about the diagnosis. Annotations
                   include whether a mental health condition is present and how
                   recently the diagnosis happened. Furthermore, we include
                   exact temporal spans that relate to the date of diagnosis.
                   This information is valuable for various computational
                   methods to examine mental health through social media because
                   one's mental health state is not static. We also test several
                   baseline classification and extraction approaches, which
                   suggest that extracting temporal information from
                   self-reported diagnosis statements is challenging.",
  month         =  jun,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1806.07916"
}

@MISC{Kurita_Keita2018-yk,
  title        = "An Overview of Normalization Methods in Deep Learning",
  author       = "{Kurita, Keita}",
  booktitle    = "Machine Learning Explained",
  abstract     = "Normalization in deep learning has always been a hot topic.
                  Getting normalization right can be a crucial factor in getting
                  your model to train effectively, but this isn’t as easy as it
                  sounds…",
  month        =  nov,
  year         =  2018,
  howpublished = "\url{http://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/}",
  note         = "Accessed: 2018-12-5"
}

@ARTICLE{Thompson2018-da,
  title         = "Relevant Word Order Vectorization for Improved Natural
                   Language Processing in Electronic Healthcare Records",
  author        = "Thompson, Jeffrey and Hu, Jinxiang and Mudaranthakam, Dinesh
                   Pal and Streeter, David and Neums, Lisa and Park, Michele and
                   Koestler, Devin C and Gajewski, Byron and Mayo, Matthew S",
  journal       = "arXiv [cs.CL]",
  abstract      = "Objective: Electronic health records (EHR) represent a rich
                   resource for conducting observational studies, supporting
                   clinical trials, and more. However, much of the relevant
                   information is stored in an unstructured format that makes it
                   difficult to use. Natural language processing approaches that
                   attempt to automatically classify the data depend on
                   vectorization algorithms that impose structure on the text,
                   but these algorithms were not designed for the unique
                   characteristics of EHR. Here, we propose a new algorithm for
                   structuring so-called free-text that may help researchers
                   make better use of EHR. We call this method Relevant Word
                   Order Vectorization (RWOV). Materials and Methods: As a
                   proof-of-concept, we attempted to classify the hormone
                   receptor status of breast cancer patients treated at the
                   University of Kansas Medical Center during a recent year,
                   from the unstructured text of pathology reports. Our approach
                   attempts to account for the semi-structured way that
                   healthcare providers often enter information. We compared
                   this approach to the ngrams and word2vec methods. Results:
                   Our approach resulted in the most consistently high accuracy,
                   as measured by F1 score and area under the receiver operating
                   characteristic curve (AUC). Discussion: Our results suggest
                   that methods of structuring free text that take into account
                   its context may show better performance, and that our
                   approach is promising. Conclusion: By using a method that
                   accounts for the fact that healthcare providers tend to use
                   certain key words repetitively and that the order of these
                   key words is important, we showed improved performance over
                   methods that do not.",
  month         =  dec,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1812.02627"
}

@ARTICLE{Shang2017-gf,
  title         = "Automated Phrase Mining from Massive Text Corpora",
  author        = "Shang, Jingbo and Liu, Jialu and Jiang, Meng and Ren, Xiang
                   and Voss, Clare R and Han, Jiawei",
  journal       = "arXiv [cs.CL]",
  abstract      = "As one of the fundamental tasks in text analysis, phrase
                   mining aims at extracting quality phrases from a text corpus.
                   Phrase mining is important in various tasks such as
                   information extraction/retrieval, taxonomy construction, and
                   topic modeling. Most existing methods rely on complex,
                   trained linguistic analyzers, and thus likely have
                   unsatisfactory performance on text corpora of new domains and
                   genres without extra but expensive adaption. Recently, a few
                   data-driven methods have been developed successfully for
                   extraction of phrases from massive domain-specific text.
                   However, none of the state-of-the-art models is fully
                   automated because they require human experts for designing
                   rules or labeling phrases. Since one can easily obtain many
                   quality phrases from public knowledge bases to a scale that
                   is much larger than that produced by human experts, in this
                   paper, we propose a novel framework for automated phrase
                   mining, AutoPhrase, which leverages this large amount of
                   high-quality phrases in an effective way and achieves better
                   performance compared to limited human labeled phrases. In
                   addition, we develop a POS-guided phrasal segmentation model,
                   which incorporates the shallow syntactic information in
                   part-of-speech (POS) tags to further enhance the performance,
                   when a POS tagger is available. Note that, AutoPhrase can
                   support any language as long as a general knowledge base
                   (e.g., Wikipedia) in that language is available, while
                   benefiting from, but not requiring, a POS tagger. Compared to
                   the state-of-the-art methods, the new method has shown
                   significant improvements in effectiveness on five real-world
                   datasets across different domains and languages.",
  month         =  feb,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1702.04457"
}

@ARTICLE{Kumar2018-hs,
  title         = "Von Mises-Fisher Loss for Training Sequence to Sequence
                   Models with Continuous Outputs",
  author        = "Kumar, Sachin and Tsvetkov, Yulia",
  journal       = "arXiv [cs.CL]",
  abstract      = "The Softmax function is used in the final layer of nearly all
                   existing sequence-to-sequence models for language generation.
                   However, it is usually the slowest layer to compute which
                   limits the vocabulary size to a subset of most frequent
                   types; and it has a large memory footprint. We propose a
                   general technique for replacing the softmax layer with a
                   continuous embedding layer. Our primary innovations are a
                   novel probabilistic loss, and a training and inference
                   procedure in which we generate a probability distribution
                   over pre-trained word embeddings, instead of a multinomial
                   distribution over the vocabulary obtained via softmax. We
                   evaluate this new class of sequence-to-sequence models with
                   continuous outputs on the task of neural machine translation.
                   We show that our models obtain upto 2.5x speed-up in training
                   time while performing on par with the state-of-the-art models
                   in terms of translation quality. These models are capable of
                   handling very large vocabularies without compromising on
                   translation quality. They also produce more meaningful errors
                   than in the softmax-based models, as these errors typically
                   lie in a subspace of the vector space of the reference
                   translations.",
  month         =  dec,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1812.04616"
}

@ARTICLE{Zhang2014-fq,
  title    = "A Review on Multi-Label Learning Algorithms",
  author   = "Zhang, M and Zhou, Z",
  journal  = "IEEE transactions on knowledge and data engineering",
  volume   =  26,
  number   =  8,
  pages    = "1819--1837",
  abstract = "Multi-label learning studies the problem where each example is
              represented by a single instance while associated with a set of
              labels simultaneously. During the past decade, significant amount
              of progresses have been made toward this emerging machine learning
              paradigm. This paper aims to provide a timely review on this area
              with emphasis on state-of-the-art multi-label learning algorithms.
              Firstly, fundamentals on multi-label learning including formal
              definition and evaluation metrics are given. Secondly and
              primarily, eight representative multi-label learning algorithms
              are scrutinized under common notations with relevant analyses and
              discussions. Thirdly, several related learning settings are
              briefly summarized. As a conclusion, online resources and open
              research problems on multi-label learning are outlined for
              reference purposes.",
  month    =  aug,
  year     =  2014,
  keywords = "learning (artificial intelligence);multilabel learning
              algorithms;instance learning;machine learning paradigm;formal
              definition;evaluation metrics;learning
              settings;Training;Correlation;Supervised
              learning;Semantics;Machine learning algorithms;Algorithm design
              and analysis;Vectors;Computing Methodologies;Artificial
              Intelligence;Learning;Information Technology and Systems;Database
              Management;Database Applications;Data mining;Multi-label
              learning;label correlations;problem transformation;algorithm
              adaptation",
  doi      = "10.1109/TKDE.2013.39",
  issn     = "1041-4347"
}

@ARTICLE{Lee2018-qe,
  title         = "Coupled Representation Learning for Domains, Intents and
                   Slots in Spoken Language Understanding",
  author        = "Lee, Jihwan and Kim, Dongchan and Sarikaya, Ruhi and Kim,
                   Young-Bum",
  journal       = "arXiv [cs.CL]",
  abstract      = "Representation learning is an essential problem in a wide
                   range of applications and it is important for performing
                   downstream tasks successfully. In this paper, we propose a
                   new model that learns coupled representations of domains,
                   intents, and slots by taking advantage of their hierarchical
                   dependency in a Spoken Language Understanding system. Our
                   proposed model learns the vector representation of intents
                   based on the slots tied to these intents by aggregating the
                   representations of the slots. Similarly, the vector
                   representation of a domain is learned by aggregating the
                   representations of the intents tied to a specific domain. To
                   the best of our knowledge, it is the first approach to
                   jointly learning the representations of domains, intents, and
                   slots using their hierarchical relationships. The
                   experimental results demonstrate the effectiveness of the
                   representations learned by our model, as evidenced by
                   improved performance on the contextual cross-domain reranking
                   task.",
  month         =  dec,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1812.06083"
}

@INPROCEEDINGS{Cao2007-dd,
  title     = "Learning to Rank: From Pairwise Approach to Listwise Approach",
  author    = "Cao, Zhe and Qin, Tao and Liu, Tie-Yan and Tsai, Ming-Feng and
               Li, Hang",
  booktitle = "Proceedings of the 24th International Conference on Machine
               Learning",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "129--136",
  abstract  = "The paper is concerned with learning to rank, which is to
               construct a model or a function for ranking objects. Learning to
               rank is useful for document retrieval, collaborative filtering,
               and many other applications. Several methods for learning to rank
               have been proposed, which …",
  series    = "ICML '07",
  year      =  2007,
  doi       = "10.1145/1273496.1273513",
  isbn      =  9781595937933
}

@INPROCEEDINGS{Lee2017-ax,
  title     = "Adverse Drug Event Detection in Tweets with Semi-Supervised
               Convolutional Neural Networks",
  author    = "Lee, Kathy and Qadir, Ashequl and Hasan, Sadid A and Datla, Vivek
               and Prakash, Aaditya and Liu, Joey and Farri, Oladimeji",
  booktitle = "Proceedings of the 26th International Conference on World Wide
               Web",
  publisher = "International World Wide Web Conferences Steering Committee",
  pages     = "705--714",
  month     =  apr,
  year      =  2017,
  keywords  = "adverse drug events; healthcare; pharmacovigilance;
               semi-supervised convolutional neural networks; social media; text
               classification",
  doi       = "10.1145/3038912.3052671",
  isbn      =  9781450349130
}

@ARTICLE{Wu2015-pl,
  title    = "A Study of Neural Word Embeddings for Named Entity Recognition in
              Clinical Text",
  author   = "Wu, Yonghui and Xu, Jun and Jiang, Min and Zhang, Yaoyun and Xu,
              Hua",
  journal  = "AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA
              Symposium",
  volume   =  2015,
  pages    = "1326--1333",
  abstract = "Clinical Named Entity Recognition (NER) is a critical task for
              extracting important patient information from clinical text to
              support clinical and translational research. This study explored
              the neural word embeddings derived from a large unlabeled clinical
              corpus for clinical NER. We systematically compared two neural
              word embedding algorithms and three different strategies for
              deriving distributed word representations. Two neural word
              embeddings were derived from the unlabeled Multiparameter
              Intelligent Monitoring in Intensive Care (MIMIC) II corpus
              (403,871 notes). The results from both 2010 i2b2 and 2014 Semantic
              Evaluation (SemEval) data showed that the binarized word embedding
              features outperformed other strategies for deriving distributed
              word representations. The binarized embedding features improved
              the F1-score of the Conditional Random Fields based clinical NER
              system by 2.3\% on i2b2 data and 2.4\% on SemEval data. The
              combined feature from the binarized embeddings and the Brown
              clusters improved the F1-score of the clinical NER system by 2.9\%
              on i2b2 data and 2.7\% on SemEval data. Our study also showed that
              the distributed word embedding features derived from a large
              unlabeled corpus can be better than the widely used Brown
              clusters. Further analysis found that the neural word embeddings
              captured a wide range of semantic relations, which could be
              discretized into distributed word representations to benefit the
              clinical NER system. The low-cost distributed feature
              representation can be adapted to any other clinical natural
              language processing research.",
  month    =  nov,
  year     =  2015,
  pmc      = "PMC4765694",
  pmid     =  26958273,
  issn     = "1942-597X,1559-4076",
  language = "en"
}

@ARTICLE{Yang2019-fx,
  title         = "{XLNet}: Generalized Autoregressive Pretraining for Language
                   Understanding",
  author        = "Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell,
                   Jaime and Salakhutdinov, Ruslan and Le, Quoc V",
  journal       = "arXiv [cs.CL]",
  abstract      = "With the capability of modeling bidirectional contexts,
                   denoising autoencoding based pretraining like BERT achieves
                   better performance than pretraining approaches based on
                   autoregressive language modeling. However, relying on
                   corrupting the input with masks, BERT neglects dependency
                   between the masked positions and suffers from a
                   pretrain-finetune discrepancy. In light of these pros and
                   cons, we propose XLNet, a generalized autoregressive
                   pretraining method that (1) enables learning bidirectional
                   contexts by maximizing the expected likelihood over all
                   permutations of the factorization order and (2) overcomes the
                   limitations of BERT thanks to its autoregressive formulation.
                   Furthermore, XLNet integrates ideas from Transformer-XL, the
                   state-of-the-art autoregressive model, into pretraining.
                   Empirically, XLNet outperforms BERT on 20 tasks, often by a
                   large margin, and achieves state-of-the-art results on 18
                   tasks including question answering, natural language
                   inference, sentiment analysis, and document ranking.",
  month         =  jun,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1906.08237"
}

@ARTICLE{Sil2017-lj,
  title         = "Neural Cross-Lingual Entity Linking",
  author        = "Sil, Avirup and Kundu, Gourab and Florian, Radu and Hamza,
                   Wael",
  journal       = "arXiv [cs.CL]",
  abstract      = "A major challenge in Entity Linking (EL) is making effective
                   use of contextual information to disambiguate mentions to
                   Wikipedia that might refer to different entities in different
                   contexts. The problem exacerbates with cross-lingual EL which
                   involves linking mentions written in non-English documents to
                   entries in the English Wikipedia: to compare textual clues
                   across languages we need to compute similarity between
                   textual fragments across languages. In this paper, we propose
                   a neural EL model that trains fine-grained similarities and
                   dissimilarities between the query and candidate document from
                   multiple perspectives, combined with convolution and tensor
                   networks. Further, we show that this English-trained system
                   can be applied, in zero-shot learning, to other languages by
                   making surprisingly effective use of multi-lingual
                   embeddings. The proposed system has strong empirical evidence
                   yielding state-of-the-art results in English as well as
                   cross-lingual: Spanish and Chinese TAC 2015 datasets.",
  month         =  dec,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1712.01813"
}

@ARTICLE{Papagiannopoulou2019-zw,
  title         = "A Review of Keyphrase Extraction",
  author        = "Papagiannopoulou, Eirini and Tsoumakas, Grigorios",
  journal       = "arXiv [cs.CL]",
  abstract      = "Automated keyphrase extraction is a crucial textual
                   information processing task regarding the most types of
                   digital content management systems. It concerns the selection
                   of representative and characteristic phrases from a document
                   that express all aspects related to its content. This article
                   introduces the task of keyphrase extraction and provides a
                   view of existing work that is well organized and
                   comprehensive. Moreover, it discusses the different
                   evaluation approaches giving meaningful insights and
                   highlighting open issues. Finally, a comparative experimental
                   study for popular unsupervised techniques on five datasets is
                   presented.",
  month         =  may,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1905.05044"
}

@ARTICLE{Neelakantan2015-er,
  title         = "Learning Dictionaries for Named Entity Recognition using
                   Minimal Supervision",
  author        = "Neelakantan, Arvind and Collins, Michael",
  journal       = "arXiv [cs.CL]",
  abstract      = "This paper describes an approach for automatic construction
                   of dictionaries for Named Entity Recognition (NER) using
                   large amounts of unlabeled data and a few seed examples. We
                   use Canonical Correlation Analysis (CCA) to obtain lower
                   dimensional embeddings (representations) for candidate
                   phrases and classify these phrases using a small number of
                   labeled examples. Our method achieves 16.5\% and 11.3\% F-1
                   score improvement over co-training on disease and virus NER
                   respectively. We also show that by adding candidate phrase
                   embeddings as features in a sequence tagger gives better
                   performance compared to using word embeddings.",
  month         =  apr,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1504.06650"
}

@ARTICLE{P2014-lc,
  title         = "An Autoencoder Approach to Learning Bilingual Word
                   Representations",
  author        = "P, Sarath Chandar A and Lauly, Stanislas and Larochelle, Hugo
                   and Khapra, Mitesh M and Ravindran, Balaraman and Raykar,
                   Vikas and Saha, Amrita",
  journal       = "arXiv [cs.CL]",
  abstract      = "Cross-language learning allows us to use training data from
                   one language to build models for a different language. Many
                   approaches to bilingual learning require that we have
                   word-level alignment of sentences from parallel corpora. In
                   this work we explore the use of autoencoder-based methods for
                   cross-language learning of vectorial word representations
                   that are aligned between two languages, while not relying on
                   word-level alignments. We show that by simply learning to
                   reconstruct the bag-of-words representations of aligned
                   sentences, within and between languages, we can in fact learn
                   high-quality representations and do without word alignments.
                   Since training autoencoders on word observations presents
                   certain computational issues, we propose and compare
                   different variations adapted to this setting. We also propose
                   an explicit correlation maximizing regularizer that leads to
                   significant improvement in the performance. We empirically
                   investigate the success of our approach on the problem of
                   cross-language test classification, where a classifier
                   trained on a given language (e.g., English) must learn to
                   generalize to a different language (e.g., German). These
                   experiments demonstrate that our approaches are competitive
                   with the state-of-the-art, achieving up to 10-14 percentage
                   point improvements over the best reported results on this
                   task.",
  month         =  feb,
  year          =  2014,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1402.1454"
}

@ARTICLE{Gomaa2013-lr,
  title     = "A survey of text similarity approaches",
  author    = "Gomaa, W H and Fahmy, A A",
  journal   = "International Journal of Computer Applications in Technology",
  publisher = "Citeseer",
  abstract  = "Measuring the similarity between words, sentences, paragraphs and
               documents is an important component in various tasks such as
               information retrieval, document clustering, word-sense
               disambiguation, automatic essay scoring, short answer grading,
               machine …",
  year      =  2013,
  issn      = "0952-8091"
}

@ARTICLE{Lee2017-vq,
  title         = "Transfer Learning for Named-Entity Recognition with Neural
                   Networks",
  author        = "Lee, Ji Young and Dernoncourt, Franck and Szolovits, Peter",
  journal       = "arXiv [cs.CL]",
  abstract      = "Recent approaches based on artificial neural networks (ANNs)
                   have shown promising results for named-entity recognition
                   (NER). In order to achieve high performances, ANNs need to be
                   trained on a large labeled dataset. However, labels might be
                   difficult to obtain for the dataset on which the user wants
                   to perform NER: label scarcity is particularly pronounced for
                   patient note de-identification, which is an instance of NER.
                   In this work, we analyze to what extent transfer learning may
                   address this issue. In particular, we demonstrate that
                   transferring an ANN model trained on a large labeled dataset
                   to another dataset with a limited number of labels improves
                   upon the state-of-the-art results on two different datasets
                   for patient note de-identification.",
  month         =  may,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1705.06273"
}

@ARTICLE{Ferragina2010-be,
  title         = "Fast and accurate annotation of short texts with Wikipedia
                   pages",
  author        = "Ferragina, Paolo and Scaiella, Ugo",
  journal       = "arXiv [cs.IR]",
  abstract      = "We address the problem of cross-referencing text fragments
                   with Wikipedia pages, in a way that synonymy and polysemy
                   issues are resolved accurately and efficiently. We take
                   inspiration from a recent flow of work [Cucerzan 2007,
                   Mihalcea and Csomai 2007, Milne and Witten 2008, Chakrabarti
                   et al 2009], and extend their scenario from the annotation of
                   long documents to the annotation of short texts, such as
                   snippets of search-engine results, tweets, news, blogs, etc..
                   These short and poorly composed texts pose new challenges in
                   terms of efficiency and effectiveness of the annotation
                   process, that we address by designing and engineering TAGME,
                   the first system that performs an accurate and on-the-fly
                   annotation of these short textual fragments. A large set of
                   experiments shows that TAGME outperforms state-of-the-art
                   algorithms when they are adapted to work on short texts and
                   it results fast and competitive on long texts.",
  month         =  jun,
  year          =  2010,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "1006.3498"
}

@ARTICLE{Lample2019-ah,
  title         = "Cross-lingual Language Model Pretraining",
  author        = "Lample, Guillaume and Conneau, Alexis",
  journal       = "arXiv [cs.CL]",
  abstract      = "Recent studies have demonstrated the efficiency of generative
                   pretraining for English natural language understanding. In
                   this work, we extend this approach to multiple languages and
                   show the effectiveness of cross-lingual pretraining. We
                   propose two methods to learn cross-lingual language models
                   (XLMs): one unsupervised that only relies on monolingual
                   data, and one supervised that leverages parallel data with a
                   new cross-lingual language model objective. We obtain
                   state-of-the-art results on cross-lingual classification,
                   unsupervised and supervised machine translation. On XNLI, our
                   approach pushes the state of the art by an absolute gain of
                   4.9\% accuracy. On unsupervised machine translation, we
                   obtain 34.3 BLEU on WMT'16 German-English, improving the
                   previous state of the art by more than 9 BLEU. On supervised
                   machine translation, we obtain a new state of the art of 38.5
                   BLEU on WMT'16 Romanian-English, outperforming the previous
                   best approach by more than 4 BLEU. Our code and pretrained
                   models will be made publicly available.",
  month         =  jan,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1901.07291"
}

@ARTICLE{Peters2019-rw,
  title         = "To Tune or Not to Tune? Adapting Pretrained Representations
                   to Diverse Tasks",
  author        = "Peters, Matthew and Ruder, Sebastian and Smith, Noah A",
  journal       = "arXiv [cs.CL]",
  abstract      = "While most previous work has focused on different pretraining
                   objectives and architectures for transfer learning, we ask
                   how to best adapt the pretrained model to a given target
                   task. We focus on the two most common forms of adaptation,
                   feature extraction (where the pretrained weights are frozen),
                   and directly fine-tuning the pretrained model. Our empirical
                   results across diverse NLP tasks with two state-of-the-art
                   models show that the relative performance of fine-tuning vs.
                   feature extraction depends on the similarity of the
                   pretraining and target tasks. We explore possible
                   explanations for this finding and provide a set of adaptation
                   guidelines for the NLP practitioner.",
  month         =  mar,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1903.05987"
}

@INCOLLECTION{Vaswani2017-fw,
  title     = "Attention is All you Need",
  author    = "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit,
               Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Ł Ukasz and
               Polosukhin, Illia",
  editor    = "Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and
               Fergus, R and Vishwanathan, S and Garnett, R",
  booktitle = "Advances in Neural Information Processing Systems 30",
  publisher = "Curran Associates, Inc.",
  pages     = "5998--6008",
  year      =  2017
}

@ARTICLE{Shazeer2017-qa,
  title         = "Outrageously Large Neural Networks: The Sparsely-Gated
                   Mixture-of-Experts Layer",
  author        = "Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof
                   and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean,
                   Jeff",
  journal       = "arXiv [cs.LG]",
  abstract      = "The capacity of a neural network to absorb information is
                   limited by its number of parameters. Conditional computation,
                   where parts of the network are active on a per-example basis,
                   has been proposed in theory as a way of dramatically
                   increasing model capacity without a proportional increase in
                   computation. In practice, however, there are significant
                   algorithmic and performance challenges. In this work, we
                   address these challenges and finally realize the promise of
                   conditional computation, achieving greater than 1000x
                   improvements in model capacity with only minor losses in
                   computational efficiency on modern GPU clusters. We introduce
                   a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting
                   of up to thousands of feed-forward sub-networks. A trainable
                   gating network determines a sparse combination of these
                   experts to use for each example. We apply the MoE to the
                   tasks of language modeling and machine translation, where
                   model capacity is critical for absorbing the vast quantities
                   of knowledge available in the training corpora. We present
                   model architectures in which a MoE with up to 137 billion
                   parameters is applied convolutionally between stacked LSTM
                   layers. On large language modeling and machine translation
                   benchmarks, these models achieve significantly better results
                   than state-of-the-art at lower computational cost.",
  month         =  jan,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1701.06538"
}

@ARTICLE{Shen2018-ut,
  title         = "Ordered Neurons: Integrating Tree Structures into Recurrent
                   Neural Networks",
  author        = "Shen, Yikang and Tan, Shawn and Sordoni, Alessandro and
                   Courville, Aaron",
  journal       = "arXiv [cs.CL]",
  abstract      = "Natural language is hierarchically structured: smaller units
                   (e.g., phrases) are nested within larger units (e.g.,
                   clauses). When a larger constituent ends, all of the smaller
                   constituents that are nested within it must also be closed.
                   While the standard LSTM architecture allows different neurons
                   to track information at different time scales, it does not
                   have an explicit bias towards modeling a hierarchy of
                   constituents. This paper proposes to add such an inductive
                   bias by ordering the neurons; a vector of master input and
                   forget gates ensures that when a given neuron is updated, all
                   the neurons that follow it in the ordering are also updated.
                   Our novel recurrent architecture, ordered neurons LSTM
                   (ON-LSTM), achieves good performance on four different tasks:
                   language modeling, unsupervised parsing, targeted syntactic
                   evaluation, and logical inference.",
  month         =  oct,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1810.09536"
}

@INPROCEEDINGS{Ettinger2016-gg,
  title     = "Retrofitting sense-specific word vectors using parallel text",
  author    = "Ettinger, Allyson and Resnik, Philip and Carpuat, Marine",
  booktitle = "Proceedings of the 2016 Conference of the North American Chapter
               of the Association for Computational Linguistics: Human Language
               Technologies",
  publisher = "aclweb.org",
  pages     = "1378--1383",
  abstract  = "… to retrofit existing word vector representations and create a
               sense-based vec … Retrofitting sense-specific embed- dings using
               only 300k sentence pairs, which repre- sent about 5\% of the
               total training … Improving statisti- cal machine translation
               using word sense disambigua- tion …",
  year      =  2016
}

@ARTICLE{Song2019-ek,
  title         = "{MASS}: Masked Sequence to Sequence Pre-training for Language
                   Generation",
  author        = "Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and
                   Liu, Tie-Yan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Pre-training and fine-tuning, e.g., BERT, have achieved great
                   success in language understanding by transferring knowledge
                   from rich-resource pre-training task to the low/zero-resource
                   downstream tasks. Inspired by the success of BERT, we propose
                   MAsked Sequence to Sequence pre-training (MASS) for the
                   encoder-decoder based language generation tasks. MASS adopts
                   the encoder-decoder framework to reconstruct a sentence
                   fragment given the remaining part of the sentence: its
                   encoder takes a sentence with randomly masked fragment
                   (several consecutive tokens) as input, and its decoder tries
                   to predict this masked fragment. In this way, MASS can
                   jointly train the encoder and decoder to develop the
                   capability of representation extraction and language
                   modeling. By further fine-tuning on a variety of
                   zero/low-resource language generation tasks, including neural
                   machine translation, text summarization and conversational
                   response generation (3 tasks and totally 8 datasets), MASS
                   achieves significant improvements over the baselines without
                   pre-training or with other pre-training methods. Specially,
                   we achieve the state-of-the-art accuracy (37.5 in terms of
                   BLEU score) on the unsupervised English-French translation,
                   even beating the early attention-based supervised model.",
  month         =  may,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1905.02450"
}

@INPROCEEDINGS{Joachims2017-fd,
  title     = "Unbiased Learning-to-Rank with Biased Feedback",
  author    = "Joachims, Thorsten and Swaminathan, Adith and Schnabel, Tobias",
  booktitle = "Proceedings of the Tenth ACM International Conference on Web
               Search and Data Mining",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "781--789",
  abstract  = "Implicit feedback (eg, clicks, dwell times, etc.) is an abundant
               source of data in human- interactive systems. While implicit
               feedback has many advantages (eg, it is inexpensive to collect,
               user centric, and timely), its inherent biases are a key obstacle
               to its effective use. For …",
  series    = "WSDM '17",
  year      =  2017,
  keywords  = "click models, implicit feedback, learning to rank, propensity
               weighting, ranking svm",
  doi       = "10.1145/3018661.3018699",
  isbn      =  9781450346757
}

@ARTICLE{Peters2019-bq,
  title         = "Sparse Sequence-to-Sequence Models",
  author        = "Peters, Ben and Niculae, Vlad and Martins, André F T",
  journal       = "arXiv [cs.CL]",
  abstract      = "Sequence-to-sequence models are a powerful workhorse of NLP.
                   Most variants employ a softmax transformation in both their
                   attention mechanism and output layer, leading to dense
                   alignments and strictly positive output probabilities. This
                   density is wasteful, making models less interpretable and
                   assigning probability mass to many implausible outputs. In
                   this paper, we propose sparse sequence-to-sequence models,
                   rooted in a new family of $\alpha$-entmax transformations,
                   which includes softmax and sparsemax as particular cases, and
                   is sparse for any $\alpha > 1$. We provide fast algorithms to
                   evaluate these transformations and their gradients, which
                   scale well for large vocabulary sizes. Our models are able to
                   produce sparse alignments and to assign nonzero probability
                   to a short list of plausible outputs, sometimes rendering
                   beam search exact. Experiments on morphological inflection
                   and machine translation reveal consistent gains over dense
                   models.",
  month         =  may,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1905.05702"
}

@ARTICLE{Mielke2019-tk,
  title         = "What Kind of Language Is Hard to Language-Model?",
  author        = "Mielke, Sebastian J and Cotterell, Ryan and Gorman, Kyle and
                   Roark, Brian and Eisner, Jason",
  journal       = "arXiv [cs.CL]",
  abstract      = "How language-agnostic are current state-of-the-art NLP tools?
                   Are there some types of language that are easier to model
                   with current methods? In prior work (Cotterell et al., 2018)
                   we attempted to address this question for language modeling,
                   and observed that recurrent neural network language models do
                   not perform equally well over all the high-resource European
                   languages found in the Europarl corpus. We speculated that
                   inflectional morphology may be the primary culprit for the
                   discrepancy. In this paper, we extend these earlier
                   experiments to cover 69 languages from 13 language families
                   using a multilingual Bible corpus. Methodologically, we
                   introduce a new paired-sample multiplicative mixed-effects
                   model to obtain language difficulty coefficients from
                   at-least-pairwise parallel corpora. In other words, the model
                   is aware of inter-sentence variation and can handle missing
                   data. Exploiting this model, we show that ``translationese''
                   is not any easier to model than natively written language in
                   a fair comparison. Trying to answer the question of what
                   features difficult languages have in common, we try and fail
                   to reproduce our earlier (Cotterell et al., 2018) observation
                   about morphological complexity and instead reveal far simpler
                   statistics of the data that seem to drive complexity in a
                   much larger sample.",
  month         =  jun,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1906.04726"
}

@ARTICLE{Sun2019-sn,
  title         = "{ERNIE}: Enhanced Representation through Knowledge
                   Integration",
  author        = "Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and
                   Chen, Xuyi and Zhang, Han and Tian, Xin and Zhu, Danxiang and
                   Tian, Hao and Wu, Hua",
  journal       = "arXiv [cs.CL]",
  abstract      = "We present a novel language representation model enhanced by
                   knowledge called ERNIE (Enhanced Representation through
                   kNowledge IntEgration). Inspired by the masking strategy of
                   BERT, ERNIE is designed to learn language representation
                   enhanced by knowledge masking strategies, which includes
                   entity-level masking and phrase-level masking. Entity-level
                   strategy masks entities which are usually composed of
                   multiple words.Phrase-level strategy masks the whole phrase
                   which is composed of several words standing together as a
                   conceptual unit.Experimental results show that ERNIE
                   outperforms other baseline methods, achieving new
                   state-of-the-art results on five Chinese natural language
                   processing tasks including natural language inference,
                   semantic similarity, named entity recognition, sentiment
                   analysis and question answering. We also demonstrate that
                   ERNIE has more powerful knowledge inference capacity on a
                   cloze test.",
  month         =  apr,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1904.09223"
}

@ARTICLE{Gashteovski2019-fm,
  title         = "{OPIEC}: An Open Information Extraction Corpus",
  author        = "Gashteovski, Kiril and Wanner, Sebastian and Hertling, Sven
                   and Broscheit, Samuel and Gemulla, Rainer",
  journal       = "arXiv [cs.CL]",
  abstract      = "Open information extraction (OIE) systems extract relations
                   and their arguments from natural language text in an
                   unsupervised manner. The resulting extractions are a valuable
                   resource for downstream tasks such as knowledge base
                   construction, open question answering, or event schema
                   induction. In this paper, we release, describe, and analyze
                   an OIE corpus called OPIEC, which was extracted from the text
                   of English Wikipedia. OPIEC complements the available OIE
                   resources: It is the largest OIE corpus publicly available to
                   date (over 340M triples) and contains valuable metadata such
                   as provenance information, confidence scores, linguistic
                   annotations, and semantic annotations including spatial and
                   temporal information. We analyze the OPIEC corpus by
                   comparing its content with knowledge bases such as DBpedia or
                   YAGO, which are also based on Wikipedia. We found that most
                   of the facts between entities present in OPIEC cannot be
                   found in DBpedia and/or YAGO, that OIE facts often differ in
                   the level of specificity compared to knowledge base facts,
                   and that OIE open relations are generally highly polysemous.
                   We believe that the OPIEC corpus is a valuable resource for
                   future research on automated knowledge base construction.",
  month         =  apr,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1904.12324"
}

@ARTICLE{Le2019-km,
  title         = "Distant Learning for Entity Linking with Automatic Noise
                   Detection",
  author        = "Le, Phong and Titov, Ivan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Accurate entity linkers have been produced for domains and
                   languages where annotated data (i.e., texts linked to a
                   knowledge base) is available. However, little progress has
                   been made for the settings where no or very limited amounts
                   of labeled data are present (e.g., legal or most scientific
                   domains). In this work, we show how we can learn to link
                   mentions without having any labeled examples, only a
                   knowledge base and a collection of unannotated texts from the
                   corresponding domain. In order to achieve this, we frame the
                   task as a multi-instance learning problem and rely on surface
                   matching to create initial noisy labels. As the learning
                   signal is weak and our surrogate labels are noisy, we
                   introduce a noise detection component in our model: it lets
                   the model detect and disregard examples which are likely to
                   be noisy. Our method, jointly learning to detect noise and
                   link entities, greatly outperforms the surface matching
                   baseline and for a subset of entity categories even
                   approaches the performance of supervised learning.",
  month         =  may,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1905.07189"
}

@ARTICLE{Liu2019-uq,
  title         = "Linguistic Knowledge and Transferability of Contextual
                   Representations",
  author        = "Liu, Nelson F and Gardner, Matt and Belinkov, Yonatan and
                   Peters, Matthew E and Smith, Noah A",
  journal       = "arXiv [cs.CL]",
  abstract      = "Contextual word representations derived from large-scale
                   neural language models are successful across a diverse set of
                   NLP tasks, suggesting that they encode useful and
                   transferable features of language. To shed light on the
                   linguistic knowledge they capture, we study the
                   representations produced by several recent pretrained
                   contextualizers (variants of ELMo, the OpenAI transformer
                   language model, and BERT) with a suite of seventeen diverse
                   probing tasks. We find that linear models trained on top of
                   frozen contextual representations are competitive with
                   state-of-the-art task-specific models in many cases, but fail
                   on tasks requiring fine-grained linguistic knowledge (e.g.,
                   conjunct identification). To investigate the transferability
                   of contextual word representations, we quantify differences
                   in the transferability of individual layers within
                   contextualizers, especially between recurrent neural networks
                   (RNNs) and transformers. For instance, higher layers of RNNs
                   are more task-specific, while transformer layers do not
                   exhibit the same monotonic trend. In addition, to better
                   understand what makes contextual word representations
                   transferable, we compare language model pretraining with
                   eleven supervised pretraining tasks. For any given task,
                   pretraining on a closely related task yields better
                   performance than language model pretraining (which is better
                   on average) when the pretraining dataset is fixed. However,
                   language model pretraining on more data gives the best
                   results.",
  month         =  mar,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1903.08855"
}

@ARTICLE{Gururangan2019-fd,
  title         = "Variational Pretraining for Semi-supervised Text
                   Classification",
  author        = "Gururangan, Suchin and Dang, Tam and Card, Dallas and Smith,
                   Noah A",
  journal       = "arXiv [cs.CL]",
  abstract      = "We introduce VAMPIRE, a lightweight pretraining framework for
                   effective text classification when data and computing
                   resources are limited. We pretrain a unigram document model
                   as a variational autoencoder on in-domain, unlabeled data and
                   use its internal states as features in a downstream
                   classifier. Empirically, we show the relative strength of
                   VAMPIRE against computationally expensive contextual
                   embeddings and other popular semi-supervised baselines under
                   low resource settings. We also find that fine-tuning to
                   in-domain data is crucial to achieving decent performance
                   from contextual embeddings when working with limited
                   supervision. We accompany this paper with code to pretrain
                   and use VAMPIRE embeddings in downstream tasks.",
  month         =  jun,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1906.02242"
}

@ARTICLE{Lin2019-op,
  title         = "Open Sesame: Getting Inside {BERT}'s Linguistic Knowledge",
  author        = "Lin, Yongjie and Tan, Yi Chern and Frank, Robert",
  journal       = "arXiv [cs.CL]",
  abstract      = "How and to what extent does BERT encode
                   syntactically-sensitive hierarchical information or
                   positionally-sensitive linear information? Recent work has
                   shown that contextual representations like BERT perform well
                   on tasks that require sensitivity to linguistic structure. We
                   present here two studies which aim to provide a better
                   understanding of the nature of BERT's representations. The
                   first of these focuses on the identification of
                   structurally-defined elements using diagnostic classifiers,
                   while the second explores BERT's representation of
                   subject-verb agreement and anaphor-antecedent dependencies
                   through a quantitative assessment of self-attention vectors.
                   In both cases, we find that BERT encodes positional
                   information about word tokens well on its lower layers, but
                   switches to a hierarchically-oriented encoding on higher
                   layers. We conclude then that BERT's representations do
                   indeed model linguistically relevant aspects of hierarchical
                   structure, though they do not appear to show the sharp
                   sensitivity to hierarchical structure that is found in human
                   processing of reflexive anaphora.",
  month         =  jun,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1906.01698"
}

@ARTICLE{Coenen2019-sd,
  title         = "Visualizing and Measuring the Geometry of {BERT}",
  author        = "Coenen, Andy and Reif, Emily and Yuan, Ann and Kim, Been and
                   Pearce, Adam and Viégas, Fernanda and Wattenberg, Martin",
  journal       = "arXiv [cs.LG]",
  abstract      = "Transformer architectures show significant promise for
                   natural language processing. Given that a single pretrained
                   model can be fine-tuned to perform well on many different
                   tasks, these networks appear to extract generally useful
                   linguistic features. A natural question is how such networks
                   represent this information internally. This paper describes
                   qualitative and quantitative investigations of one
                   particularly effective model, BERT. At a high level,
                   linguistic features seem to be represented in separate
                   semantic and syntactic subspaces. We find evidence of a
                   fine-grained geometric representation of word senses. We also
                   present empirical descriptions of syntactic representations
                   in both attention matrices and individual word embeddings, as
                   well as a mathematical argument to explain the geometry of
                   these representations.",
  month         =  jun,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1906.02715"
}

@ARTICLE{Logeswaran2019-wo,
  title         = "Zero-Shot Entity Linking by Reading Entity Descriptions",
  author        = "Logeswaran, Lajanugen and Chang, Ming-Wei and Lee, Kenton and
                   Toutanova, Kristina and Devlin, Jacob and Lee, Honglak",
  journal       = "arXiv [cs.CL]",
  abstract      = "We present the zero-shot entity linking task, where mentions
                   must be linked to unseen entities without in-domain labeled
                   data. The goal is to enable robust transfer to highly
                   specialized domains, and so no metadata or alias tables are
                   assumed. In this setting, entities are only identified by
                   text descriptions, and models must rely strictly on language
                   understanding to resolve the new entities. First, we show
                   that strong reading comprehension models pre-trained on large
                   unlabeled data can be used to generalize to unseen entities.
                   Second, we propose a simple and effective adaptive
                   pre-training strategy, which we term domain-adaptive
                   pre-training (DAP), to address the domain shift problem
                   associated with linking unseen entities in a new domain. We
                   present experiments on a new dataset that we construct for
                   this task and show that DAP improves over strong pre-training
                   baselines, including BERT. The data and code are available at
                   https://github.com/lajanugen/zeshel.",
  month         =  jun,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1906.07348"
}

@ARTICLE{Alt2019-rc,
  title         = "Fine-tuning Pre-Trained Transformer Language Models to
                   Distantly Supervised Relation Extraction",
  author        = "Alt, Christoph and Hübner, Marc and Hennig, Leonhard",
  journal       = "arXiv [cs.CL]",
  abstract      = "Distantly supervised relation extraction is widely used to
                   extract relational facts from text, but suffers from noisy
                   labels. Current relation extraction methods try to alleviate
                   the noise by multi-instance learning and by providing
                   supporting linguistic and contextual information to more
                   efficiently guide the relation classification. While
                   achieving state-of-the-art results, we observed these models
                   to be biased towards recognizing a limited set of relations
                   with high precision, while ignoring those in the long tail.
                   To address this gap, we utilize a pre-trained language model,
                   the OpenAI Generative Pre-trained Transformer (GPT) [Radford
                   et al., 2018]. The GPT and similar models have been shown to
                   capture semantic and syntactic features, and also a notable
                   amount of ``common-sense'' knowledge, which we hypothesize
                   are important features for recognizing a more diverse set of
                   relations. By extending the GPT to the distantly supervised
                   setting, and fine-tuning it on the NYT10 dataset, we show
                   that it predicts a larger set of distinct relation types with
                   high confidence. Manual and automated evaluation of our model
                   shows that it achieves a state-of-the-art AUC score of 0.422
                   on the NYT10 dataset, and performs especially well at higher
                   recall levels.",
  month         =  jun,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1906.08646"
}

@ARTICLE{Liu2019-ra,
  title         = "Incorporating Priors with Feature Attribution on Text
                   Classification",
  author        = "Liu, Frederick and Avci, Besim",
  journal       = "arXiv [cs.CL]",
  abstract      = "Feature attribution methods, proposed recently, help users
                   interpret the predictions of complex models. Our approach
                   integrates feature attributions into the objective function
                   to allow machine learning practitioners to incorporate priors
                   in model building. To demonstrate the effectiveness our
                   technique, we apply it to two tasks: (1) mitigating
                   unintended bias in text classifiers by neutralizing identity
                   terms; (2) improving classifier performance in a scarce data
                   setting by forcing the model to focus on toxic terms. Our
                   approach adds an L2 distance loss between feature
                   attributions and task-specific prior values to the objective.
                   Our experiments show that i) a classifier trained with our
                   technique reduces undesired model biases without a trade off
                   on the original task; ii) incorporating priors helps model
                   performance in scarce data settings.",
  month         =  jun,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1906.08286"
}

@ARTICLE{Lample2019-og,
  title         = "Large Memory Layers with Product Keys",
  author        = "Lample, Guillaume and Sablayrolles, Alexandre and Ranzato,
                   Marc'aurelio and Denoyer, Ludovic and Jégou, Hervé",
  journal       = "arXiv [cs.CL]",
  abstract      = "This paper introduces a structured memory which can be easily
                   integrated into a neural network. The memory is very large by
                   design and therefore significantly increases the capacity of
                   the architecture, by up to a billion parameters with a
                   negligible computational overhead. Its design and access
                   pattern is based on product keys, which enable fast and exact
                   nearest neighbor search. The ability to increase the number
                   of parameters while keeping the same computational budget
                   lets the overall system strike a better trade-off between
                   prediction accuracy and computation efficiency both at
                   training and test time. This memory layer allows us to tackle
                   very large scale language modeling tasks. In our experiments
                   we consider a dataset with up to 30 billion words, and we
                   plug our memory layer in a state-of-the-art transformer-based
                   architecture. In particular, we found that a memory augmented
                   model with only 12 layers outperforms a baseline transformer
                   model with 24 layers, while being twice faster at inference
                   time. We release our code for reproducibility purposes.",
  month         =  jul,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1907.05242"
}

@ARTICLE{Feng2019-db,
  title         = "Misleading Failures of Partial-input Baselines",
  author        = "Feng, Shi and Wallace, Eric and Boyd-Graber, Jordan",
  journal       = "arXiv [cs.LG]",
  abstract      = "Recent work establishes dataset difficulty and removes
                   annotation artifacts via partial-input baselines (e.g.,
                   hypothesis-only models for SNLI or question-only models for
                   VQA). When a partial-input baseline gets high accuracy, a
                   dataset is cheatable. However, the converse is not
                   necessarily true: the failure of a partial-input baseline
                   does not mean a dataset is free of artifacts. To illustrate
                   this, we first design artificial datasets which contain
                   trivial patterns in the full input that are undetectable by
                   any partial-input model. Next, we identify such artifacts in
                   the SNLI dataset - a hypothesis-only model augmented with
                   trivial patterns in the premise can solve 15\% of the
                   examples that are previously considered ``hard''. Our work
                   provides a caveat for the use of partial-input baselines for
                   dataset verification and creation.",
  month         =  may,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1905.05778"
}

@INPROCEEDINGS{Balasubramanian2010-gq,
  title     = "Exploring Reductions for Long Web Queries",
  author    = "Balasubramanian, Niranjan and Kumaran, Giridhar and Carvalho,
               Vitor R",
  booktitle = "Proceedings of the 33rd International ACM SIGIR Conference on
               Research and Development in Information Retrieval",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "571--578",
  series    = "SIGIR '10",
  year      =  2010,
  keywords  = "combining searches, learning to rank, query reformulation",
  doi       = "10.1145/1835449.1835545",
  isbn      =  9781450301534
}

@INPROCEEDINGS{Zhou2006-lx,
  title     = "{MaxMatcher}: Biological Concept Extraction Using Approximate
               Dictionary Lookup",
  author    = "Zhou, Xiaohua and Zhang, Xiaodan and Hu, Xiaohua",
  booktitle = "PRICAI 2006: Trends in Artificial Intelligence",
  publisher = "Springer Berlin Heidelberg",
  pages     = "1145--1149",
  abstract  = "Dictionary-based biological concept extraction is still the
               state-of-the-art approach to large-scale biomedical literature
               annotation and indexing. The exact dictionary lookup is a very
               simple approach, but always achieves low extraction recall
               because a biological term often has many variants while a
               dictionary is impossible to collect all of them. We propose a
               generic extraction approach, referred to as approximate
               dictionary lookup, to cope with term variations and implement it
               as an extraction system called MaxMatcher. The basic idea of this
               approach is to capture the significant words instead of all words
               to a particular concept. The new approach dramatically improves
               the extraction recall while maintaining the precision. In a
               comparative study on GENIA corpus, the recall of the new approach
               reaches a 57\% recall while the exact dictionary lookup only
               achieves a 26\% recall.",
  year      =  2006,
  doi       = "10.1007/978-3-540-36668-3\_150"
}

@INPROCEEDINGS{Cui2005-xp,
  title     = "Question Answering Passage Retrieval Using Dependency Relations",
  author    = "Cui, Hang and Sun, Renxu and Li, Keya and Kan, Min-Yen and Chua,
               Tat-Seng",
  booktitle = "Proceedings of the 28th Annual International ACM SIGIR Conference
               on Research and Development in Information Retrieval",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "400--407",
  series    = "SIGIR '05",
  year      =  2005,
  keywords  = "dependency parsing, passage retrieval, question answering",
  doi       = "10.1145/1076034.1076103",
  isbn      =  9781595930347
}

@INPROCEEDINGS{Soldaini2017-sx,
  title     = "Learning to Rank for Consumer Health Search: A Semantic Approach",
  author    = "Soldaini, Luca and Goharian, Nazli",
  booktitle = "Advances in Information Retrieval",
  publisher = "Springer International Publishing",
  pages     = "640--646",
  abstract  = "For many internet users, searching for health advice online is
               the first step in seeking treatment. We present a Learning to
               Rank system that uses a novel set of syntactic and semantic
               features to improve consumer health search. Our approach was
               evaluated on the 2016 CLEF eHealth dataset, outperforming the
               best method by 26.6\% in NDCG@10.",
  year      =  2017,
  doi       = "10.1007/978-3-319-56608-5\_60"
}

@INPROCEEDINGS{Sakai2019-cv,
  title     = "Which Diversity Evaluation Measures Are ``Good''?",
  author    = "Sakai, Tetsuya and Zeng, Zhaohao",
  booktitle = "Proceedings of the 42Nd International ACM SIGIR Conference on
               Research and Development in Information Retrieval",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "595--604",
  series    = "SIGIR'19",
  year      =  2019,
  keywords  = "evaluation measures, search result diversification, user
               preferences",
  doi       = "10.1145/3331184.3331215",
  isbn      =  9781450361729
}

@ARTICLE{Fan2019-gc,
  title         = "Reducing Transformer Depth on Demand with Structured Dropout",
  author        = "Fan, Angela and Grave, Edouard and Joulin, Armand",
  journal       = "arXiv [cs.LG]",
  abstract      = "Overparameterized transformer networks have obtained state of
                   the art results in various natural language processing tasks,
                   such as machine translation, language modeling, and question
                   answering. These models contain hundreds of millions of
                   parameters, necessitating a large amount of computation and
                   making them prone to overfitting. In this work, we explore
                   LayerDrop, a form of structured dropout, which has a
                   regularization effect during training and allows for
                   efficient pruning at inference time. In particular, we show
                   that it is possible to select sub-networks of any depth from
                   one large network without having to finetune them and with
                   limited impact on performance. We demonstrate the
                   effectiveness of our approach by improving the state of the
                   art on machine translation, language modeling, summarization,
                   question answering, and language understanding benchmarks.
                   Moreover, we show that our approach leads to small BERT-like
                   models of higher quality compared to training from scratch or
                   using distillation.",
  month         =  sep,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1909.11556"
}

@BOOK{Murphy2012-xi,
  title     = "Machine Learning: A Probabilistic Perspective",
  author    = "Murphy, Kevin P",
  publisher = "MIT Press",
  abstract  = "A comprehensive introduction to machine learning that uses
               probabilistic models and inference as a unifying approach.Today's
               Web-enabled deluge of electronic data calls for automated methods
               of data analysis. Machine learning provides these, developing
               methods that can automatically detect patterns in data and then
               use the uncovered patterns to predict future data. This textbook
               offers a comprehensive and self-contained introduction to the
               field of machine learning, based on a unified, probabilistic
               approach.The coverage combines breadth and depth, offering
               necessary background material on such topics as probability,
               optimization, and linear algebra as well as discussion of recent
               developments in the field, including conditional random fields,
               L1 regularization, and deep learning. The book is written in an
               informal, accessible style, complete with pseudo-code for the
               most important algorithms. All topics are copiously illustrated
               with color images and worked examples drawn from such application
               domains as biology, text processing, computer vision, and
               robotics. Rather than providing a cookbook of different heuristic
               methods, the book stresses a principled model-based approach,
               often using the language of graphical models to specify models in
               a concise and intuitive way. Almost all the models described have
               been implemented in a MATLAB software package—PMTK (probabilistic
               modeling toolkit)—that is freely available online. The book is
               suitable for upper-level undergraduates with an
               introductory-level college math background and beginning graduate
               students.",
  month     =  sep,
  year      =  2012,
  isbn      =  9780262304320,
  language  = "en"
}

@ARTICLE{Karpukhin2020-cn,
  title         = "Dense Passage Retrieval for Open-Domain Question Answering",
  author        = "Karpukhin, Vladimir and Oğuz, Barlas and Min, Sewon and
                   Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen,
                   Danqi and Yih, Wen-Tau",
  journal       = "arXiv [cs.CL]",
  abstract      = "Open-domain question answering relies on efficient passage
                   retrieval to select candidate contexts, where traditional
                   sparse vector space models, such as TF-IDF or BM25, are the
                   de facto method. In this work, we show that retrieval can be
                   practically implemented using dense representations alone,
                   where embeddings are learned from a small number of questions
                   and passages by a simple dual-encoder framework. When
                   evaluated on a wide range of open-domain QA datasets, our
                   dense retriever outperforms a strong Lucene-BM25 system
                   largely by 9\%-19\% absolute in terms of top-20 passage
                   retrieval accuracy, and helps our end-to-end QA system
                   establish new state-of-the-art on multiple open-domain QA
                   benchmarks.",
  month         =  apr,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2004.04906"
}

@ARTICLE{Eisenschlos2019-et,
  title         = "{MultiFiT}: Efficient Multi-lingual Language Model
                   Fine-tuning",
  author        = "Eisenschlos, Julian and Ruder, Sebastian and Czapla, Piotr
                   and Kardas, Marcin and Gugger, Sylvain and Howard, Jeremy",
  journal       = "arXiv [cs.CL]",
  abstract      = "Pretrained language models are promising particularly for
                   low-resource languages as they only require unlabelled data.
                   However, training existing models requires huge amounts of
                   compute, while pretrained cross-lingual models often
                   underperform on low-resource languages. We propose
                   Multi-lingual language model Fine-Tuning (MultiFiT) to enable
                   practitioners to train and fine-tune language models
                   efficiently in their own language. In addition, we propose a
                   zero-shot method using an existing pretrained cross-lingual
                   model. We evaluate our methods on two widely used
                   cross-lingual classification datasets where they outperform
                   models pretrained on orders of magnitude more data and
                   compute. We release all models and code.",
  month         =  sep,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1909.04761"
}

@INPROCEEDINGS{Bruch2019-ig,
  title     = "Revisiting Approximate Metric Optimization in the Age of Deep
               Neural Networks",
  author    = "Bruch, Sebastian and Zoghi, Masrour and Bendersky, Michael and
               Najork, Marc",
  booktitle = "Proceedings of the 42nd International ACM SIGIR Conference on
               Research and Development in Information Retrieval",
  publisher = "ACM",
  pages     = "1241--1244",
  month     =  jul,
  year      =  2019,
  keywords  = "deep neural networks for IR; direct ranking metric optimization;
               learning to rank",
  doi       = "10.1145/3331184.3331347",
  isbn      =  9781450361729
}

@ARTICLE{Wiegreffe2019-kg,
  title         = "Attention is not not Explanation",
  author        = "Wiegreffe, Sarah and Pinter, Yuval",
  journal       = "arXiv [cs.CL]",
  abstract      = "Attention mechanisms play a central role in NLP systems,
                   especially within recurrent neural network (RNN) models.
                   Recently, there has been increasing interest in whether or
                   not the intermediate representations offered by these modules
                   may be used to explain the reasoning for a model's
                   prediction, and consequently reach insights regarding the
                   model's decision-making process. A recent paper claims that
                   `Attention is not Explanation' (Jain and Wallace, 2019). We
                   challenge many of the assumptions underlying this work,
                   arguing that such a claim depends on one's definition of
                   explanation, and that testing it needs to take into account
                   all elements of the model, using a rigorous experimental
                   design. We propose four alternative tests to determine
                   when/whether attention can be used as explanation: a simple
                   uniform-weights baseline; a variance calibration based on
                   multiple random seed runs; a diagnostic framework using
                   frozen weights from pretrained models; and an end-to-end
                   adversarial attention training protocol. Each allows for
                   meaningful interpretation of attention mechanisms in RNN
                   models. We show that even when reliable adversarial
                   distributions can be found, they don't perform well on the
                   simple diagnostic, indicating that prior work does not
                   disprove the usefulness of attention mechanisms for
                   explainability.",
  month         =  aug,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1908.04626"
}

@BOOK{Bishop2006-gm,
  title     = "Pattern recognition and machine learning",
  author    = "Bishop, Christopher M",
  publisher = "Springer Science+ Business Media",
  abstract  = "Pattern recognition has its origins in engineering, whereas
               machine learning grew out of computer science. However, these
               activities can be viewed as two facets of the same field, and
               together they have undergone substantial development over the
               past ten years. In …",
  year      =  2006
}

@ARTICLE{Raffel2019-af,
  title         = "Exploring the Limits of Transfer Learning with a Unified
                   Text-to-Text Transformer",
  author        = "Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee,
                   Katherine and Narang, Sharan and Matena, Michael and Zhou,
                   Yanqi and Li, Wei and Liu, Peter J",
  journal       = "arXiv [cs.LG]",
  abstract      = "Transfer learning, where a model is first pre-trained on a
                   data-rich task before being fine-tuned on a downstream task,
                   has emerged as a powerful technique in natural language
                   processing (NLP). The effectiveness of transfer learning has
                   given rise to a diversity of approaches, methodology, and
                   practice. In this paper, we explore the landscape of transfer
                   learning techniques for NLP by introducing a unified
                   framework that converts every language problem into a
                   text-to-text format. Our systematic study compares
                   pre-training objectives, architectures, unlabeled datasets,
                   transfer approaches, and other factors on dozens of language
                   understanding tasks. By combining the insights from our
                   exploration with scale and our new ``Colossal Clean Crawled
                   Corpus'', we achieve state-of-the-art results on many
                   benchmarks covering summarization, question answering, text
                   classification, and more. To facilitate future work on
                   transfer learning for NLP, we release our dataset,
                   pre-trained models, and code.",
  month         =  oct,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1910.10683"
}

@MISC{Zadrozny2002-lc,
  title   = "Transforming classifier scores into accurate multiclass probability
             estimates",
  author  = "Zadrozny, Bianca and Elkan, Charles",
  journal = "Proceedings of the eighth ACM SIGKDD international conference on
             Knowledge discovery and data mining - KDD '02",
  year    =  2002,
  doi     = "10.1145/775047.775151"
}

@ARTICLE{Peskov2019-bp,
  title         = "Mitigating Noisy Inputs for Question Answering",
  author        = "Peskov, Denis and Barrow, Joe and Rodriguez, Pedro and
                   Neubig, Graham and Boyd-Graber, Jordan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Natural language processing systems are often downstream of
                   unreliable inputs: machine translation, optical character
                   recognition, or speech recognition. For instance, virtual
                   assistants can only answer your questions after understanding
                   your speech. We investigate and mitigate the effects of noise
                   from Automatic Speech Recognition systems on two factoid
                   Question Answering (QA) tasks. Integrating confidences into
                   the model and forced decoding of unknown words are
                   empirically shown to improve the accuracy of downstream
                   neural QA systems. We create and train models on a synthetic
                   corpus of over 500,000 noisy sentences and evaluate on two
                   human corpora from Quizbowl and Jeopardy! competitions.",
  month         =  aug,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1908.02914"
}

@ARTICLE{Garg2019-by,
  title         = "{TANDA}: Transfer and Adapt Pre-Trained Transformer Models
                   for Answer Sentence Selection",
  author        = "Garg, Siddhant and Vu, Thuy and Moschitti, Alessandro",
  journal       = "arXiv [cs.CL]",
  abstract      = "We propose TANDA, an effective technique for fine-tuning
                   pre-trained Transformer models for natural language tasks.
                   Specifically, we first transfer a pre-trained model into a
                   model for a general task by fine-tuning it with a large and
                   high-quality dataset. We then perform a second fine-tuning
                   step to adapt the transferred model to the target domain. We
                   demonstrate the benefits of our approach for answer sentence
                   selection, which is a well-known inference task in Question
                   Answering. We built a large scale dataset to enable the
                   transfer step, exploiting the Natural Questions dataset. Our
                   approach establishes the state of the art on two well-known
                   benchmarks, WikiQA and TREC-QA, achieving MAP scores of 92\%
                   and 94.3\%, respectively, which largely outperform the
                   previous highest scores of 83.4\% and 87.5\%, obtained in
                   very recent work. We empirically show that TANDA generates
                   more stable and robust models reducing the effort required
                   for selecting optimal hyper-parameters. Additionally, we show
                   that the transfer step of TANDA makes the adaptation step
                   more robust to noise. This enables a more effective use of
                   noisy datasets for fine-tuning. Finally, we also confirm the
                   positive impact of TANDA in an industrial setting, using
                   domain specific datasets subject to different types of noise.",
  month         =  nov,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1911.04118"
}

@ARTICLE{Zhang2018-nm,
  title         = "Accelerating Neural Transformer via an Average Attention
                   Network",
  author        = "Zhang, Biao and Xiong, Deyi and Su, Jinsong",
  journal       = "arXiv [cs.CL]",
  abstract      = "With parallelizable attention networks, the neural
                   Transformer is very fast to train. However, due to the
                   auto-regressive architecture and self-attention in the
                   decoder, the decoding procedure becomes slow. To alleviate
                   this issue, we propose an average attention network as an
                   alternative to the self-attention network in the decoder of
                   the neural Transformer. The average attention network
                   consists of two layers, with an average layer that models
                   dependencies on previous positions and a gating layer that is
                   stacked over the average layer to enhance the expressiveness
                   of the proposed attention network. We apply this network on
                   the decoder part of the neural Transformer to replace the
                   original target-side self-attention model. With masking
                   tricks and dynamic programming, our model enables the neural
                   Transformer to decode sentences over four times faster than
                   its original version with almost no loss in training time and
                   translation performance. We conduct a series of experiments
                   on WMT17 translation tasks, where on 6 different language
                   pairs, we obtain robust and consistent speed-ups in decoding.",
  month         =  may,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1805.00631"
}

@MISC{Xiao2019-sl,
  title   = "Sharing Attention Weights for Fast Transformer",
  author  = "Xiao, Tong and Li, Yinqiao and Zhu, Jingbo and Yu, Zhengtao and
             Liu, Tongran",
  journal = "Proceedings of the Twenty-Eighth International Joint Conference on
             Artificial Intelligence",
  year    =  2019,
  doi     = "10.24963/ijcai.2019/735"
}

@ARTICLE{Child2019-zy,
  title         = "Generating Long Sequences with Sparse Transformers",
  author        = "Child, Rewon and Gray, Scott and Radford, Alec and Sutskever,
                   Ilya",
  journal       = "arXiv [cs.LG]",
  abstract      = "Transformers are powerful sequence models, but require time
                   and memory that grows quadratically with the sequence length.
                   In this paper we introduce sparse factorizations of the
                   attention matrix which reduce this to $O(n \sqrt{n})$. We
                   also introduce a) a variation on architecture and
                   initialization to train deeper networks, b) the recomputation
                   of attention matrices to save memory, and c) fast attention
                   kernels for training. We call networks with these changes
                   Sparse Transformers, and show they can model sequences tens
                   of thousands of timesteps long using hundreds of layers. We
                   use the same architecture to model images, audio, and text
                   from raw bytes, setting a new state of the art for density
                   modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate
                   unconditional samples that demonstrate global coherence and
                   great diversity, and show it is possible in principle to use
                   self-attention to model sequences of length one million or
                   more.",
  month         =  apr,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1904.10509"
}

@ARTICLE{Li2019-ap,
  title         = "Specializing Word Embeddings (for Parsing) by Information
                   Bottleneck",
  author        = "Li, Xiang Lisa and Eisner, Jason",
  journal       = "arXiv [cs.CL]",
  abstract      = "Pre-trained word embeddings like ELMo and BERT contain rich
                   syntactic and semantic information, resulting in
                   state-of-the-art performance on various tasks. We propose a
                   very fast variational information bottleneck (VIB) method to
                   nonlinearly compress these embeddings, keeping only the
                   information that helps a discriminative parser. We compress
                   each word embedding to either a discrete tag or a continuous
                   vector. In the discrete version, our automatically compressed
                   tags form an alternative tag set: we show experimentally that
                   our tags capture most of the information in traditional POS
                   tag annotations, but our tag sequences can be parsed more
                   accurately at the same level of tag granularity. In the
                   continuous version, we show experimentally that moderately
                   compressing the word embeddings by our method yields a more
                   accurate parser in 8 of 9 languages, unlike simple
                   dimensionality reduction.",
  month         =  oct,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1910.00163"
}

@ARTICLE{Wang2016-js,
  title         = "A Compare-Aggregate Model for Matching Text Sequences",
  author        = "Wang, Shuohang and Jiang, Jing",
  journal       = "arXiv [cs.CL]",
  abstract      = "Many NLP tasks including machine comprehension, answer
                   selection and text entailment require the comparison between
                   sequences. Matching the important units between sequences is
                   a key to solve these problems. In this paper, we present a
                   general ``compare-aggregate'' framework that performs
                   word-level matching followed by aggregation using
                   Convolutional Neural Networks. We particularly focus on the
                   different comparison functions we can use to match two
                   vectors. We use four different datasets to evaluate the
                   model. We find that some simple comparison functions based on
                   element-wise operations can work better than standard neural
                   network and neural tensor network.",
  month         =  nov,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1611.01747"
}

@ARTICLE{Yoon2019-gh,
  title         = "A Compare-Aggregate Model with Latent Clustering for Answer
                   Selection",
  author        = "Yoon, Seunghyun and Dernoncourt, Franck and Kim, Doo Soon and
                   Bui, Trung and Jung, Kyomin",
  journal       = "arXiv [cs.CL]",
  abstract      = "In this paper, we propose a novel method for a sentence-level
                   answer-selection task that is a fundamental problem in
                   natural language processing. First, we explore the effect of
                   additional information by adopting a pretrained language
                   model to compute the vector representation of the input text
                   and by applying transfer learning from a large-scale corpus.
                   Second, we enhance the compare-aggregate model by proposing a
                   novel latent clustering method to compute additional
                   information within the target corpus and by changing the
                   objective function from listwise to pointwise. To evaluate
                   the performance of the proposed approaches, experiments are
                   performed with the WikiQA and TREC-QA datasets. The empirical
                   results demonstrate the superiority of our proposed approach,
                   which achieve state-of-the-art performance for both datasets.",
  month         =  may,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1905.12897"
}

@ARTICLE{Provilkov2019-sa,
  title         = "{BPE}-Dropout: Simple and Effective Subword Regularization",
  author        = "Provilkov, Ivan and Emelianenko, Dmitrii and Voita, Elena",
  journal       = "arXiv [cs.CL]",
  abstract      = "Subword segmentation is widely used to address the open
                   vocabulary problem in machine translation. The dominant
                   approach to subword segmentation is Byte Pair Encoding (BPE),
                   which keeps the most frequent words intact while splitting
                   the rare ones into multiple tokens. While multiple
                   segmentations are possible even with the same vocabulary, BPE
                   splits words into unique sequences; this may prevent a model
                   from better learning the compositionality of words and being
                   robust to segmentation errors. So far, the only way to
                   overcome this BPE imperfection, its deterministic nature, was
                   to create another subword segmentation algorithm (Kudo,
                   2018). In contrast, we show that BPE itself incorporates the
                   ability to produce multiple segmentations of the same word.
                   We introduce BPE-dropout - simple and effective subword
                   regularization method based on and compatible with
                   conventional BPE. It stochastically corrupts the segmentation
                   procedure of BPE, which leads to producing multiple
                   segmentations within the same fixed BPE framework. Using
                   BPE-dropout during training and the standard BPE during
                   inference improves translation quality up to 3 BLEU compared
                   to BPE and up to 0.9 BLEU compared to the previous subword
                   regularization.",
  month         =  oct,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1910.13267"
}

@ARTICLE{Liu2019-cc,
  title         = "{RoBERTa}: A Robustly Optimized {BERT} Pretraining Approach",
  author        = "Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei
                   and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis,
                   Mike and Zettlemoyer, Luke and Stoyanov, Veselin",
  journal       = "arXiv [cs.CL]",
  abstract      = "Language model pretraining has led to significant performance
                   gains but careful comparison between different approaches is
                   challenging. Training is computationally expensive, often
                   done on private datasets of different sizes, and, as we will
                   show, hyperparameter choices have significant impact on the
                   final results. We present a replication study of BERT
                   pretraining (Devlin et al., 2019) that carefully measures the
                   impact of many key hyperparameters and training data size. We
                   find that BERT was significantly undertrained, and can match
                   or exceed the performance of every model published after it.
                   Our best model achieves state-of-the-art results on GLUE,
                   RACE and SQuAD. These results highlight the importance of
                   previously overlooked design choices, and raise questions
                   about the source of recently reported improvements. We
                   release our models and code.",
  month         =  jul,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1907.11692"
}

@ARTICLE{Tyagi2019-wx,
  title         = "Fast Intent Classification for Spoken Language Understanding",
  author        = "Tyagi, Akshit and Sharma, Varun and Gupta, Rahul and Samson,
                   Lynn and Zhuang, Nan and Wang, Zihang and Campbell, Bill",
  journal       = "arXiv [cs.CL]",
  abstract      = "Spoken Language Understanding (SLU) systems consist of
                   several machine learning components operating together (e.g.
                   intent classification, named entity recognition and
                   resolution). Deep learning models have obtained state of the
                   art results on several of these tasks, largely attributed to
                   their better modeling capacity. However, an increase in
                   modeling capacity comes with added costs of higher latency
                   and energy usage, particularly when operating on low
                   complexity devices. To address the latency and computational
                   complexity issues, we explore a BranchyNet scheme on an
                   intent classification scheme within SLU systems. The
                   BranchyNet scheme when applied to a high complexity model,
                   adds exit points at various stages in the model allowing
                   early decision making for a set of queries to the SLU model.
                   We conduct experiments on the Facebook Semantic Parsing
                   dataset with two candidate model architectures for intent
                   classification. Our experiments show that the BranchyNet
                   scheme provides gains in terms of computational complexity
                   without compromising model accuracy. We also conduct
                   analytical studies regarding the improvements in the
                   computational cost, distribution of utterances that egress
                   from various exit points and the impact of adding more
                   complexity to models with the BranchyNet scheme.",
  month         =  dec,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1912.01728"
}

@ARTICLE{Teerapittayanon2017-nl,
  title         = "{BranchyNet}: Fast Inference via Early Exiting from Deep
                   Neural Networks",
  author        = "Teerapittayanon, Surat and McDanel, Bradley and Kung, H T",
  journal       = "arXiv [cs.NE]",
  abstract      = "Deep neural networks are state of the art methods for many
                   learning tasks due to their ability to extract increasingly
                   better features at each network layer. However, the improved
                   performance of additional layers in a deep network comes at
                   the cost of added latency and energy usage in feedforward
                   inference. As networks continue to get deeper and larger,
                   these costs become more prohibitive for real-time and
                   energy-sensitive applications. To address this issue, we
                   present BranchyNet, a novel deep network architecture that is
                   augmented with additional side branch classifiers. The
                   architecture allows prediction results for a large portion of
                   test samples to exit the network early via these branches
                   when samples can already be inferred with high confidence.
                   BranchyNet exploits the observation that features learned at
                   an early layer of a network may often be sufficient for the
                   classification of many data points. For more difficult
                   samples, which are expected less frequently, BranchyNet will
                   use further or all network layers to provide the best
                   likelihood of correct prediction. We study the BranchyNet
                   architecture using several well-known networks (LeNet,
                   AlexNet, ResNet) and datasets (MNIST, CIFAR10) and show that
                   it can both improve accuracy and significantly reduce the
                   inference time of the network.",
  month         =  sep,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.NE",
  eprint        = "1709.01686"
}

@INPROCEEDINGS{Gallagher2019-zz,
  title       = "Joint Optimization of Cascade Ranking Models",
  author      = "Gallagher, Luke and Chen, Ruey-Cheng and Blanco, Roi and
                 Culpepper, J Shane",
  booktitle   = "Proceedings of the Twelfth ACM International Conference on Web
                 Search and Data Mining",
  publisher   = "dl.acm.org",
  institution = "ACM",
  pages       = "15--23",
  abstract    = "Reducing excessive costs in feature acquisition and model
                 evaluation has been a long- standing challenge in
                 learning-to-rank systems. A cascaded ranking architecture turns
                 ranking into a pipeline of multiple stages, and has been shown
                 to be a powerful approach to balancing efficiency and
                 effectiveness trade-offs in large-scale search systems.
                 However, learning a cascade model is often complex, and usually
                 performed stagewise independently across the entire ranking
                 pipeline. In this work we show that learning a cascade ranking
                 …",
  year        =  2019
}

@ARTICLE{Kitaev2020-wc,
  title         = "Reformer: The Efficient Transformer",
  author        = "Kitaev, Nikita and Kaiser, Łukasz and Levskaya, Anselm",
  journal       = "arXiv [cs.LG]",
  abstract      = "Large Transformer models routinely achieve state-of-the-art
                   results on a number of tasks but training these models can be
                   prohibitively costly, especially on long sequences. We
                   introduce two techniques to improve the efficiency of
                   Transformers. For one, we replace dot-product attention by
                   one that uses locality-sensitive hashing, changing its
                   complexity from O($L^2$) to O($L\log L$), where $L$ is the
                   length of the sequence. Furthermore, we use reversible
                   residual layers instead of the standard residuals, which
                   allows storing activations only once in the training process
                   instead of $N$ times, where $N$ is the number of layers. The
                   resulting model, the Reformer, performs on par with
                   Transformer models while being much more memory-efficient and
                   much faster on long sequences.",
  month         =  jan,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2001.04451"
}

@ARTICLE{Fisas2015-mw,
  title   = "On the Discoursive Structure of Computer Graphics Research Papers",
  author  = "Fisas, Beatriz and Saggion, Horacio and Ronzano, Francesco",
  journal = "LAW@ NAACL-HLT",
  volume  =  2015,
  pages   = "42--51",
  year    =  2015
}

@ARTICLE{Fiesler2018-le,
  title     = "“participant” perceptions of Twitter research ethics",
  author    = "Fiesler, Casey and Proferes, Nicholas",
  journal   = "Social media + society",
  publisher = "SAGE Publications",
  volume    =  4,
  number    =  1,
  pages     =  205630511876336,
  abstract  = "Social computing systems such as Twitter present new research
               sites that have provided billions of data points to researchers.
               However, the availability of public social media data has also
               presented ethical challenges. As the research community works to
               create ethical norms, we should be considering users’ concerns as
               well. With this in mind, we report on an exploratory survey of
               Twitter users’ perceptions of the use of tweets in research.
               Within our survey sample, few users were previously aware that
               their public tweets could be used by researchers, and the
               majority felt that researchers should not be able to use tweets
               without consent. However, we find that these attitudes are highly
               contextual, depending on factors such as how the research is
               conducted or disseminated, who is conducting it, and what the
               study is about. The findings of this study point to potential
               best practices for researchers conducting observation and
               analysis of public data.",
  month     =  jan,
  year      =  2018,
  doi       = "10.1177/2056305118763366",
  issn      = "2056-3051",
  language  = "en"
}

@ARTICLE{Ayers2018-rr,
  title     = "Don't quote me: reverse identification of research participants
               in social media studies",
  author    = "Ayers, John W and Caputi, Theodore L and Nebeker, Camille and
               Dredze, Mark",
  journal   = "npj digital medicine",
  publisher = "Springer Science and Business Media LLC",
  volume    =  1,
  number    =  1,
  pages     =  30,
  abstract  = "We investigated if participants in social media surveillance
               studies could be reverse identified by reviewing all articles
               published on PubMed in 2015 or 2016 with the words ``Twitter''
               and either ``read,'' ``coded,'' or ``content'' in the title or
               abstract. Seventy-two percent (95\% CI: 63-80) of articles quoted
               at least one participant's tweet and searching for the quoted
               content led to the participant 84\% (95\% CI: 74-91) of the time.
               Twenty-one percent (95\% CI: 13-29) of articles disclosed a
               participant's Twitter username thereby making the participant
               immediately identifiable. Only one article reported obtaining
               consent to disclose identifying information and institutional
               review board (IRB) involvement was mentioned in only 40\% (95\%
               CI: 31-50) of articles, of which 17\% (95\% CI: 10-25) received
               IRB-approval and 23\% (95\% CI:16-32) were deemed exempt.
               Biomedical publications are routinely including identifiable
               information by quoting tweets or revealing usernames which, in
               turn, violates ICMJE ethical standards governing scientific
               ethics, even though said content is scientifically unnecessary.
               We propose that authors convey aggregate findings without
               revealing participants' identities, editors refuse to publish
               reports that reveal a participant's identity, and IRBs attend to
               these privacy issues when reviewing studies involving social
               media data. These strategies together will ensure participants
               are protected going forward.",
  month     =  aug,
  year      =  2018,
  keywords  = "Epidemiology; Translational research",
  doi       = "10.1038/s41746-018-0036-2",
  pmc       = "PMC6550214",
  pmid      =  31304312,
  issn      = "2398-6352",
  language  = "en"
}

@ARTICLE{Jain2019-fg,
  title         = "Attention is not Explanation",
  author        = "Jain, Sarthak and Wallace, Byron C",
  journal       = "arXiv [cs.CL]",
  abstract      = "Attention mechanisms have seen wide adoption in neural NLP
                   models. In addition to improving predictive performance,
                   these are often touted as affording transparency: models
                   equipped with attention provide a distribution over
                   attended-to input units, and this is often presented (at
                   least implicitly) as communicating the relative importance of
                   inputs. However, it is unclear what relationship exists
                   between attention weights and model outputs. In this work, we
                   perform extensive experiments across a variety of NLP tasks
                   that aim to assess the degree to which attention weights
                   provide meaningful `explanations' for predictions. We find
                   that they largely do not. For example, learned attention
                   weights are frequently uncorrelated with gradient-based
                   measures of feature importance, and one can identify very
                   different attention distributions that nonetheless yield
                   equivalent predictions. Our findings show that standard
                   attention modules do not provide meaningful explanations and
                   should not be treated as though they do. Code for all
                   experiments is available at
                   https://github.com/successar/AttentionExplanation.",
  month         =  feb,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1902.10186"
}

@ARTICLE{Schwartz2020-ho,
  title         = "The Right Tool for the Job: Matching Model and Instance
                   Complexities",
  author        = "Schwartz, Roy and Stanovsky, Gabi and Swayamdipta, Swabha and
                   Dodge, Jesse and Smith, Noah A",
  journal       = "arXiv [cs.CL]",
  abstract      = "As NLP models become larger, executing a trained model
                   requires significant computational resources incurring
                   monetary and environmental costs. To better respect a given
                   inference budget, we propose a modification to contextual
                   representation fine-tuning which, during inference, allows
                   for an early (and fast) ``exit'' from neural network
                   calculations for simple instances, and late (and accurate)
                   exit for hard instances. To achieve this, we add classifiers
                   to different layers of BERT and use their calibrated
                   confidence scores to make early exit decisions. We test our
                   proposed modification on five different datasets in two
                   tasks: three text classification datasets and two natural
                   language inference benchmarks. Our method presents a
                   favorable speed/accuracy tradeoff in almost all cases,
                   producing models which are up to five times faster than the
                   state of the art, while preserving their accuracy. Our method
                   also requires almost no additional training resources (in
                   either time or parameters) compared to the baseline BERT
                   model. Finally, our method alleviates the need for costly
                   retraining of multiple models at different levels of
                   efficiency; we allow users to control the inference
                   speed/accuracy tradeoff using a single trained model, by
                   setting a single variable at inference time. We publicly
                   release our code.",
  month         =  apr,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2004.07453"
}

@INPROCEEDINGS{Smucker2007-aa,
  title     = "A comparison of statistical significance tests for information
               retrieval evaluation",
  author    = "Smucker, Mark D and Allan, James and Carterette, Ben",
  booktitle = "Proceedings of the sixteenth ACM conference on Conference on
               information and knowledge management",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "623--632",
  series    = "CIKM '07",
  month     =  nov,
  year      =  2007,
  keywords  = "statistical significance, hypothesis test, student's t-test,
               sign, permutation, wilcoxon, randomization, bootstrap",
  doi       = "10.1145/1321440.1321528",
  isbn      =  9781595938039
}

@ARTICLE{MacAvaney2020-rn,
  title         = "Efficient Document Re-Ranking for Transformers by
                   Precomputing Term Representations",
  author        = "MacAvaney, Sean and Nardini, Franco Maria and Perego,
                   Raffaele and Tonellotto, Nicola and Goharian, Nazli and
                   Frieder, Ophir",
  journal       = "arXiv [cs.IR]",
  abstract      = "Deep pretrained transformer networks are effective at various
                   ranking tasks, such as question answering and ad-hoc document
                   ranking. However, their computational expenses deem them
                   cost-prohibitive in practice. Our proposed approach, called
                   PreTTR (Precomputing Transformer Term Representations),
                   considerably reduces the query-time latency of deep
                   transformer networks (up to a 42x speedup on web document
                   ranking) making these networks more practical to use in a
                   real-time ranking scenario. Specifically, we precompute part
                   of the document term representations at indexing time without
                   a query, and merge them with the query representation at
                   query time to compute the final ranking score. Due to the
                   large size of the token representations, we also propose an
                   effective approach to reduce the storage requirement by
                   training a compression layer to match attention scores. Our
                   compression technique reduces the storage required up to 95\%
                   and it can be applied without a substantial degradation in
                   ranking performance.",
  month         =  apr,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "2004.14255"
}

@ARTICLE{MacAvaney2020-en,
  title         = "Training Curricula for Open Domain Answer Re-Ranking",
  author        = "MacAvaney, Sean and Nardini, Franco Maria and Perego,
                   Raffaele and Tonellotto, Nicola and Goharian, Nazli and
                   Frieder, Ophir",
  journal       = "arXiv [cs.IR]",
  abstract      = "In precision-oriented tasks like answer ranking, it is more
                   important to rank many relevant answers highly than to
                   retrieve all relevant answers. It follows that a good ranking
                   strategy would be to learn how to identify the easiest
                   correct answers first (i.e., assign a high ranking score to
                   answers that have characteristics that usually indicate
                   relevance, and a low ranking score to those with
                   characteristics that do not), before incorporating more
                   complex logic to handle difficult cases (e.g., semantic
                   matching or reasoning). In this work, we apply this idea to
                   the training of neural answer rankers using curriculum
                   learning. We propose several heuristics to estimate the
                   difficulty of a given training sample. We show that the
                   proposed heuristics can be used to build a training
                   curriculum that down-weights difficult samples early in the
                   training process. As the training process progresses, our
                   approach gradually shifts to weighting all samples equally,
                   regardless of difficulty. We present a comprehensive
                   evaluation of our proposed idea on three answer ranking
                   datasets. Results show that our approach leads to superior
                   performance of two leading neural ranking architectures,
                   namely BERT and ConvKNRM, using both pointwise and pairwise
                   losses. When applied to a BERT-based ranker, our method
                   yields up to a 4\% improvement in MRR and a 9\% improvement
                   in P@1 (compared to the model trained without a curriculum).
                   This results in models that can achieve comparable
                   performance to more expensive state-of-the-art techniques.",
  month         =  apr,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "2004.14269"
}

@ARTICLE{Kikkawa2022-fm,
  title    = "Dataset of first appearances of the scholarly bibliographic
              references on Wikipedia articles",
  author   = "Kikkawa, Jiro and Takaku, Masao and Yoshikane, Fuyuki",
  journal  = "Scientific data",
  volume   =  9,
  number   =  1,
  pages    =  85,
  abstract = "Referencing scholarly documents as information sources on
              Wikipedia is important because it supports or improves the quality
              of Wikipedia content. Several studies have been conducted
              regarding scholarly references on Wikipedia; however, little is
              known of the editors and their edits contributing to add the
              scholarly references on Wikipedia. In this study, we develop a
              methodology to detect the oldest scholarly reference added to
              Wikipedia articles by which a certain paper is uniquely
              identifiable as the ``first appearance of the scholarly
              reference.'' We identified the first appearances of 923,894
              scholarly references (611,119 unique DOIs) in 180,795 unique pages
              on English Wikipedia as of March 1, 2017 and stored them in the
              dataset. Moreover, we assessed the precision of the dataset, which
              was highly precise regardless of the research field. Finally, we
              demonstrate the potential of our dataset. This dataset is unique
              and attracts those who are interested in how the scholarly
              references on Wikipedia grew and which editors added them.",
  month    =  mar,
  year     =  2022,
  doi      = "10.1038/s41597-022-01190-z",
  pmc      = "PMC8921307",
  pmid     =  35288593,
  issn     = "2052-4463",
  language = "en"
}

@ARTICLE{Singh2020-nb,
  title         = "Wikipedia Citations: A comprehensive dataset of citations
                   with identifiers extracted from English Wikipedia",
  author        = "Singh, Harshdeep and West, Robert and Colavizza, Giovanni",
  journal       = "arXiv [cs.DL]",
  abstract      = "Wikipedia's contents are based on reliable and published
                   sources. To this date, relatively little is known about what
                   sources Wikipedia relies on, in part because extracting
                   citations and identifying cited sources is challenging. To
                   close this gap, we release Wikipedia Citations, a
                   comprehensive dataset of citations extracted from Wikipedia.
                   A total of 29.3M citations were extracted from 6.1M English
                   Wikipedia articles as of May 2020, and classified as being to
                   books, journal articles or Web contents. We were thus able to
                   extract 4.0M citations to scholarly publications with known
                   identifiers -- including DOI, PMC, PMID, and ISBN -- and
                   further equip an extra 261K citations with DOIs from
                   Crossref. As a result, we find that 6.7\% of Wikipedia
                   articles cite at least one journal article with an associated
                   DOI, and that Wikipedia cites just 2\% of all articles with a
                   DOI currently indexed in the Web of Science. We release our
                   code to allow the community to extend upon our work and
                   update the dataset in the future.",
  month         =  jul,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.DL",
  eprint        = "2007.07022"
}

@ARTICLE{McCoy2023-nh,
  title         = "Embers of autoregression: Understanding large language models
                   through the problem they are trained to solve",
  author        = "McCoy, R Thomas and Yao, Shunyu and Friedman, Dan and Hardy,
                   Matthew and Griffiths, Thomas L",
  journal       = "arXiv [cs.CL]",
  abstract      = "The widespread adoption of large language models (LLMs) makes
                   it important to recognize their strengths and limitations. We
                   argue that in order to develop a holistic understanding of
                   these systems we need to consider the problem that they were
                   trained to solve: next-word prediction over Internet text. By
                   recognizing the pressures that this task exerts we can make
                   predictions about the strategies that LLMs will adopt,
                   allowing us to reason about when they will succeed or fail.
                   This approach - which we call the teleological approach -
                   leads us to identify three factors that we hypothesize will
                   influence LLM accuracy: the probability of the task to be
                   performed, the probability of the target output, and the
                   probability of the provided input. We predict that LLMs will
                   achieve higher accuracy when these probabilities are high
                   than when they are low - even in deterministic settings where
                   probability should not matter. To test our predictions, we
                   evaluate two LLMs (GPT-3.5 and GPT-4) on eleven tasks, and we
                   find robust evidence that LLMs are influenced by probability
                   in the ways that we have hypothesized. In many cases, the
                   experiments reveal surprising failure modes. For instance,
                   GPT-4's accuracy at decoding a simple cipher is 51\% when the
                   output is a high-probability word sequence but only 13\% when
                   it is low-probability. These results show that AI
                   practitioners should be careful about using LLMs in
                   low-probability situations. More broadly, we conclude that we
                   should not evaluate LLMs as if they are humans but should
                   instead treat them as a distinct type of system - one that
                   has been shaped by its own particular set of pressures.",
  month         =  sep,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2309.13638"
}

@ARTICLE{Gardner2020-kx,
  title         = "Determining Question-Answer Plausibility in Crowdsourced
                   Datasets Using Multi-Task Learning",
  author        = "Gardner, Rachel and Varma, Maya and Zhu, Clare and Krishna,
                   Ranjay",
  journal       = "arXiv [cs.CL]",
  abstract      = "Datasets extracted from social networks and online forums are
                   often prone to the pitfalls of natural language, namely the
                   presence of unstructured and noisy data. In this work, we
                   seek to enable the collection of high-quality question-answer
                   datasets from social media by proposing a novel task for
                   automated quality analysis and data cleaning: question-answer
                   (QA) plausibility. Given a machine or user-generated question
                   and a crowd-sourced response from a social media user, we
                   determine if the question and response are valid; if so, we
                   identify the answer within the free-form response. We design
                   BERT-based models to perform the QA plausibility task, and we
                   evaluate the ability of our models to generate a clean,
                   usable question-answer dataset. Our highest-performing
                   approach consists of a single-task model which determines the
                   plausibility of the question, followed by a multi-task model
                   which evaluates the plausibility of the response as well as
                   extracts answers (Question Plausibility AUROC=0.75, Response
                   Plausibility AUROC=0.78, Answer Extraction F1=0.665).",
  month         =  nov,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2011.04883"
}

@INPROCEEDINGS{Kumaran2009-wo,
  title     = "Reducing Long Queries Using Query Quality Predictors",
  author    = "Kumaran, Giridhar and Carvalho, Vitor R",
  booktitle = "Proceedings of the 32Nd International ACM SIGIR Conference on
               Research and Development in Information Retrieval",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "564--571",
  series    = "SIGIR '09",
  year      =  2009,
  keywords  = "long queries, query quality, query reduction, verbose queries",
  doi       = "10.1145/1571941.1572038",
  isbn      =  9781605584836
}

@INPROCEEDINGS{Sennrich2019-pn,
  title     = "Revisiting Low-Resource Neural Machine Translation: A Case Study",
  author    = "Sennrich, Rico and Zhang, Biao",
  booktitle = "Proceedings of the 57th Annual Meeting of the Association for
               Computational Linguistics",
  publisher = "Association for Computational Linguistics",
  address   = "Florence, Italy",
  pages     = "211--221",
  abstract  = "It has been shown that the performance of neural machine
               translation (NMT) drops starkly in low-resource conditions,
               underperforming phrase-based statistical machine translation
               (PBSMT) and requiring large amounts of auxiliary data to achieve
               competitive results. In this paper, we re-assess the validity of
               these results, arguing that they are the result of lack of system
               adaptation to low-resource settings. We discuss some pitfalls to
               be aware of when training low-resource NMT systems, and recent
               techniques that have shown to be especially helpful in
               low-resource settings, resulting in a set of best practices for
               low-resource NMT. In our experiments on German--English with
               different amounts of IWSLT14 training data, we show that, without
               the use of any auxiliary monolingual or multilingual data, an
               optimized NMT system can outperform PBSMT with far less data than
               previously claimed. We also apply these techniques to a
               low-resource Korean--English dataset, surpassing previously
               reported results by 4 BLEU.",
  month     =  jul,
  year      =  2019,
  doi       = "10.18653/v1/P19-1021"
}

@INPROCEEDINGS{Severyn2015-pm,
  title     = "Learning to Rank Short Text Pairs with Convolutional Deep Neural
               Networks",
  author    = "Severyn, Aliaksei and Moschitti, Alessandro",
  booktitle = "SIGIR",
  year      =  2015
}

@INPROCEEDINGS{Kovaleva2019-lj,
  title     = "Revealing the Dark Secrets of {BERT}",
  author    = "Kovaleva, Olga and Romanov, Alexey and Rogers, Anna and
               Rumshisky, Anna",
  booktitle = "Proceedings of the 2019 Conference on Empirical Methods in
               Natural Language Processing and the 9th International Joint
               Conference on Natural Language Processing (EMNLP-IJCNLP)",
  publisher = "Association for Computational Linguistics",
  address   = "Hong Kong, China",
  pages     = "4365--4374",
  abstract  = "BERT-based architectures currently give state-of-the-art
               performance on many NLP tasks, but little is known about the
               exact mechanisms that contribute to its success. In the current
               work, we focus on the interpretation of self-attention, which is
               one of the fundamental underlying components of BERT. Using a
               subset of GLUE tasks and a set of handcrafted
               features-of-interest, we propose the methodology and carry out a
               qualitative and quantitative analysis of the information encoded
               by the individual BERT's heads. Our findings suggest that there
               is a limited set of attention patterns that are repeated across
               different heads, indicating the overall model
               overparametrization. While different heads consistently use the
               same attention patterns, they have varying impact on performance
               across different tasks. We show that manually disabling attention
               in certain heads leads to a performance improvement over the
               regular fine-tuned BERT models.",
  month     =  nov,
  year      =  2019,
  doi       = "10.18653/v1/D19-1445"
}

@INPROCEEDINGS{Gururangan2020-fm,
  title     = "Don{'}t Stop Pretraining: Adapt Language Models to Domains and
               Tasks",
  author    = "Gururangan, Suchin and Marasović, Ana and Swayamdipta, Swabha and
               Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A",
  booktitle = "Proceedings of the 58th Annual Meeting of the Association for
               Computational Linguistics",
  publisher = "Association for Computational Linguistics",
  address   = "Online",
  pages     = "8342--8360",
  abstract  = "Language models pretrained on text from a wide variety of sources
               form the foundation of today's NLP. In light of the success of
               these broad-coverage models, we investigate whether it is still
               helpful to tailor a pretrained model to the domain of a target
               task. We present a study across four domains (biomedical and
               computer science publications, news, and reviews) and eight
               classification tasks, showing that a second phase of pretraining
               in-domain (domain-adaptive pretraining) leads to performance
               gains, under both high- and low-resource settings. Moreover,
               adapting to the task's unlabeled data (task-adaptive pretraining)
               improves performance even after domain-adaptive pretraining.
               Finally, we show that adapting to a task corpus augmented using
               simple data selection strategies is an effective alternative,
               especially when resources for domain-adaptive pretraining might
               be unavailable. Overall, we consistently find that multi-phase
               adaptive pretraining offers large gains in task performance.",
  month     =  jul,
  year      =  2020,
  doi       = "10.18653/v1/2020.acl-main.740"
}

@ARTICLE{Lewis2020-wi,
  title         = "Pre-training via Paraphrasing",
  author        = "Lewis, Mike and Ghazvininejad, Marjan and Ghosh, Gargi and
                   Aghajanyan, Armen and Wang, Sida and Zettlemoyer, Luke",
  journal       = "arXiv [cs.CL]",
  abstract      = "We introduce MARGE, a pre-trained sequence-to-sequence model
                   learned with an unsupervised multi-lingual multi-document
                   paraphrasing objective. MARGE provides an alternative to the
                   dominant masked language modeling paradigm, where we
                   self-supervise the reconstruction of target text by
                   retrieving a set of related texts (in many languages) and
                   conditioning on them to maximize the likelihood of generating
                   the original. We show it is possible to jointly learn to do
                   retrieval and reconstruction, given only a random
                   initialization. The objective noisily captures aspects of
                   paraphrase, translation, multi-document summarization, and
                   information retrieval, allowing for strong zero-shot
                   performance on several tasks. For example, with no additional
                   task-specific training we achieve BLEU scores of up to 35.8
                   for document translation. We further show that fine-tuning
                   gives strong performance on a range of discriminative and
                   generative tasks in many languages, making MARGE the most
                   generally applicable pre-training method to date.",
  month         =  jun,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2006.15020"
}

@INPROCEEDINGS{Lee2019-dm,
  title     = "Latent Retrieval for Weakly Supervised Open Domain Question
               Answering",
  author    = "Lee, Kenton and Chang, Ming-Wei and Toutanova, Kristina",
  booktitle = "Proceedings of the 57th Annual Meeting of the Association for
               Computational Linguistics",
  publisher = "Association for Computational Linguistics",
  address   = "Florence, Italy",
  pages     = "6086--6096",
  abstract  = "Recent work on open domain question answering (QA) assumes strong
               supervision of the supporting evidence and/or assumes a blackbox
               information retrieval (IR) system to retrieve evidence
               candidates. We argue that both are suboptimal, since gold
               evidence is not always available, and QA is fundamentally
               different from IR. We show for the first time that it is possible
               to jointly learn the retriever and reader from question-answer
               string pairs and without any IR system. In this setting, evidence
               retrieval from all of Wikipedia is treated as a latent variable.
               Since this is impractical to learn from scratch, we pre-train the
               retriever with an Inverse Cloze Task. We evaluate on open
               versions of five QA datasets. On datasets where the questioner
               already knows the answer, a traditional IR system such as BM25 is
               sufficient. On datasets where a user is genuinely seeking an
               answer, we show that learned retrieval is crucial, outperforming
               BM25 by up to 19 points in exact match.",
  month     =  jul,
  year      =  2019,
  doi       = "10.18653/v1/P19-1612"
}

@INPROCEEDINGS{Mielke2019-jw,
  title     = "What Kind of Language Is Hard to Language-Model?",
  author    = "Mielke, Sabrina J and Cotterell, Ryan and Gorman, Kyle and Roark,
               Brian and Eisner, Jason",
  booktitle = "Proceedings of the 57th Annual Meeting of the Association for
               Computational Linguistics",
  publisher = "Association for Computational Linguistics",
  address   = "Florence, Italy",
  pages     = "4975--4989",
  abstract  = "How language-agnostic are current state-of-the-art NLP tools? Are
               there some types of language that are easier to model with
               current methods? In prior work (Cotterell et al., 2018) we
               attempted to address this question for language modeling, and
               observed that recurrent neural network language models do not
               perform equally well over all the high-resource European
               languages found in the Europarl corpus. We speculated that
               inflectional morphology may be the primary culprit for the
               discrepancy. In this paper, we extend these earlier experiments
               to cover 69 languages from 13 language families using a
               multilingual Bible corpus. Methodologically, we introduce a new
               paired-sample multiplicative mixed-effects model to obtain
               language difficulty coefficients from at-least-pairwise parallel
               corpora. In other words, the model is aware of inter-sentence
               variation and can handle missing data. Exploiting this model, we
               show that ``translationese'' is not any easier to model than
               natively written language in a fair comparison. Trying to answer
               the question of what features difficult languages have in common,
               we try and fail to reproduce our earlier (Cotterell et al., 2018)
               observation about morphological complexity and instead reveal far
               simpler statistics of the data that seem to drive complexity in a
               much larger sample.",
  month     =  jul,
  year      =  2019,
  doi       = "10.18653/v1/P19-1491"
}

@ARTICLE{Turc2019-no,
  title         = "Well-Read Students Learn Better: On the Importance of
                   Pre-training Compact Models",
  author        = "Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and
                   Toutanova, Kristina",
  journal       = "arXiv [cs.CL]",
  abstract      = "Recent developments in natural language representations have
                   been accompanied by large and expensive models that leverage
                   vast amounts of general-domain text through self-supervised
                   pre-training. Due to the cost of applying such models to
                   down-stream tasks, several model compression techniques on
                   pre-trained language representations have been proposed (Sun
                   et al., 2019; Sanh, 2019). However, surprisingly, the simple
                   baseline of just pre-training and fine-tuning compact models
                   has been overlooked. In this paper, we first show that
                   pre-training remains important in the context of smaller
                   architectures, and fine-tuning pre-trained compact models can
                   be competitive to more elaborate methods proposed in
                   concurrent work. Starting with pre-trained compact models, we
                   then explore transferring task knowledge from large
                   fine-tuned models through standard knowledge distillation.
                   The resulting simple, yet effective and general algorithm,
                   Pre-trained Distillation, brings further improvements.
                   Through extensive experiments, we more generally explore the
                   interaction between pre-training and distillation under two
                   variables that have been under-studied: model size and
                   properties of unlabeled task data. One surprising observation
                   is that they have a compound effect even when sequentially
                   applied on the same data. To accelerate future research, we
                   will make our 24 pre-trained miniature BERT models publicly
                   available.",
  month         =  aug,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1908.08962"
}

@ARTICLE{Roberts2020-nr,
  title         = "How Much Knowledge Can You Pack Into the Parameters of a
                   Language Model?",
  author        = "Roberts, Adam and Raffel, Colin and Shazeer, Noam",
  journal       = "arXiv [cs.CL]",
  abstract      = "It has recently been observed that neural language models
                   trained on unstructured text can implicitly store and
                   retrieve knowledge using natural language queries. In this
                   short paper, we measure the practical utility of this
                   approach by fine-tuning pre-trained models to answer
                   questions without access to any external context or
                   knowledge. We show that this approach scales with model size
                   and performs competitively with open-domain systems that
                   explicitly retrieve answers from an external knowledge source
                   when answering questions. To facilitate reproducibility and
                   future work, we release our code and trained models at
                   https://goo.gle/t5-cbqa.",
  month         =  feb,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2002.08910"
}

@ARTICLE{Lin2020-bb,
  title         = "Pretrained transformers for text ranking: {BERT} and beyond",
  author        = "Lin, Jimmy and Nogueira, Rodrigo and Yates, Andrew",
  journal       = "arXiv [cs.IR]",
  abstract      = "The goal of text ranking is to generate an ordered list of
                   texts retrieved from a corpus in response to a query.
                   Although the most common formulation of text ranking is
                   search, instances of the task can also be found in many
                   natural language processing applications. This survey
                   provides an overview of text ranking with neural network
                   architectures known as transformers, of which BERT is the
                   best-known example. The combination of transformers and
                   self-supervised pretraining has been responsible for a
                   paradigm shift in natural language processing (NLP),
                   information retrieval (IR), and beyond. In this survey, we
                   provide a synthesis of existing work as a single point of
                   entry for practitioners who wish to gain a better
                   understanding of how to apply transformers to text ranking
                   problems and researchers who wish to pursue work in this
                   area. We cover a wide range of modern techniques, grouped
                   into two high-level categories: transformer models that
                   perform reranking in multi-stage architectures and dense
                   retrieval techniques that perform ranking directly. There are
                   two themes that pervade our survey: techniques for handling
                   long documents, beyond typical sentence-by-sentence
                   processing in NLP, and techniques for addressing the tradeoff
                   between effectiveness (i.e., result quality) and efficiency
                   (e.g., query latency, model and index size). Although
                   transformer architectures and pretraining techniques are
                   recent innovations, many aspects of how they are applied to
                   text ranking are relatively well understood and represent
                   mature techniques. However, there remain many open research
                   questions, and thus in addition to laying out the foundations
                   of pretrained transformers for text ranking, this survey also
                   attempts to prognosticate where the field is heading.",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "2010.06467"
}

@ARTICLE{Zhu2020-jl,
  title         = "Don't Parse, Insert: Multilingual Semantic Parsing with
                   Insertion Based Decoding",
  author        = "Zhu, Qile and Khan, Haidar and Soltan, Saleh and Rawls,
                   Stephen and Hamza, Wael",
  journal       = "arXiv [cs.CL]",
  abstract      = "Semantic parsing is one of the key components of natural
                   language understanding systems. A successful parse transforms
                   an input utterance to an action that is easily understood by
                   the system. Many algorithms have been proposed to solve this
                   problem, from conventional rulebased or statistical
                   slot-filling systems to shiftreduce based neural parsers. For
                   complex parsing tasks, the state-of-the-art method is based
                   on autoregressive sequence to sequence models to generate the
                   parse directly. This model is slow at inference time,
                   generating parses in O(n) decoding steps (n is the length of
                   the target sequence). In addition, we demonstrate that this
                   method performs poorly in zero-shot cross-lingual transfer
                   learning settings. In this paper, we propose a
                   non-autoregressive parser which is based on the insertion
                   transformer to overcome these two issues. Our approach 1)
                   speeds up decoding by 3x while outperforming the
                   autoregressive model and 2) significantly improves
                   cross-lingual transfer in the low-resource setting by 37\%
                   compared to autoregressive baseline. We test our approach on
                   three well-known monolingual datasets: ATIS, SNIPS and TOP.
                   For cross lingual semantic parsing, we use the MultiATIS++
                   and the multilingual TOP datasets.",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2010.03714"
}

@ARTICLE{Rongali2020-zv,
  title         = "Don't Parse, Generate! A Sequence to Sequence Architecture
                   for Task-Oriented Semantic Parsing",
  author        = "Rongali, Subendhu and Soldaini, Luca and Monti, Emilio and
                   Hamza, Wael",
  journal       = "arXiv [cs.CL]",
  abstract      = "Virtual assistants such as Amazon Alexa, Apple Siri, and
                   Google Assistant often rely on a semantic parsing component
                   to understand which action(s) to execute for an utterance
                   spoken by its users. Traditionally, rule-based or statistical
                   slot-filling systems have been used to parse ``simple''
                   queries; that is, queries that contain a single action and
                   can be decomposed into a set of non-overlapping entities.
                   More recently, shift-reduce parsers have been proposed to
                   process more complex utterances. These methods, while
                   powerful, impose specific limitations on the type of queries
                   that can be parsed; namely, they require a query to be
                   representable as a parse tree. In this work, we propose a
                   unified architecture based on Sequence to Sequence models and
                   Pointer Generator Network to handle both simple and complex
                   queries. Unlike other works, our approach does not impose any
                   restriction on the semantic parse schema. Furthermore,
                   experiments show that it achieves state of the art
                   performance on three publicly available datasets (ATIS,
                   SNIPS, Facebook TOP), relatively improving between 3.3\% and
                   7.7\% in exact match accuracy over previous systems. Finally,
                   we show the effectiveness of our approach on two internal
                   datasets.",
  month         =  jan,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2001.11458"
}

@ARTICLE{Du2020-dh,
  title         = "Self-training Improves Pre-training for Natural Language
                   Understanding",
  author        = "Du, Jingfei and Grave, Edouard and Gunel, Beliz and
                   Chaudhary, Vishrav and Celebi, Onur and Auli, Michael and
                   Stoyanov, Ves and Conneau, Alexis",
  journal       = "arXiv [cs.CL]",
  abstract      = "Unsupervised pre-training has led to much recent progress in
                   natural language understanding. In this paper, we study
                   self-training as another way to leverage unlabeled data
                   through semi-supervised learning. To obtain additional data
                   for a specific task, we introduce SentAugment, a data
                   augmentation method which computes task-specific query
                   embeddings from labeled data to retrieve sentences from a
                   bank of billions of unlabeled sentences crawled from the web.
                   Unlike previous semi-supervised methods, our approach does
                   not require in-domain unlabeled data and is therefore more
                   generally applicable. Experiments show that self-training is
                   complementary to strong RoBERTa baselines on a variety of
                   tasks. Our augmentation approach leads to scalable and
                   effective self-training with improvements of up to 2.6\% on
                   standard text classification benchmarks. Finally, we also
                   show strong gains on knowledge-distillation and few-shot
                   learning.",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2010.02194"
}

@ARTICLE{Card2020-wg,
  title         = "With Little Power Comes Great Responsibility",
  author        = "Card, Dallas and Henderson, Peter and Khandelwal, Urvashi and
                   Jia, Robin and Mahowald, Kyle and Jurafsky, Dan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Despite its importance to experimental design, statistical
                   power (the probability that, given a real effect, an
                   experiment will reject the null hypothesis) has largely been
                   ignored by the NLP community. Underpowered experiments make
                   it more difficult to discern the difference between
                   statistical noise and meaningful model improvements, and
                   increase the chances of exaggerated findings. By
                   meta-analyzing a set of existing NLP papers and datasets, we
                   characterize typical power for a variety of settings and
                   conclude that underpowered experiments are common in the NLP
                   literature. In particular, for several tasks in the popular
                   GLUE benchmark, small test sets mean that most attempted
                   comparisons to state of the art models will not be adequately
                   powered. Similarly, based on reasonable assumptions, we find
                   that the most typical experimental design for human rating
                   studies will be underpowered to detect small model
                   differences, of the sort that are frequently studied. For
                   machine translation, we find that typical test sets of 2000
                   sentences have approximately 75\% power to detect differences
                   of 1 BLEU point. To improve the situation going forward, we
                   give an overview of best practices for power analysis in NLP
                   and release a series of notebooks to assist with future power
                   analyses.",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2010.06595"
}

@ARTICLE{Chen2023-lo,
  title         = "Symbolic discovery of optimization algorithms",
  author        = "Chen, Xiangning and Liang, Chen and Huang, Da and Real,
                   Esteban and Wang, Kaiyuan and Liu, Yao and Pham, Hieu and
                   Dong, Xuanyi and Luong, Thang and Hsieh, Cho-Jui and Lu,
                   Yifeng and Le, Quoc V",
  journal       = "arXiv [cs.LG]",
  abstract      = "We present a method to formulate algorithm discovery as
                   program search, and apply it to discover optimization
                   algorithms for deep neural network training. We leverage
                   efficient search techniques to explore an infinite and sparse
                   program space. To bridge the large generalization gap between
                   proxy and target tasks, we also introduce program selection
                   and simplification strategies. Our method discovers a simple
                   and effective optimization algorithm, $\textbf{Lion}$
                   ($\textit{Evo$\textbf{L}$ved S$\textbf{i}$gn
                   M$\textbf{o}$me$\textbf{n}$tum}$). It is more
                   memory-efficient than Adam as it only keeps track of the
                   momentum. Different from adaptive optimizers, its update has
                   the same magnitude for each parameter calculated through the
                   sign operation. We compare Lion with widely used optimizers,
                   such as Adam and Adafactor, for training a variety of models
                   on different tasks. On image classification, Lion boosts the
                   accuracy of ViT by up to 2\% on ImageNet and saves up to 5x
                   the pre-training compute on JFT. On vision-language
                   contrastive learning, we achieve 88.3\% $\textit{zero-shot}$
                   and 91.1\% $\textit{fine-tuning}$ accuracy on ImageNet,
                   surpassing the previous best results by 2\% and 0.1\%,
                   respectively. On diffusion models, Lion outperforms Adam by
                   achieving a better FID score and reducing the training
                   compute by up to 2.3x. For autoregressive, masked language
                   modeling, and fine-tuning, Lion exhibits a similar or better
                   performance compared to Adam. Our analysis of Lion reveals
                   that its performance gain grows with the training batch size.
                   It also requires a smaller learning rate than Adam due to the
                   larger norm of the update produced by the sign function.
                   Additionally, we examine the limitations of Lion and identify
                   scenarios where its improvements are small or not
                   statistically significant. Lion is also successfully deployed
                   in production systems such as Google search ads CTR model.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2302.06675"
}

@ARTICLE{Ofir-Press2021-ck,
  title         = "Train short, test long: Attention with Linear Biases enables
                   input length extrapolation",
  author        = "{Ofir Press} and Smith, Noah A and Lewis, Mike",
  journal       = "arXiv [cs.CL]",
  abstract      = "Since the introduction of the transformer model by Vaswani et
                   al. (2017), a fundamental question has yet to be answered:
                   how does a model achieve extrapolation at inference time for
                   sequences that are longer than it saw during training? We
                   first show that extrapolation can be enabled by simply
                   changing the position representation method, though we find
                   that current methods do not allow for efficient
                   extrapolation. We therefore introduce a simpler and more
                   efficient position method, Attention with Linear Biases
                   (ALiBi). ALiBi does not add positional embeddings to word
                   embeddings; instead, it biases query-key attention scores
                   with a penalty that is proportional to their distance. We
                   show that this method trains a 1.3 billion parameter model on
                   input sequences of length 1024 that extrapolates to input
                   sequences of length 2048, achieving the same perplexity as a
                   sinusoidal position embedding model trained on inputs of
                   length 2048 but training 11\% faster and using 11\% less
                   memory. ALiBi's inductive bias towards recency also leads it
                   to outperform multiple strong position methods on the
                   WikiText-103 benchmark.",
  month         =  aug,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2108.12409"
}

@ARTICLE{Baumgartner2020-ga,
  title         = "The Pushshift Reddit Dataset",
  author        = "Baumgartner, Jason and Zannettou, Savvas and Keegan, Brian
                   and Squire, Megan and Blackburn, Jeremy",
  journal       = "arXiv [cs.SI]",
  abstract      = "Social media data has become crucial to the advancement of
                   scientific understanding. However, even though it has become
                   ubiquitous, just collecting large-scale social media data
                   involves a high degree of engineering skill set and
                   computational resources. In fact, research is often times
                   gated by data engineering problems that must be overcome
                   before analysis can proceed. This has resulted recognition of
                   datasets as meaningful research contributions in and of
                   themselves. Reddit, the so called ``front page of the
                   Internet,'' in particular has been the subject of numerous
                   scientific studies. Although Reddit is relatively open to
                   data acquisition compared to social media platforms like
                   Facebook and Twitter, the technical barriers to acquisition
                   still remain. Thus, Reddit's millions of subreddits, hundreds
                   of millions of users, and hundreds of billions of comments
                   are at the same time relatively accessible, but time
                   consuming to collect and analyze systematically. In this
                   paper, we present the Pushshift Reddit dataset. Pushshift is
                   a social media data collection, analysis, and archiving
                   platform that since 2015 has collected Reddit data and made
                   it available to researchers. Pushshift's Reddit dataset is
                   updated in real-time, and includes historical data back to
                   Reddit's inception. In addition to monthly dumps, Pushshift
                   provides computational tools to aid in searching,
                   aggregating, and performing exploratory analysis on the
                   entirety of the dataset. The Pushshift Reddit dataset makes
                   it possible for social media researchers to reduce time spent
                   in the data collection, cleaning, and storage phases of their
                   projects.",
  month         =  jan,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.SI",
  eprint        = "2001.08435"
}

@ARTICLE{Bloom1970-aa,
  title     = "Space/time trade-offs in hash coding with allowable errors",
  author    = "Bloom, Burton H",
  journal   = "Communications of the ACM",
  publisher = "Association for Computing Machinery (ACM)",
  volume    =  13,
  number    =  7,
  pages     = "422--426",
  abstract  = "In this paper trade-offs among certain computational factors in
               hash coding are analyzed. The paradigm problem considered is that
               of testing a series of messages one-by-one for membership in a
               given set of messages. Two new hash-coding methods are examined
               and compared with a particular conventional hash-coding method.
               The computational factors considered are the size of the hash
               area (space), the time required to identify a message as a
               nonmember of the given set (reject time), and an allowable error
               frequency. The new methods are intended to reduce the amount of
               space required to contain the hash-coded information from that
               associated with conventional methods. The reduction in space is
               accomplished by exploiting the possibility that a small fraction
               of errors of commission may be tolerable in some applications, in
               particular, applications in which a large amount of data is
               involved and a core resident hash area is consequently not
               feasible using conventional methods. In such applications, it is
               envisaged that overall performance could be improved by using a
               smaller core resident hash area in conjunction with the new
               methods and, when necessary, by using some secondary and perhaps
               time-consuming test to “catch” the small fraction of errors
               associated with the new methods. An example is discussed which
               illustrates possible areas of application for the new methods.
               Analysis of the paradigm problem demonstrates that allowing a
               small number of test messages to be falsely identified as members
               of the given set will permit a much smaller hash area to be used
               without increasing reject time.",
  month     =  jul,
  year      =  1970,
  doi       = "10.1145/362686.362692",
  issn      = "0001-0782,1557-7317",
  language  = "en"
}

@ARTICLE{Lewis2020-fy,
  title         = "Retrieval-Augmented Generation for Knowledge-Intensive {NLP}
                   Tasks",
  author        = "Lewis, Patrick and Perez, Ethan and Piktus, Aleksandara and
                   Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and
                   Küttler, Heinrich and Lewis, Mike and Yih, Wen-Tau and
                   Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large pre-trained language models have been shown to store
                   factual knowledge in their parameters, and achieve
                   state-of-the-art results when fine-tuned on downstream NLP
                   tasks. However, their ability to access and precisely
                   manipulate knowledge is still limited, and hence on
                   knowledge-intensive tasks, their performance lags behind
                   task-specific architectures. Additionally, providing
                   provenance for their decisions and updating their world
                   knowledge remain open research problems. Pre-trained models
                   with a differentiable access mechanism to explicit
                   non-parametric memory can overcome this issue, but have so
                   far been only investigated for extractive downstream tasks.
                   We explore a general-purpose fine-tuning recipe for
                   retrieval-augmented generation (RAG) -- models which combine
                   pre-trained parametric and non-parametric memory for language
                   generation. We introduce RAG models where the parametric
                   memory is a pre-trained seq2seq model and the non-parametric
                   memory is a dense vector index of Wikipedia, accessed with a
                   pre-trained neural retriever. We compare two RAG
                   formulations, one which conditions on the same retrieved
                   passages across the whole generated sequence, the other can
                   use different passages per token. We fine-tune and evaluate
                   our models on a wide range of knowledge-intensive NLP tasks
                   and set the state-of-the-art on three open domain QA tasks,
                   outperforming parametric seq2seq models and task-specific
                   retrieve-and-extract architectures. For language generation
                   tasks, we find that RAG models generate more specific,
                   diverse and factual language than a state-of-the-art
                   parametric-only seq2seq baseline.",
  month         =  may,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2005.11401"
}

@ARTICLE{Wang2023-jv,
  title         = "{DORIS}-{MAE}: Scientific {DOcument} Retrieval using
                   multi-level Aspect-based queries",
  author        = "Wang, Jianyou and Wang, Kaicheng and Wang, Xiaoyue and Naidu,
                   Prudhviraj and Bergen, Leon and Paturi, Ramamohan",
  journal       = "arXiv [cs.IR]",
  abstract      = "In scientific research, the ability to effectively retrieve
                   relevant documents based on complex, multifaceted queries is
                   critical. Existing evaluation datasets for this task are
                   limited, primarily due to the high cost and effort required
                   to annotate resources that effectively represent complex
                   queries. To address this, we propose a novel task, Scientific
                   DOcument Retrieval using Multi-level Aspect-based quEries
                   (DORIS-MAE), which is designed to handle the complex nature
                   of user queries in scientific research. We developed a
                   benchmark dataset within the field of computer science,
                   consisting of 100 human-authored complex query cases. For
                   each complex query, we assembled a collection of 100 relevant
                   documents and produced annotated relevance scores for ranking
                   them. Recognizing the significant labor of expert annotation,
                   we also introduce Anno-GPT, a scalable framework for
                   validating the performance of Large Language Models (LLMs) on
                   expert-level dataset annotation tasks. LLM annotation of the
                   DORIS-MAE dataset resulted in a 500x reduction in cost,
                   without compromising quality. Furthermore, due to the
                   multi-tiered structure of these complex queries, the
                   DORIS-MAE dataset can be extended to over 4,000 sub-query
                   test cases without requiring additional annotation. We
                   evaluated 17 recent retrieval methods on DORIS-MAE, observing
                   notable performance drops compared to traditional datasets.
                   This highlights the need for better approaches to handle
                   complex, multifaceted queries in scientific research. Our
                   dataset and codebase are available at
                   https://github.com/Real-Doris-Mae/Doris-Mae-Dataset.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "2310.04678"
}

@ARTICLE{Nanda2023-fx,
  title         = "Progress measures for grokking via mechanistic
                   interpretability",
  author        = "Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith,
                   Jess and Steinhardt, Jacob",
  journal       = "arXiv [cs.LG]",
  abstract      = "Neural networks often exhibit emergent behavior, where
                   qualitatively new capabilities arise from scaling up the
                   amount of parameters, training data, or training steps. One
                   approach to understanding emergence is to find continuous
                   \textit{progress measures} that underlie the seemingly
                   discontinuous qualitative changes. We argue that progress
                   measures can be found via mechanistic interpretability:
                   reverse-engineering learned behaviors into their individual
                   components. As a case study, we investigate the
                   recently-discovered phenomenon of ``grokking'' exhibited by
                   small transformers trained on modular addition tasks. We
                   fully reverse engineer the algorithm learned by these
                   networks, which uses discrete Fourier transforms and
                   trigonometric identities to convert addition to rotation
                   about a circle. We confirm the algorithm by analyzing the
                   activations and weights and by performing ablations in
                   Fourier space. Based on this understanding, we define
                   progress measures that allow us to study the dynamics of
                   training and split training into three continuous phases:
                   memorization, circuit formation, and cleanup. Our results
                   show that grokking, rather than being a sudden shift, arises
                   from the gradual amplification of structured mechanisms
                   encoded in the weights, followed by the later removal of
                   memorizing components.",
  month         =  jan,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2301.05217"
}

@ARTICLE{Xiong2023-mh,
  title         = "Effective long-context scaling of foundation models",
  author        = "Xiong, Wenhan and Liu, Jingyu and Molybog, Igor and Zhang,
                   Hejia and Bhargava, Prajjwal and Hou, Rui and Martin, Louis
                   and Rungta, Rashi and Sankararaman, Karthik Abinav and Oguz,
                   Barlas and Khabsa, Madian and Fang, Han and Mehdad, Yashar
                   and Narang, Sharan and Malik, Kshitiz and Fan, Angela and
                   Bhosale, Shruti and Edunov, Sergey and Lewis, Mike and Wang,
                   Sinong and Ma, Hao",
  journal       = "arXiv [cs.CL]",
  abstract      = "We present a series of long-context LLMs that support
                   effective context windows of up to 32,768 tokens. Our model
                   series are built through continual pretraining from Llama 2
                   with longer training sequences and on a dataset where long
                   texts are upsampled. We perform extensive evaluation on
                   language modeling, synthetic context probing tasks, and a
                   wide range of research benchmarks. On research benchmarks,
                   our models achieve consistent improvements on most regular
                   tasks and significant improvements on long-context tasks over
                   Llama 2. Notably, with a cost-effective instruction tuning
                   procedure that does not require human-annotated long
                   instruction data, the 70B variant can already surpass
                   gpt-3.5-turbo-16k's overall performance on a suite of
                   long-context tasks. Alongside these results, we provide an
                   in-depth analysis on the individual components of our method.
                   We delve into Llama's position encodings and discuss its
                   limitation in modeling long dependencies. We also examine the
                   impact of various design choices in the pretraining process,
                   including the data mix and the training curriculum of
                   sequence lengths -- our ablation experiments suggest that
                   having abundant long texts in the pretrain dataset is not the
                   key to achieving strong performance, and we empirically
                   verify that long context continual pretraining is more
                   efficient and similarly effective compared to pretraining
                   from scratch with long sequences.",
  month         =  sep,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2309.16039"
}

@ARTICLE{Wang2023-zf,
  title         = "Mitigating the impact of false negatives in dense retrieval
                   with contrastive confidence regularization",
  author        = "Wang, Shiqi and Zhang, Yeqin and Nguyen, Cam-Tu",
  journal       = "arXiv [cs.CL]",
  abstract      = "In open-domain Question Answering (QA), dense retrieval is
                   crucial for finding relevant passages for answer generation.
                   Typically, contrastive learning is used to train a retrieval
                   model that maps passages and queries to the same semantic
                   space. The objective is to make similar ones closer and
                   dissimilar ones further apart. However, training such a
                   system is challenging due to the false negative issue, where
                   relevant passages may be missed during data annotation. Hard
                   negative sampling, which is commonly used to improve
                   contrastive learning, can introduce more noise in training.
                   This is because hard negatives are those closer to a given
                   query, and thus more likely to be false negatives. To address
                   this issue, we propose a novel contrastive confidence
                   regularizer for Noise Contrastive Estimation (NCE) loss, a
                   commonly used loss for dense retrieval. Our analysis shows
                   that the regularizer helps dense retrieval models be more
                   robust against false negatives with a theoretical guarantee.
                   Additionally, we propose a model-agnostic method to filter
                   out noisy negative passages in the dataset, improving any
                   downstream dense retrieval models. Through experiments on
                   three datasets, we demonstrate that our method achieves
                   better retrieval performance in comparison to existing
                   state-of-the-art dense retrieval systems.",
  month         =  dec,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2401.00165"
}

@ARTICLE{Xue2020-nh,
  title         = "{mT5}: A massively multilingual pre-trained text-to-text
                   transformer",
  author        = "Xue, Linting and Constant, Noah and Roberts, Adam and Kale,
                   Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua,
                   Aditya and Raffel, Colin",
  journal       = "arXiv [cs.CL]",
  abstract      = "The recent ``Text-to-Text Transfer Transformer'' (T5)
                   leveraged a unified text-to-text format and scale to attain
                   state-of-the-art results on a wide variety of
                   English-language NLP tasks. In this paper, we introduce mT5,
                   a multilingual variant of T5 that was pre-trained on a new
                   Common Crawl-based dataset covering 101 languages. We detail
                   the design and modified training of mT5 and demonstrate its
                   state-of-the-art performance on many multilingual benchmarks.
                   We also describe a simple technique to prevent ``accidental
                   translation'' in the zero-shot setting, where a generative
                   model chooses to (partially) translate its prediction into
                   the wrong language. All of the code and model checkpoints
                   used in this work are publicly available.",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2010.11934"
}

@ARTICLE{Bisk2019-vg,
  title         = "{PIQA}: Reasoning about physical commonsense in natural
                   language",
  author        = "Bisk, Yonatan and Zellers, Rowan and Bras, Ronan Le and Gao,
                   Jianfeng and Choi, Yejin",
  journal       = "arXiv [cs.CL]",
  abstract      = "To apply eyeshadow without a brush, should I use a cotton
                   swab or a toothpick? Questions requiring this kind of
                   physical commonsense pose a challenge to today's natural
                   language understanding systems. While recent pretrained
                   models (such as BERT) have made progress on question
                   answering over more abstract domains - such as news articles
                   and encyclopedia entries, where text is plentiful - in more
                   physical domains, text is inherently limited due to reporting
                   bias. Can AI systems learn to reliably answer physical
                   common-sense questions without experiencing the physical
                   world? In this paper, we introduce the task of physical
                   commonsense reasoning and a corresponding benchmark dataset
                   Physical Interaction: Question Answering or PIQA. Though
                   humans find the dataset easy (95\% accuracy), large
                   pretrained models struggle (77\%). We provide analysis about
                   the dimensions of knowledge that existing models lack, which
                   offers significant opportunities for future research.",
  month         =  nov,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1911.11641"
}

@ARTICLE{Sakaguchi2019-vl,
  title         = "{WinoGrande}: An adversarial Winograd Schema Challenge at
                   scale",
  author        = "Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula,
                   Chandra and Choi, Yejin",
  journal       = "arXiv [cs.CL]",
  abstract      = "The Winograd Schema Challenge (WSC) (Levesque, Davis, and
                   Morgenstern 2011), a benchmark for commonsense reasoning, is
                   a set of 273 expert-crafted pronoun resolution problems
                   originally designed to be unsolvable for statistical models
                   that rely on selectional preferences or word associations.
                   However, recent advances in neural language models have
                   already reached around 90\% accuracy on variants of WSC. This
                   raises an important question whether these models have truly
                   acquired robust commonsense capabilities or whether they rely
                   on spurious biases in the datasets that lead to an
                   overestimation of the true capabilities of machine
                   commonsense. To investigate this question, we introduce
                   WinoGrande, a large-scale dataset of 44k problems, inspired
                   by the original WSC design, but adjusted to improve both the
                   scale and the hardness of the dataset. The key steps of the
                   dataset construction consist of (1) a carefully designed
                   crowdsourcing procedure, followed by (2) systematic bias
                   reduction using a novel AfLite algorithm that generalizes
                   human-detectable word associations to machine-detectable
                   embedding associations. The best state-of-the-art methods on
                   WinoGrande achieve 59.4-79.1\%, which are 15-35\% below human
                   performance of 94.0\%, depending on the amount of the
                   training data allowed. Furthermore, we establish new
                   state-of-the-art results on five related benchmarks - WSC
                   (90.1\%), DPR (93.1\%), COPA (90.6\%), KnowRef (85.6\%), and
                   Winogender (97.1\%). These results have dual implications: on
                   one hand, they demonstrate the effectiveness of WinoGrande
                   when used as a resource for transfer learning. On the other
                   hand, they raise a concern that we are likely to be
                   overestimating the true capabilities of machine commonsense
                   across all these benchmarks. We emphasize the importance of
                   algorithmic bias reduction in existing and future benchmarks
                   to mitigate such overestimation.",
  month         =  jul,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1907.10641"
}

@ARTICLE{Mihaylov2018-lz,
  title         = "Can a suit of armor conduct electricity? A new dataset for
                   open book question answering",
  author        = "Mihaylov, Todor and Clark, Peter and Khot, Tushar and
                   Sabharwal, Ashish",
  journal       = "arXiv [cs.CL]",
  abstract      = "We present a new kind of question answering dataset,
                   OpenBookQA, modeled after open book exams for assessing human
                   understanding of a subject. The open book that comes with our
                   questions is a set of 1329 elementary level science facts.
                   Roughly 6000 questions probe an understanding of these facts
                   and their application to novel situations. This requires
                   combining an open book fact (e.g., metals conduct
                   electricity) with broad common knowledge (e.g., a suit of
                   armor is made of metal) obtained from other sources. While
                   existing QA datasets over documents or knowledge bases, being
                   generally self-contained, focus on linguistic understanding,
                   OpenBookQA probes a deeper understanding of both the
                   topic---in the context of common knowledge---and the language
                   it is expressed in. Human performance on OpenBookQA is close
                   to 92\%, but many state-of-the-art pre-trained QA methods
                   perform surprisingly poorly, worse than several simple neural
                   baselines we develop. Our oracle experiments designed to
                   circumvent the knowledge retrieval bottleneck demonstrate the
                   value of both the open book and additional facts. We leave it
                   as a challenge to solve the retrieval problem in this
                   multi-hop setting and to close the large gap to human
                   performance.",
  month         =  sep,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1809.02789"
}

@ARTICLE{Welbl2017-cj,
  title         = "Crowdsourcing Multiple Choice Science Questions",
  author        = "Welbl, Johannes and Liu, Nelson F and Gardner, Matt",
  journal       = "arXiv [cs.HC]",
  abstract      = "We present a novel method for obtaining high-quality,
                   domain-targeted multiple choice questions from crowd workers.
                   Generating these questions can be difficult without trading
                   away originality, relevance or diversity in the answer
                   options. Our method addresses these problems by leveraging a
                   large corpus of domain-specific text and a small set of
                   existing questions. It produces model suggestions for
                   document selection and answer distractor choice which aid the
                   human question generation process. With this method we have
                   assembled SciQ, a dataset of 13.7K multiple choice science
                   exam questions (Dataset available at
                   http://allenai.org/data.html). We demonstrate that the method
                   produces in-domain questions by providing an analysis of this
                   new dataset and by showing that humans cannot distinguish the
                   crowdsourced questions from original questions. When using
                   SciQ as additional training data to existing questions, we
                   observe accuracy improvements on real science exams.",
  month         =  jul,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC",
  eprint        = "1707.06209"
}

@ARTICLE{Clark2018-lp,
  title    = "Think you have Solved Question Answering? Try {ARC}, the {AI2}
              Reasoning Challenge",
  author   = "Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar
              and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind",
  abstract = "We present a new question set, text corpus, and baselines
              assembled to encourage AI research in advanced question answering.
              Together, these constitute the AI2 Reasoning Challenge (ARC),
              which requires far more powerful knowledge and reasoning than
              previous challenges such as SQuAD or SNLI. The ARC question set is
              partitioned into a Challenge Set and an Easy Set, where the
              Challenge Set contains only questions answered incorrectly by both
              a retrieval-based algorithm and a word co-occurence algorithm. The
              dataset contains only natural, grade-school science questions
              (authored for human tests), and is the largest public-domain set
              of this kind (7,787 questions). We test several baselines on the
              Challenge Set, including leading neural models from the SQuAD and
              SNLI tasks, and find that none are able to significantly
              outperform a random baseline, reflecting the difficult nature of
              this task. We are also releasing the ARC Corpus, a corpus of 14M
              science sentences relevant to the task, and implementations of the
              three neural baseline models tested. Can your model perform
              better? We pose ARC as a challenge to the community.",
  month    =  mar,
  year     =  2018,
  eprint   = "1803.05457"
}

@ARTICLE{Wang2018-zl,
  title         = "{GLUE}: A multi-task benchmark and analysis platform for
                   natural language understanding",
  author        = "Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill,
                   Felix and Levy, Omer and Bowman, Samuel R",
  journal       = "arXiv [cs.CL]",
  abstract      = "For natural language understanding (NLU) technology to be
                   maximally useful, both practically and as a scientific object
                   of study, it must be general: it must be able to process
                   language in a way that is not exclusively tailored to any one
                   specific task or dataset. In pursuit of this objective, we
                   introduce the General Language Understanding Evaluation
                   benchmark (GLUE), a tool for evaluating and analyzing the
                   performance of models across a diverse range of existing NLU
                   tasks. GLUE is model-agnostic, but it incentivizes sharing
                   knowledge across tasks because certain tasks have very
                   limited training data. We further provide a hand-crafted
                   diagnostic test suite that enables detailed linguistic
                   analysis of NLU models. We evaluate baselines based on
                   current methods for multi-task and transfer learning and find
                   that they do not immediately give substantial improvements
                   over the aggregate performance of training a separate model
                   per task, indicating room for improvement in developing
                   general and robust NLU systems.",
  month         =  apr,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1804.07461"
}

@ARTICLE{Wang2019-hj,
  title         = "{SuperGLUE}: A stickier benchmark for general-purpose
                   language understanding systems",
  author        = "Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and
                   Singh, Amanpreet and Michael, Julian and Hill, Felix and
                   Levy, Omer and Bowman, Samuel R",
  journal       = "arXiv [cs.CL]",
  abstract      = "In the last year, new models and methods for pretraining and
                   transfer learning have driven striking performance
                   improvements across a range of language understanding tasks.
                   The GLUE benchmark, introduced a little over one year ago,
                   offers a single-number metric that summarizes progress on a
                   diverse set of such tasks, but performance on the benchmark
                   has recently surpassed the level of non-expert humans,
                   suggesting limited headroom for further research. In this
                   paper we present SuperGLUE, a new benchmark styled after GLUE
                   with a new set of more difficult language understanding
                   tasks, a software toolkit, and a public leaderboard.
                   SuperGLUE is available at super.gluebenchmark.com.",
  month         =  may,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1905.00537"
}

@ARTICLE{Webber2010-ct,
  title     = "A similarity measure for indefinite rankings",
  author    = "Webber, William and Moffat, Alistair and Zobel, Justin",
  journal   = "ACM Transactions on Information and System Security",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  28,
  number    =  4,
  pages     = "1--38",
  abstract  = "Ranked lists are encountered in research and daily life and it is
               often of interest to compare these lists even when they are
               incomplete or have only some members in common. An example is
               document rankings returned for the same query by different search
               engines. A measure of the similarity between incomplete rankings
               should handle nonconjointness, weight high ranks more heavily
               than low, and be monotonic with increasing depth of evaluation;
               but no measure satisfying all these criteria currently exists. In
               this article, we propose a new measure having these qualities,
               namely rank-biased overlap (RBO). The RBO measure is based on a
               simple probabilistic user model. It provides monotonicity by
               calculating, at a given depth of evaluation, a base score that is
               non-decreasing with additional evaluation, and a maximum score
               that is nonincreasing. An extrapolated score can be calculated
               between these bounds if a point estimate is required. RBO has a
               parameter which determines the strength of the weighting to top
               ranks. We extend RBO to handle tied ranks and rankings of
               different lengths. Finally, we give examples of the use of the
               measure in comparing the results produced by public search
               engines and in assessing retrieval systems in the laboratory.",
  month     =  nov,
  year      =  2010,
  keywords  = "probabilistic models, Rank correlation, ranking",
  doi       = "10.1145/1852102.1852106",
  issn      = "1094-9224,1046-8188"
}

@ARTICLE{Reimers2020-fj,
  title         = "The Curse of Dense Low-Dimensional Information Retrieval for
                   Large Index Sizes",
  author        = "Reimers, Nils and Gurevych, Iryna",
  journal       = "arXiv [cs.IR]",
  abstract      = "Information Retrieval using dense low-dimensional
                   representations recently became popular and showed
                   out-performance to traditional sparse-representations like
                   BM25. However, no previous work investigated how dense
                   representations perform with large index sizes. We show
                   theoretically and empirically that the performance for dense
                   representations decreases quicker than sparse representations
                   for increasing index sizes. In extreme cases, this can even
                   lead to a tipping point where at a certain index size sparse
                   representations outperform dense representations. We show
                   that this behavior is tightly connected to the number of
                   dimensions of the representations: The lower the dimension,
                   the higher the chance for false positives, i.e. returning
                   irrelevant documents.",
  month         =  dec,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "2012.14210"
}

@ARTICLE{Henderson2020-lz,
  title         = "Towards the Systematic Reporting of the Energy and Carbon
                   Footprints of Machine Learning",
  author        = "Henderson, Peter and Hu, Jieru and Romoff, Joshua and
                   Brunskill, Emma and Jurafsky, Dan and Pineau, Joelle",
  journal       = "arXiv [cs.CY]",
  abstract      = "Accurate reporting of energy and carbon usage is essential
                   for understanding the potential climate impacts of machine
                   learning research. We introduce a framework that makes this
                   easier by providing a simple interface for tracking realtime
                   energy consumption and carbon emissions, as well as
                   generating standardized online appendices. Utilizing this
                   framework, we create a leaderboard for energy efficient
                   reinforcement learning algorithms to incentivize responsible
                   research in this area as an example for other areas of
                   machine learning. Finally, based on case studies using our
                   framework, we propose strategies for mitigation of carbon
                   emissions and reduction of energy consumption. By making
                   accounting easier, we hope to further the sustainable
                   development of machine learning experiments and spur more
                   research into energy efficient algorithms.",
  month         =  jan,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY",
  eprint        = "2002.05651"
}

@ARTICLE{Kim2021-uk,
  title         = "{I}-{BERT}: Integer-only {BERT} Quantization",
  author        = "Kim, Sehoon and Gholami, Amir and Yao, Zhewei and Mahoney,
                   Michael W and Keutzer, Kurt",
  journal       = "arXiv [cs.CL]",
  abstract      = "Transformer based models, like BERT and RoBERTa, have
                   achieved state-of-the-art results in many Natural Language
                   Processing tasks. However, their memory footprint, inference
                   latency, and power consumption are prohibitive for many edge
                   processors, and it has been a challenge to deploy these
                   models for edge applications and devices that have resource
                   constraints. While quantization can be a viable solution to
                   this, previous work on quantizing Transformer based models
                   uses floating-point arithmetic during inference, thus
                   limiting model deployment on many edge processors. In this
                   work, we propose a novel integer-only quantization scheme for
                   Transformer based models that quantizes the entire inference
                   process. In particular, we demonstrate how to approximate
                   nonlinear operations in Transformer architectures, e.g.,
                   GELU, Softmax, and Layer Normalization, with lightweight
                   integer computations. We use those approximations in our
                   method, I-BERT, with an end-to-end integer-only inference,
                   and without any floating point calculation. We test our
                   approach on GLUE downstream tasks using RoBERTa-Base and
                   RoBERTa-Large. For both cases, with an 8-bit integer-only
                   quantization scheme, I-BERT achieves similar accuracy as
                   compared to the full-precision baseline.",
  month         =  jan,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2101.01321"
}

@ARTICLE{Khattab2020-zh,
  title         = "{ColBERT}: Efficient and effective passage search via
                   contextualized late interaction over {BERT}",
  author        = "Khattab, Omar and Zaharia, Matei",
  journal       = "arXiv [cs.IR]",
  abstract      = "Recent progress in Natural Language Understanding (NLU) is
                   driving fast-paced advances in Information Retrieval (IR),
                   largely owed to fine-tuning deep language models (LMs) for
                   document ranking. While remarkably effective, the ranking
                   models based on these LMs increase computational cost by
                   orders of magnitude over prior approaches, particularly as
                   they must feed each query-document pair through a massive
                   neural network to compute a single relevance score. To tackle
                   this, we present ColBERT, a novel ranking model that adapts
                   deep LMs (in particular, BERT) for efficient retrieval.
                   ColBERT introduces a late interaction architecture that
                   independently encodes the query and the document using BERT
                   and then employs a cheap yet powerful interaction step that
                   models their fine-grained similarity. By delaying and yet
                   retaining this fine-granular interaction, ColBERT can
                   leverage the expressiveness of deep LMs while simultaneously
                   gaining the ability to pre-compute document representations
                   offline, considerably speeding up query processing. Beyond
                   reducing the cost of re-ranking the documents retrieved by a
                   traditional model, ColBERT's pruning-friendly interaction
                   mechanism enables leveraging vector-similarity indexes for
                   end-to-end retrieval directly from a large document
                   collection. We extensively evaluate ColBERT using two recent
                   passage search datasets. Results show that ColBERT's
                   effectiveness is competitive with existing BERT-based models
                   (and outperforms every non-BERT baseline), while executing
                   two orders-of-magnitude faster and requiring four
                   orders-of-magnitude fewer FLOPs per query.",
  month         =  apr,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "2004.12832"
}

@ARTICLE{Su2021-sh,
  title         = "{CSS}-{LM}: A Contrastive Framework for Semi-supervised
                   Fine-tuning of Pre-trained Language Models",
  author        = "Su, Yusheng and Han, Xu and Lin, Yankai and Zhang, Zhengyan
                   and Liu, Zhiyuan and Li, Peng and Zhou, Jie and Sun, Maosong",
  journal       = "arXiv [cs.CL]",
  abstract      = "Fine-tuning pre-trained language models (PLMs) has
                   demonstrated its effectiveness on various downstream NLP
                   tasks recently. However, in many low-resource scenarios, the
                   conventional fine-tuning strategies cannot sufficiently
                   capture the important semantic features for downstream tasks.
                   To address this issue, we introduce a novel framework (named
                   ``CSS-LM'') to improve the fine-tuning phase of PLMs via
                   contrastive semi-supervised learning. Specifically, given a
                   specific task, we retrieve positive and negative instances
                   from large-scale unlabeled corpora according to their
                   domain-level and class-level semantic relatedness to the
                   task. We then perform contrastive semi-supervised learning on
                   both the retrieved unlabeled and original labeled instances
                   to help PLMs capture crucial task-related semantic features.
                   The experimental results show that CSS-LM achieves better
                   results than the conventional fine-tuning strategy on a
                   series of downstream tasks with few-shot settings, and
                   outperforms the latest supervised contrastive fine-tuning
                   strategies. Our datasets and source code will be available to
                   provide more details.",
  month         =  feb,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2102.03752"
}

@ARTICLE{Zhang2023-kr,
  title         = "Rank-without-{GPT}: Building {GPT}-Independent Listwise
                   Rerankers on Open-Source Large Language Models",
  author        = "Zhang, Xinyu and Hofstätter, Sebastian and Lewis, Patrick and
                   Tang, Raphael and Lin, Jimmy",
  journal       = "arXiv [cs.CL]",
  abstract      = "Listwise rerankers based on large language models (LLM) are
                   the zero-shot state-of-the-art. However, current works in
                   this direction all depend on the GPT models, making it a
                   single point of failure in scientific reproducibility.
                   Moreover, it raises the concern that the current research
                   findings only hold for GPT models but not LLM in general. In
                   this work, we lift this pre-condition and build for the first
                   time effective listwise rerankers without any form of
                   dependency on GPT. Our passage retrieval experiments show
                   that our best list se reranker surpasses the listwise
                   rerankers based on GPT-3.5 by 13\% and achieves 97\%
                   effectiveness of the ones built on GPT-4. Our results also
                   show that the existing training datasets, which were
                   expressly constructed for pointwise ranking, are insufficient
                   for building such listwise rerankers. Instead, high-quality
                   listwise ranking data is required and crucial, calling for
                   further work on building human-annotated listwise data
                   resources.",
  month         =  dec,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2312.02969"
}

@ARTICLE{Pradeep2023-wy,
  title         = "{RankZephyr}: Effective and Robust Zero-Shot Listwise
                   Reranking is a Breeze!",
  author        = "Pradeep, Ronak and Sharifymoghaddam, Sahel and Lin, Jimmy",
  journal       = "arXiv [cs.IR]",
  abstract      = "In information retrieval, proprietary large language models
                   (LLMs) such as GPT-4 and open-source counterparts such as
                   LLaMA and Vicuna have played a vital role in reranking.
                   However, the gap between open-source and closed models
                   persists, with reliance on proprietary, non-transparent
                   models constraining reproducibility. Addressing this gap, we
                   introduce RankZephyr, a state-of-the-art, open-source LLM for
                   listwise zero-shot reranking. RankZephyr not only bridges the
                   effectiveness gap with GPT-4 but in some cases surpasses the
                   proprietary model. Our comprehensive evaluations across
                   several datasets (TREC Deep Learning Tracks; NEWS and COVID
                   from BEIR) showcase this ability. RankZephyr benefits from
                   strategic training choices and is resilient against
                   variations in initial document ordering and the number of
                   documents reranked. Additionally, our model outperforms GPT-4
                   on the NovelEval test set, comprising queries and passages
                   past its training period, which addresses concerns about data
                   contamination. To foster further research in this rapidly
                   evolving field, we provide all code necessary to reproduce
                   our results at https://github.com/castorini/rank\_llm.",
  month         =  dec,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "2312.02724"
}

@ARTICLE{Sinha2021-dz,
  title         = "Masked Language Modeling and the Distributional Hypothesis:
                   Order Word Matters Pre-training for Little",
  author        = "Sinha, Koustuv and Jia, Robin and Hupkes, Dieuwke and Pineau,
                   Joelle and Williams, Adina and Kiela, Douwe",
  journal       = "arXiv [cs.CL]",
  abstract      = "A possible explanation for the impressive performance of
                   masked language model (MLM) pre-training is that such models
                   have learned to represent the syntactic structures prevalent
                   in classical NLP pipelines. In this paper, we propose a
                   different explanation: MLMs succeed on downstream tasks
                   almost entirely due to their ability to model higher-order
                   word co-occurrence statistics. To demonstrate this, we
                   pre-train MLMs on sentences with randomly shuffled word
                   order, and show that these models still achieve high accuracy
                   after fine-tuning on many downstream tasks -- including on
                   tasks specifically designed to be challenging for models that
                   ignore word order. Our models perform surprisingly well
                   according to some parametric syntactic probes, indicating
                   possible deficiencies in how we test representations for
                   syntactic information. Overall, our results show that purely
                   distributional information largely explains the success of
                   pre-training, and underscore the importance of curating
                   challenging evaluation datasets that require deeper
                   linguistic knowledge.",
  month         =  apr,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2104.06644"
}

@ARTICLE{Mialon2023-qv,
  title         = "Augmented Language Models: A survey",
  author        = "Mialon, Grégoire and Dessì, Roberto and Lomeli, Maria and
                   Nalmpantis, Christoforos and Pasunuru, Ram and Raileanu,
                   Roberta and Rozière, Baptiste and Schick, Timo and
                   Dwivedi-Yu, Jane and Celikyilmaz, Asli and Grave, Edouard and
                   LeCun, Yann and Scialom, Thomas",
  journal       = "arXiv [cs.CL]",
  abstract      = "This survey reviews works in which language models (LMs) are
                   augmented with reasoning skills and the ability to use tools.
                   The former is defined as decomposing a potentially complex
                   task into simpler subtasks while the latter consists in
                   calling external modules such as a code interpreter. LMs can
                   leverage these augmentations separately or in combination via
                   heuristics, or learn to do so from demonstrations. While
                   adhering to a standard missing tokens prediction objective,
                   such augmented LMs can use various, possibly non-parametric
                   external modules to expand their context processing ability,
                   thus departing from the pure language modeling paradigm. We
                   therefore refer to them as Augmented Language Models (ALMs).
                   The missing token objective allows ALMs to learn to reason,
                   use tools, and even act, while still performing standard
                   natural language tasks and even outperforming most regular
                   LMs on several benchmarks. In this work, after reviewing
                   current advance in ALMs, we conclude that this new research
                   direction has the potential to address common limitations of
                   traditional LMs such as interpretability, consistency, and
                   scalability issues.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2302.07842"
}

@INPROCEEDINGS{Sakai2007-aj,
  title     = "Alternatives to Bpref",
  author    = "Sakai, Tetsuya",
  booktitle = "Proceedings of the 30th annual international ACM SIGIR conference
               on Research and development in information retrieval",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "71--78",
  abstract  = "Recently, a number of TREC tracks have adopted a retrieval
               effectiveness metric called bpref which has been designed for
               evaluation environments with incomplete relevance data. A
               graded-relevance version of this metric called rpref has also
               been proposed. However, we show that the application of
               Q-measure, normalised Discounted Cumulative Gain (nDCG) or
               Average Precision (AveP)to condensed lists, obtained by ?ltering
               out all unjudged documents from the original ranked lists, is
               actually a better solution to the incompleteness problem than
               bpref. Furthermore, we show that the use of graded relevance
               boosts the robustness of IR evaluation to incompleteness and
               therefore that Q-measure and nDCG based on condensed lists are
               the best choices. To this end, we use four graded-relevance test
               collections from NTCIR to compare ten different IR metrics in
               terms of system ranking stability and pairwise discriminative
               power.",
  series    = "SIGIR '07",
  month     =  jul,
  year      =  2007,
  keywords  = "graded relevance, evaluation metrics, test collection",
  doi       = "10.1145/1277741.1277756",
  isbn      =  9781595935977
}

@ARTICLE{Rogers2021-ot,
  title         = "Changing the World by Changing the Data",
  author        = "Rogers, Anna",
  journal       = "arXiv [cs.CL]",
  abstract      = "NLP community is currently investing a lot more research and
                   resources into development of deep learning models than
                   training data. While we have made a lot of progress, it is
                   now clear that our models learn all kinds of spurious
                   patterns, social biases, and annotation artifacts.
                   Algorithmic solutions have so far had limited success. An
                   alternative that is being actively discussed is more careful
                   design of datasets so as to deliver specific signals. This
                   position paper maps out the arguments for and against data
                   curation, and argues that fundamentally the point is moot:
                   curation already is and will be happening, and it is changing
                   the world. The question is only how much thought we want to
                   invest into that process.",
  month         =  may,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2105.13947"
}

@ARTICLE{Shahmohammadi2023-bo,
  title         = "{ViPE}: Visualise Pretty-much Everything",
  author        = "Shahmohammadi, Hassan and Ghosh, Adhiraj and Lensch, Hendrik
                   P A",
  journal       = "arXiv [cs.CL]",
  abstract      = "Figurative and non-literal expressions are profoundly
                   integrated in human communication. Visualising such
                   expressions allow us to convey our creative thoughts, and
                   evoke nuanced emotions. Recent text-to-image models like
                   Stable Diffusion, on the other hand, struggle to depict
                   non-literal expressions. Recent works primarily deal with
                   this issue by compiling humanly annotated datasets on a small
                   scale, which not only demands specialised expertise but also
                   proves highly inefficient. To address this issue, we
                   introduce ViPE: Visualise Pretty-much Everything. ViPE offers
                   a series of lightweight and robust language models that have
                   been trained on a large-scale set of lyrics with noisy visual
                   descriptions that represent their implicit meaning. The
                   synthetic visual descriptions are generated by GPT3.5 relying
                   on neither human annotations nor images. ViPE effectively
                   expresses any arbitrary piece of text into a visualisable
                   description, enabling meaningful and high-quality image
                   generation. We provide compelling evidence that ViPE is more
                   robust than GPT3.5 in synthesising visual elaborations. ViPE
                   also exhibits an understanding of figurative expressions
                   comparable to human experts, providing a powerful and
                   open-source backbone to many downstream applications such as
                   music video and caption generation.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2310.10543"
}

@ARTICLE{Fei2023-az,
  title         = "Extending context window of Large Language Models via
                   semantic compression",
  author        = "Fei, Weizhi and Niu, Xueyan and Zhou, Pingyi and Hou, Lu and
                   Bai, Bo and Deng, Lei and Han, Wei",
  journal       = "arXiv [cs.CL]",
  abstract      = "Transformer-based Large Language Models (LLMs) often impose
                   limitations on the length of the text input to ensure the
                   generation of fluent and relevant responses. This constraint
                   restricts their applicability in scenarios involving long
                   texts. We propose a novel semantic compression method that
                   enables generalization to texts that are 6-8 times longer,
                   without incurring significant computational costs or
                   requiring fine-tuning. Our proposed framework draws
                   inspiration from source coding in information theory and
                   employs a pre-trained model to reduce the semantic redundancy
                   of long inputs before passing them to the LLMs for downstream
                   tasks. Experimental results demonstrate that our method
                   effectively extends the context window of LLMs across a range
                   of tasks including question answering, summarization,
                   few-shot learning, and information retrieval. Furthermore,
                   the proposed semantic compression method exhibits consistent
                   fluency in text generation while reducing the associated
                   computational overhead.",
  month         =  dec,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2312.09571"
}

@ARTICLE{Thiel2023-bj,
  title    = "Identifying and Eliminating {CSAM} in Generative {ML} Training
              Data and Models",
  author   = "Thiel, David",
  abstract = "Generative Machine Learning models have been well documented as
              being able to produce explicit adult content, including child
              sexual abuse material (CSAM) as well as to alter benign imagery of
              a cl...",
  year     =  2023,
  doi      = "10.25740/kh752sm9123"
}

@ARTICLE{Birhane2021-gt,
  title         = "Multimodal datasets: misogyny, pornography, and malignant
                   stereotypes",
  author        = "Birhane, Abeba and Prabhu, Vinay Uday and Kahembwe, Emmanuel",
  journal       = "arXiv [cs.CY]",
  abstract      = "We have now entered the era of trillion parameter machine
                   learning models trained on billion-sized datasets scraped
                   from the internet. The rise of these gargantuan datasets has
                   given rise to formidable bodies of critical work that has
                   called for caution while generating these large datasets.
                   These address concerns surrounding the dubious curation
                   practices used to generate these datasets, the sordid quality
                   of alt-text data available on the world wide web, the
                   problematic content of the CommonCrawl dataset often used as
                   a source for training large language models, and the
                   entrenched biases in large-scale visio-linguistic models
                   (such as OpenAI's CLIP model) trained on opaque datasets
                   (WebImageText). In the backdrop of these specific calls of
                   caution, we examine the recently released LAION-400M dataset,
                   which is a CLIP-filtered dataset of Image-Alt-text pairs
                   parsed from the Common-Crawl dataset. We found that the
                   dataset contains, troublesome and explicit images and text
                   pairs of rape, pornography, malign stereotypes, racist and
                   ethnic slurs, and other extremely problematic content. We
                   outline numerous implications, concerns and downstream harms
                   regarding the current state of large scale datasets while
                   raising open questions for various stakeholders including the
                   AI community, regulators, policy makers and data subjects.",
  month         =  oct,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY",
  eprint        = "2110.01963"
}

@ARTICLE{Birhane2023-zg,
  title         = "Into the {LAIONs} den: Investigating hate in multimodal
                   datasets",
  author        = "Birhane, Abeba and Prabhu, Vinay and Han, Sang and Boddeti,
                   Vishnu Naresh and Luccioni, Alexandra Sasha",
  journal       = "arXiv [cs.CY]",
  abstract      = "'Scale the model, scale the data, scale the compute' is the
                   reigning sentiment in the world of generative AI today. While
                   the impact of model scaling has been extensively studied, we
                   are only beginning to scratch the surface of data scaling and
                   its consequences. This is especially of critical importance
                   in the context of vision-language datasets such as LAION.
                   These datasets are continually growing in size and are built
                   based on large-scale internet dumps such as the Common Crawl,
                   which is known to have numerous drawbacks ranging from
                   quality, legality, and content. The datasets then serve as
                   the backbone for large generative models, contributing to the
                   operationalization and perpetuation of harmful societal and
                   historical biases and stereotypes. In this paper, we
                   investigate the effect of scaling datasets on hateful content
                   through a comparative audit of two datasets: LAION-400M and
                   LAION-2B. Our results show that hate content increased by
                   nearly 12\% with dataset scale, measured both qualitatively
                   and quantitatively using a metric that we term as Hate
                   Content Rate (HCR). We also found that filtering dataset
                   contents based on Not Safe For Work (NSFW) values calculated
                   based on images alone does not exclude all the harmful
                   content in alt-text. Instead, we found that trace amounts of
                   hateful, targeted, and aggressive text remain even when
                   carrying out conservative filtering. We end with a reflection
                   and a discussion of the significance of our results for
                   dataset curation and usage in the AI community. Code and the
                   meta-data assets curated in this paper are publicly available
                   at https://github.com/vinayprabhu/hate\_scaling. Content
                   warning: This paper contains examples of hateful text that
                   might be disturbing, distressing, and/or offensive.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY",
  eprint        = "2311.03449"
}

@INPROCEEDINGS{Matic2020-pb,
  title     = "Identifying Sensitive {URLs} at Web-Scale",
  author    = "Matic, Srdjan and Iordanou, Costas and Smaragdakis, Georgios and
               Laoutaris, Nikolaos",
  booktitle = "Proceedings of the ACM Internet Measurement Conference",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  oct,
  year      =  2020,
  doi       = "10.1145/3419394.3423653",
  isbn      =  9781450381383,
  language  = "en"
}

@ARTICLE{Fiesler2020-ga,
  title    = "No Robots, Spiders, or Scrapers: Legal and Ethical Regulation of
              Data Collection Methods in Social Media Terms of Service",
  author   = "Fiesler, Casey and Beard, Nathan and Keegan, Brian C",
  journal  = "Proceedings of the International AAAI Conference on Web and Social
              Media",
  volume   =  14,
  pages    = "187--196",
  month    =  may,
  year     =  2020,
  issn     = "2334-0770,2334-0770",
  language = "en"
}

@ARTICLE{Saphra2023-zm,
  title         = "First tragedy, then parse: History repeats itself in the New
                   Era of large language models",
  author        = "Saphra, Naomi and Fleisig, Eve and Cho, Kyunghyun and Lopez,
                   Adam",
  journal       = "arXiv [cs.CL]",
  abstract      = "Many NLP researchers are experiencing an existential crisis
                   triggered by the astonishing success of ChatGPT and other
                   systems based on large language models (LLMs). After such a
                   disruptive change to our understanding of the field, what is
                   left to do? Taking a historical lens, we look for guidance
                   from the first era of LLMs, which began in 2005 with large
                   $n$-gram models for machine translation. We identify durable
                   lessons from the first era, and more importantly, we identify
                   evergreen problems where NLP researchers can continue to make
                   meaningful contributions in areas where LLMs are ascendant.
                   Among these lessons, we discuss the primacy of hardware
                   advancement in shaping the availability and importance of
                   scale, as well as the urgent challenge of quality evaluation,
                   both automated and human. We argue that disparities in scale
                   are transient and that researchers can work to reduce them;
                   that data, rather than hardware, is still a bottleneck for
                   many meaningful applications; that meaningful evaluation
                   informed by actual use is still an open problem; and that
                   there is still room for speculative approaches.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2311.05020"
}

@INPROCEEDINGS{Gao2023-bb,
  title     = "Precise zero-shot dense retrieval without relevance labels",
  author    = "Gao, Luyu and Ma, Xueguang and Lin, Jimmy and Callan, Jamie",
  booktitle = "Proceedings of the 61st Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "1762--1777",
  abstract  = "Luyu Gao, Xueguang Ma, Jimmy Lin, Jamie Callan. Proceedings of
               the 61st Annual Meeting of the Association for Computational
               Linguistics (Volume 1: Long Papers). 2023.",
  year      =  2023,
  doi       = "10.18653/v1/2023.acl-long.99"
}

@MISC{Mazzarino_undated-vw,
  title     = "{NERPII}: A Python Library to Perform Named Entity Recognition
               and Generate Personal Identifiable Information",
  author    = "Mazzarino, Simona and Minieri, Andrea and Gilli, Luca",
  publisher = "ceur-ws.org",
  year      =  2023
}

@MISC{UnknownUnknown-qj,
  title = "Gemini: A Family of Highly Capable Multimodal Models"
}

@ARTICLE{Shazeer2020-sk,
  title         = "{GLU} Variants Improve Transformer",
  author        = "Shazeer, Noam",
  journal       = "arXiv [cs.LG]",
  abstract      = "Gated Linear Units (arXiv:1612.08083) consist of the
                   component-wise product of two linear projections, one of
                   which is first passed through a sigmoid function. Variations
                   on GLU are possible, using different nonlinear (or even
                   linear) functions in place of sigmoid. We test these variants
                   in the feed-forward sublayers of the Transformer
                   (arXiv:1706.03762) sequence-to-sequence model, and find that
                   some of them yield quality improvements over the
                   typically-used ReLU or GELU activations.",
  month         =  feb,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2002.05202"
}

@ARTICLE{Ackerman2019-md,
  title     = "Syntactic and cognitive issues in investigating gendered
               coreference",
  author    = "Ackerman, Lauren",
  journal   = "Glossa a journal of general linguistics",
  publisher = "Ubiquity Press, Ltd.",
  volume    =  4,
  number    =  1,
  month     =  oct,
  year      =  2019,
  doi       = "10.5334/gjgl.721",
  issn      = "2397-1835",
  language  = "en"
}

@ARTICLE{Lee2021-gy,
  title         = "Deduplicating Training Data Makes Language Models Better",
  author        = "Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and
                   Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and
                   Carlini, Nicholas",
  journal       = "arXiv [cs.CL]",
  abstract      = "We find that existing language modeling datasets contain many
                   near-duplicate examples and long repetitive substrings. As a
                   result, over 1\% of the unprompted output of language models
                   trained on these datasets is copied verbatim from the
                   training data. We develop two tools that allow us to
                   deduplicate training datasets -- for example removing from C4
                   a single 61 word English sentence that is repeated over
                   60,000 times. Deduplication allows us to train models that
                   emit memorized text ten times less frequently and require
                   fewer train steps to achieve the same or better accuracy. We
                   can also reduce train-test overlap, which affects over 4\% of
                   the validation set of standard datasets, thus allowing for
                   more accurate evaluation. We release code for reproducing our
                   work and performing dataset deduplication at
                   https://github.com/google-research/deduplicate-text-datasets.",
  month         =  jul,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2107.06499"
}

@ARTICLE{Cao2019-jc,
  title         = "Toward Gender-Inclusive Coreference Resolution",
  author        = "Cao, Yang Trista and Daumé, III, Hal",
  journal       = "arXiv [cs.CL]",
  abstract      = "Correctly resolving textual mentions of people fundamentally
                   entails making inferences about those people. Such inferences
                   raise the risk of systemic biases in coreference resolution
                   systems, including biases that can harm binary and non-binary
                   trans and cis stakeholders. To better understand such biases,
                   we foreground nuanced conceptualizations of gender from
                   sociology and sociolinguistics, and develop two new datasets
                   for interrogating bias in crowd annotations and in existing
                   coreference resolution systems. Through these studies,
                   conducted on English text, we confirm that without
                   acknowledging and building systems that recognize the
                   complexity of gender, we build systems that lead to many
                   potential harms.",
  month         =  oct,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1910.13913"
}

@ARTICLE{Blodgett2020-cg,
  title         = "Language (Technology) is Power: A Critical Survey of ``Bias''
                   in {NLP}",
  author        = "Blodgett, Su Lin and Barocas, Solon and Daumé, III, Hal and
                   Wallach, Hanna",
  journal       = "arXiv [cs.CL]",
  abstract      = "We survey 146 papers analyzing ``bias'' in NLP systems,
                   finding that their motivations are often vague, inconsistent,
                   and lacking in normative reasoning, despite the fact that
                   analyzing ``bias'' is an inherently normative process. We
                   further find that these papers' proposed quantitative
                   techniques for measuring or mitigating ``bias'' are poorly
                   matched to their motivations and do not engage with the
                   relevant literature outside of NLP. Based on these findings,
                   we describe the beginnings of a path forward by proposing
                   three recommendations that should guide work analyzing
                   ``bias'' in NLP systems. These recommendations rest on a
                   greater recognition of the relationships between language and
                   social hierarchies, encouraging researchers and practitioners
                   to articulate their conceptualizations of ``bias''---i.e.,
                   what kinds of system behaviors are harmful, in what ways, to
                   whom, and why, as well as the normative reasoning underlying
                   these statements---and to center work around the lived
                   experiences of members of communities affected by NLP
                   systems, while interrogating and reimagining the power
                   relations between technologists and such communities.",
  month         =  may,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2005.14050"
}

@ARTICLE{Dehghani2021-rc,
  title         = "The Benchmark Lottery",
  author        = "Dehghani, Mostafa and Tay, Yi and Gritsenko, Alexey A and
                   Zhao, Zhe and Houlsby, Neil and Diaz, Fernando and Metzler,
                   Donald and Vinyals, Oriol",
  journal       = "arXiv [cs.LG]",
  abstract      = "The world of empirical machine learning (ML) strongly relies
                   on benchmarks in order to determine the relative
                   effectiveness of different algorithms and methods. This paper
                   proposes the notion of ``a benchmark lottery'' that describes
                   the overall fragility of the ML benchmarking process. The
                   benchmark lottery postulates that many factors, other than
                   fundamental algorithmic superiority, may lead to a method
                   being perceived as superior. On multiple benchmark setups
                   that are prevalent in the ML community, we show that the
                   relative performance of algorithms may be altered
                   significantly simply by choosing different benchmark tasks,
                   highlighting the fragility of the current paradigms and
                   potential fallacious interpretation derived from benchmarking
                   ML methods. Given that every benchmark makes a statement
                   about what it perceives to be important, we argue that this
                   might lead to biased progress in the community. We discuss
                   the implications of the observed phenomena and provide
                   recommendations on mitigating them using multiple machine
                   learning domains and communities as use cases, including
                   natural language processing, computer vision, information
                   retrieval, recommender systems, and reinforcement learning.",
  month         =  jul,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2107.07002"
}

@INPROCEEDINGS{Santhanam2022-ip,
  title     = "{ColBERTv2}: Effective and efficient retrieval via lightweight
               late interaction",
  author    = "Santhanam, Keshav and Khattab, Omar and Saad-Falcon, Jon and
               Potts, Christopher and Zaharia, Matei",
  booktitle = "Proceedings of the 2022 Conference of the North American Chapter
               of the Association for Computational Linguistics: Human Language
               Technologies",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "3715--3734",
  abstract  = "Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher
               Potts, Matei Zaharia. Proceedings of the 2022 Conference of the
               North American Chapter of the Association for Computational
               Linguistics: Human Language Technologies. 2022.",
  year      =  2022,
  doi       = "10.18653/v1/2022.naacl-main.272"
}

@ARTICLE{Robertson1976-rq,
  title     = "Relevance weighting of search terms",
  author    = "Robertson, S E and Jones, K Sparck",
  journal   = "Journal of the American Society for Information Science. American
               Society for Information Science",
  publisher = "Wiley",
  volume    =  27,
  number    =  3,
  pages     = "129--146",
  abstract  = "This paper examines statistical techniques for exploiting
               relevance information to weight search terms. These techniques
               are presented as a natural extension of weighting methods using
               information about the distribution of index terms in documents in
               general. A series of relevance weighting functions is derived and
               is justified by theoretical considerations. In particular, it is
               shown that specific weighted search methods are implied by a
               general probabilistic theory of retrieval. Different applications
               of relevance weighting are illustrated by experimental results
               for test collections.",
  month     =  may,
  year      =  1976,
  doi       = "10.1002/asi.4630270302",
  issn      = "1097-4571,0002-8231",
  language  = "en"
}

@ARTICLE{Khramtsova2023-bg,
  title         = "Selecting which Dense Retriever to use for Zero-Shot Search",
  author        = "Khramtsova, Ekaterina and Zhuang, Shengyao and
                   Baktashmotlagh, Mahsa and Wang, Xi and Zuccon, Guido",
  journal       = "arXiv [cs.IR]",
  abstract      = "We propose the new problem of choosing which dense retrieval
                   model to use when searching on a new collection for which no
                   labels are available, i.e. in a zero-shot setting. Many dense
                   retrieval models are readily available. Each model however is
                   characterized by very differing search effectiveness -- not
                   just on the test portion of the datasets in which the dense
                   representations have been learned but, importantly, also
                   across different datasets for which data was not used to
                   learn the dense representations. This is because dense
                   retrievers typically require training on a large amount of
                   labeled data to achieve satisfactory search effectiveness in
                   a specific dataset or domain. Moreover, effectiveness gains
                   obtained by dense retrievers on datasets for which they are
                   able to observe labels during training, do not necessarily
                   generalise to datasets that have not been observed during
                   training. This is however a hard problem: through empirical
                   experimentation we show that methods inspired by recent work
                   in unsupervised performance evaluation with the presence of
                   domain shift in the area of computer vision and machine
                   learning are not effective for choosing highly performing
                   dense retrievers in our setup. The availability of reliable
                   methods for the selection of dense retrieval models in
                   zero-shot settings that do not require the collection of
                   labels for evaluation would allow to streamline the
                   widespread adoption of dense retrieval. This is therefore an
                   important new problem we believe the information retrieval
                   community should consider. Implementation of methods, along
                   with raw result files and analysis scripts are made publicly
                   available at https://www.github.com/anonymized.",
  month         =  sep,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "2309.09403",
  doi           = "10.48550/arXiv.2309.09403"
}

@ARTICLE{Yang2022-wb,
  title         = "Parameter-efficient zero-shot transfer for cross-language
                   dense retrieval with adapters",
  author        = "Yang, Eugene and Nair, Suraj and Lawrie, Dawn and Mayfield,
                   James and Oard, Douglas W",
  journal       = "arXiv [cs.IR]",
  abstract      = "A popular approach to creating a zero-shot cross-language
                   retrieval model is to substitute a monolingual pretrained
                   language model in the retrieval model with a multilingual
                   pretrained language model such as Multilingual BERT. This
                   multilingual model is fined-tuned to the retrieval task with
                   monolingual data such as English MS MARCO using the same
                   training recipe as the monolingual retrieval model used.
                   However, such transferred models suffer from mismatches in
                   the languages of the input text during training and
                   inference. In this work, we propose transferring monolingual
                   retrieval models using adapters, a parameter-efficient
                   component for a transformer network. By adding adapters
                   pretrained on language tasks for a specific language with
                   task-specific adapters, prior work has shown that the
                   adapter-enhanced models perform better than fine-tuning the
                   entire model when transferring across languages in various
                   NLP tasks. By constructing dense retrieval models with
                   adapters, we show that models trained with monolingual data
                   are more effective than fine-tuning the entire model when
                   transferring to a Cross Language Information Retrieval (CLIR)
                   setting. However, we found that the prior suggestion of
                   replacing the language adapters to match the target language
                   at inference time is suboptimal for dense retrieval models.
                   We provide an in-depth analysis of this discrepancy between
                   other cross-language NLP tasks and CLIR.",
  month         =  dec,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "2212.10448"
}

@ARTICLE{Singh2023-sw,
  title         = "Beyond human data: Scaling self-training for problem-solving
                   with language models",
  author        = "Singh, Avi and Co-Reyes, John D and Agarwal, Rishabh and
                   Anand, Ankesh and Patil, Piyush and Liu, Peter J and
                   Harrison, James and Lee, Jaehoon and Xu, Kelvin and Parisi,
                   Aaron and Kumar, Abhishek and Alemi, Alex and Rizkowsky, Alex
                   and Nova, Azade and Adlam, Ben and Bohnet, Bernd and Sedghi,
                   Hanie and Mordatch, Igor and Simpson, Isabelle and Gur,
                   Izzeddin and Snoek, Jasper and Pennington, Jeffrey and Hron,
                   Jiri and Kenealy, Kathleen and Swersky, Kevin and Mahajan,
                   Kshiteej and Culp, Laura and Xiao, Lechao and Bileschi,
                   Maxwell L and Constant, Noah and Novak, Roman and Liu,
                   Rosanne and Warkentin, Tris and Qian, Yundi and Dyer, Ethan
                   and Neyshabur, Behnam and Sohl-Dickstein, Jascha and Fiedel,
                   Noah",
  journal       = "arXiv [cs.LG]",
  abstract      = "Fine-tuning language models~(LMs) on human-generated data
                   remains a prevalent practice. However, the performance of
                   such models is often limited by the quantity and diversity of
                   high-quality human data. In this paper, we explore whether we
                   can go beyond human data on tasks where we have access to
                   scalar feedback, for example, on math problems where one can
                   verify correctness. To do so, we investigate a simple
                   self-training method based on expectation-maximization, which
                   we call ReST$^{EM}$, where we (1) generate samples from the
                   model and filter them using binary feedback, (2) fine-tune
                   the model on these samples, and (3) repeat this process a few
                   times. Testing on advanced MATH reasoning and APPS coding
                   benchmarks using PaLM-2 models, we find that ReST$^{EM}$
                   scales favorably with model size and significantly surpasses
                   fine-tuning only on human data. Overall, our findings suggest
                   self-training with feedback can substantially reduce
                   dependence on human-generated data.",
  month         =  dec,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2312.06585"
}

@ARTICLE{Ding2020-lu,
  title         = "{RocketQA}: An optimized training approach to dense passage
                   retrieval for open-domain question answering",
  author        = "Ding, Yingqi Qu Yuchen and Liu, Jing and Liu, Kai and Ren,
                   Ruiyang and Zhao, Xin and Dong, Daxiang and Wu, Hua and Wang,
                   Haifeng",
  journal       = "arXiv [cs.CL]",
  abstract      = "In open-domain question answering, dense passage retrieval
                   has become a new paradigm to retrieve relevant passages for
                   answer finding. Typically, the dual-encoder architecture is
                   adopted to learn dense representations of questions and
                   passages for matching. However, it is difficult to train an
                   effective dual-encoder due to the challenges including the
                   discrepancy between training and inference, the existence of
                   unlabeled positives and limited training data. To address
                   these challenges, we propose an optimized training approach,
                   called RocketQA, to improving dense passage retrieval. We
                   make three major technical contributions in RocketQA, namely
                   cross-batch negatives, denoised negative sampling and data
                   augmentation. Extensive experiments show that RocketQA
                   significantly outperforms previous state-of-the-art models on
                   both MSMARCO and Natural Questions. Besides, built upon
                   RocketQA, we achieve the first rank at the leaderboard of
                   MSMARCO Passage Ranking Task.",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2010.08191"
}

@ARTICLE{Xiong2020-de,
  title         = "Approximate Nearest Neighbor negative contrastive learning
                   for dense text retrieval",
  author        = "Xiong, Lee and Xiong, Chenyan and Li, Ye and Tang, Kwok-Fung
                   and Liu, Jialin and Bennett, Paul and Ahmed, Junaid and
                   Overwijk, Arnold",
  journal       = "arXiv [cs.IR]",
  abstract      = "Conducting text retrieval in a dense learned representation
                   space has many intriguing advantages over sparse retrieval.
                   Yet the effectiveness of dense retrieval (DR) often requires
                   combination with sparse retrieval. In this paper, we identify
                   that the main bottleneck is in the training mechanisms, where
                   the negative instances used in training are not
                   representative of the irrelevant documents in testing. This
                   paper presents Approximate nearest neighbor Negative
                   Contrastive Estimation (ANCE), a training mechanism that
                   constructs negatives from an Approximate Nearest Neighbor
                   (ANN) index of the corpus, which is parallelly updated with
                   the learning process to select more realistic negative
                   training instances. This fundamentally resolves the
                   discrepancy between the data distribution used in the
                   training and testing of DR. In our experiments, ANCE boosts
                   the BERT-Siamese DR model to outperform all competitive dense
                   and sparse retrieval baselines. It nearly matches the
                   accuracy of sparse-retrieval-and-BERT-reranking using
                   dot-product in the ANCE-learned representation space and
                   provides almost 100x speed-up.",
  month         =  jul,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "2007.00808"
}

@ARTICLE{Turski2023-fi,
  title         = "{CCpdf}: Building a high quality corpus for visually rich
                   documents from web crawl data",
  author        = "Turski, Michał and Stanisławek, Tomasz and Kaczmarek, Karol
                   and Dyda, Paweł and Graliński, Filip",
  journal       = "arXiv [cs.CL]",
  abstract      = "In recent years, the field of document understanding has
                   progressed a lot. A significant part of this progress has
                   been possible thanks to the use of language models pretrained
                   on large amounts of documents. However, pretraining corpora
                   used in the domain of document understanding are single
                   domain, monolingual, or nonpublic. Our goal in this paper is
                   to propose an efficient pipeline for creating a big-scale,
                   diverse, multilingual corpus of PDF files from all over the
                   Internet using Common Crawl, as PDF files are the most
                   canonical types of documents as considered in document
                   understanding. We analysed extensively all of the steps of
                   the pipeline and proposed a solution which is a trade-off
                   between data quality and processing time. We also share a
                   CCpdf corpus in a form or an index of PDF files along with a
                   script for downloading them, which produces a collection
                   useful for language model pretraining. The dataset and tools
                   published with this paper offer researchers the opportunity
                   to develop even better multilingual language models.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2304.14953"
}

@ARTICLE{Abadji2022-im,
  title         = "Towards a cleaner document-oriented multilingual crawled
                   corpus",
  author        = "Abadji, Julien and Suarez, Pedro Ortiz and Romary, Laurent
                   and Sagot, Benoît",
  journal       = "arXiv [cs.CL]",
  abstract      = "The need for raw large raw corpora has dramatically increased
                   in recent years with the introduction of transfer learning
                   and semi-supervised learning methods to Natural Language
                   Processing. And while there have been some recent attempts to
                   manually curate the amount of data necessary to train large
                   language models, the main way to obtain this data is still
                   through automatic web crawling. In this paper we take the
                   existing multilingual web corpus OSCAR and its pipeline
                   Ungoliant that extracts and classifies data from Common Crawl
                   at the line level, and propose a set of improvements and
                   automatic annotations in order to produce a new
                   document-oriented version of OSCAR that could prove more
                   suitable to pre-train large generative language models as
                   well as hopefully other applications in Natural Language
                   Processing and Digital Humanities.",
  month         =  jan,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2201.06642"
}

@ARTICLE{Mathew2020-la,
  title         = "{HateXplain}: A benchmark dataset for explainable hate speech
                   detection",
  author        = "Mathew, Binny and Saha, Punyajoy and Yimam, Seid Muhie and
                   Biemann, Chris and Goyal, Pawan and Mukherjee, Animesh",
  journal       = "arXiv [cs.CL]",
  abstract      = "Hate speech is a challenging issue plaguing the online social
                   media. While better models for hate speech detection are
                   continuously being developed, there is little research on the
                   bias and interpretability aspects of hate speech. In this
                   paper, we introduce HateXplain, the first benchmark hate
                   speech dataset covering multiple aspects of the issue. Each
                   post in our dataset is annotated from three different
                   perspectives: the basic, commonly used 3-class classification
                   (i.e., hate, offensive or normal), the target community
                   (i.e., the community that has been the victim of hate
                   speech/offensive speech in the post), and the rationales,
                   i.e., the portions of the post on which their labelling
                   decision (as hate, offensive or normal) is based. We utilize
                   existing state-of-the-art models and observe that even models
                   that perform very well in classification do not score high on
                   explainability metrics like model plausibility and
                   faithfulness. We also observe that models, which utilize the
                   human rationales for training, perform better in reducing
                   unintended bias towards target communities. We have made our
                   code and dataset public at
                   https://github.com/punyajoy/HateXplain",
  month         =  dec,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2012.10289"
}

@ARTICLE{Wang2023-ar,
  title         = "Generative {AI} for math: Part {I} -- {MathPile}: A
                   billion-token-scale pretraining corpus for math",
  author        = "Wang, Zengzhi and Xia, Rui and Liu, Pengfei",
  journal       = "arXiv [cs.CL]",
  abstract      = "High-quality, large-scale corpora are the cornerstone of
                   building foundation models. In this work, we introduce
                   \textsc{MathPile}, a diverse and high-quality math-centric
                   corpus comprising about 9.5 billion tokens. Throughout its
                   creation, we adhered to the principle of ``\emph{less is
                   more}'', firmly believing in the supremacy of data quality
                   over quantity, even in the pre-training phase. Our meticulous
                   data collection and processing efforts included a complex
                   suite of preprocessing, prefiltering, language
                   identification, cleaning, filtering, and deduplication,
                   ensuring the high quality of our corpus. Furthermore, we
                   performed data contamination detection on downstream
                   benchmark test sets to eliminate duplicates. We hope our
                   \textsc{MathPile} can help to enhance the mathematical
                   reasoning abilities of language models. We plan to
                   open-source different versions of \mathpile with the scripts
                   used for processing, to facilitate future developments in
                   this field.",
  month         =  dec,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2312.17120"
}

@ARTICLE{Birhane2023-hg,
  title         = "On hate scaling laws for data-swamps",
  author        = "Birhane, Abeba and Prabhu, Vinay and Han, Sang and Boddeti,
                   Vishnu Naresh",
  journal       = "arXiv [cs.CY]",
  abstract      = "`Scale the model, scale the data, scale the GPU-farms' is the
                   reigning sentiment in the world of generative AI today. While
                   model scaling has been extensively studied, data scaling and
                   its downstream impacts remain under explored. This is
                   especially of critical importance in the context of
                   visio-linguistic datasets whose main source is the World Wide
                   Web, condensed and packaged as the CommonCrawl dump. This
                   large scale data-dump, which is known to have numerous
                   drawbacks, is repeatedly mined and serves as the
                   data-motherlode for large generative models. In this paper,
                   we: 1) investigate the effect of scaling datasets on hateful
                   content through a comparative audit of the LAION-400M and
                   LAION-2B-en, containing 400 million and 2 billion samples
                   respectively, and 2) evaluate the downstream impact of scale
                   on visio-linguistic models trained on these dataset variants
                   by measuring racial bias of the models trained on them using
                   the Chicago Face Dataset (CFD) as a probe. Our results show
                   that 1) the presence of hateful content in datasets, when
                   measured with a Hate Content Rate (HCR) metric on the
                   inferences of the Pysentimiento hate-detection Natural
                   Language Processing (NLP) model, increased by nearly $12\%$
                   and 2) societal biases and negative stereotypes were also
                   exacerbated with scale on the models we evaluated. As scale
                   increased, the tendency of the model to associate images of
                   human faces with the `human being' class over 7 other
                   offensive classes reduced by half. Furthermore, for the Black
                   female category, the tendency of the model to associate their
                   faces with the `criminal' class doubled, while quintupling
                   for Black male faces. We present a qualitative and historical
                   analysis of the model audit results, reflect on our findings
                   and its implications for dataset curation practice, and close
                   with a summary of our findings and potential future work to
                   be done in this area.",
  month         =  jun,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY",
  eprint        = "2306.13141",
  doi           = "10.48550/arXiv.2306.13141"
}

@ARTICLE{Jo2019-gq,
  title         = "Lessons from archives: Strategies for collecting
                   sociocultural data in machine learning",
  author        = "Jo, Eun Seo and Gebru, Timnit",
  journal       = "arXiv [cs.LG]",
  abstract      = "A growing body of work shows that many problems in fairness,
                   accountability, transparency, and ethics in machine learning
                   systems are rooted in decisions surrounding the data
                   collection and annotation process. In spite of its
                   fundamental nature however, data collection remains an
                   overlooked part of the machine learning (ML) pipeline. In
                   this paper, we argue that a new specialization should be
                   formed within ML that is focused on methodologies for data
                   collection and annotation: efforts that require institutional
                   frameworks and procedures. Specifically for sociocultural
                   data, parallels can be drawn from archives and libraries.
                   Archives are the longest standing communal effort to gather
                   human information and archive scholars have already developed
                   the language and procedures to address and discuss many
                   challenges pertaining to data collection such as consent,
                   power, inclusivity, transparency, and ethics \& privacy. We
                   discuss these five key approaches in document collection
                   practices in archives that can inform data collection in
                   sociocultural ML. By showing data collection practices from
                   another field, we encourage ML research to be more cognizant
                   and systematic in data collection and draw from
                   interdisciplinary expertise.",
  month         =  dec,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1912.10389",
  doi           = "10.1145/3351095.3372829"
}

@ARTICLE{Massanari2017-xp,
  title     = "\#Gamergate and The Fappening: How Reddit’s algorithm,
               governance, and culture support toxic technocultures",
  author    = "Massanari, Adrienne",
  journal   = "New media \& society",
  publisher = "SAGE Publications",
  volume    =  19,
  number    =  3,
  pages     = "329--346",
  abstract  = "This article considers how the social-news and community site
               Reddit.com has become a hub for anti-feminist activism. Examining
               two recent cases of what are defined as “toxic technocultures”
               (\#Gamergate and The Fappening), this work describes how Reddit’s
               design, algorithm, and platform politics implicitly support these
               kinds of cultures. In particular, this piece focuses on the ways
               in which Reddit’s karma point system, aggregation of material
               across subreddits, ease of subreddit and user account creation,
               governance structure, and policies around offensive content serve
               to provide fertile ground for anti-feminist and misogynistic
               activism. The ways in which these events and communities reflect
               certain problematic aspects of geek masculinity are also
               considered. This research is informed by the results of a
               long-term participant-observation and ethnographic study into
               Reddit’s culture and community and is grounded in actor-network
               theory.",
  month     =  mar,
  year      =  2017,
  doi       = "10.1177/1461444815608807",
  issn      = "1461-4448,1461-7315",
  language  = "en"
}

@ARTICLE{Gehman2020-oh,
  title         = "{RealToxicityPrompts}: Evaluating neural toxic degeneration
                   in language models",
  author        = "Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and
                   Choi, Yejin and Smith, Noah A",
  journal       = "arXiv [cs.CL]",
  abstract      = "Pretrained neural language models (LMs) are prone to
                   generating racist, sexist, or otherwise toxic language which
                   hinders their safe deployment. We investigate the extent to
                   which pretrained LMs can be prompted to generate toxic
                   language, and the effectiveness of controllable text
                   generation algorithms at preventing such toxic degeneration.
                   We create and release RealToxicityPrompts, a dataset of 100K
                   naturally occurring, sentence-level prompts derived from a
                   large corpus of English web text, paired with toxicity scores
                   from a widely-used toxicity classifier. Using
                   RealToxicityPrompts, we find that pretrained LMs can
                   degenerate into toxic text even from seemingly innocuous
                   prompts. We empirically assess several controllable
                   generation methods, and find that while data- or
                   compute-intensive methods (e.g., adaptive pretraining on
                   non-toxic data) are more effective at steering away from
                   toxicity than simpler solutions (e.g., banning ``bad''
                   words), no current method is failsafe against neural toxic
                   degeneration. To pinpoint the potential cause of such
                   persistent toxic degeneration, we analyze two web text
                   corpora used to pretrain several LMs (including GPT-2;
                   Radford et. al, 2019), and find a significant amount of
                   offensive, factually unreliable, and otherwise toxic content.
                   Our work provides a test bed for evaluating toxic generations
                   by LMs and stresses the need for better data selection
                   processes for pretraining.",
  month         =  sep,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2009.11462"
}

@ARTICLE{Sap2021-ne,
  title         = "Annotators with attitudes: How annotator beliefs and
                   identities bias toxic language detection",
  author        = "Sap, Maarten and Swayamdipta, Swabha and Vianna, Laura and
                   Zhou, Xuhui and Choi, Yejin and Smith, Noah A",
  journal       = "arXiv [cs.CL]",
  abstract      = "The perceived toxicity of language can vary based on
                   someone's identity and beliefs, but this variation is often
                   ignored when collecting toxic language datasets, resulting in
                   dataset and model biases. We seek to understand the who, why,
                   and what behind biases in toxicity annotations. In two online
                   studies with demographically and politically diverse
                   participants, we investigate the effect of annotator
                   identities (who) and beliefs (why), drawing from social
                   psychology research about hate speech, free speech, racist
                   beliefs, political leaning, and more. We disentangle what is
                   annotated as toxic by considering posts with three
                   characteristics: anti-Black language, African American
                   English (AAE) dialect, and vulgarity. Our results show strong
                   associations between annotator identity and beliefs and their
                   ratings of toxicity. Notably, more conservative annotators
                   and those who scored highly on our scale for racist beliefs
                   were less likely to rate anti-Black language as toxic, but
                   more likely to rate AAE as toxic. We additionally present a
                   case study illustrating how a popular toxicity detection
                   system's ratings inherently reflect only specific beliefs and
                   perspectives. Our findings call for contextualizing toxicity
                   labels in social variables, which raises immense implications
                   for toxic language annotation and detection.",
  month         =  nov,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2111.07997"
}

@ARTICLE{Welbl2021-do,
  title         = "Challenges in detoxifying language models",
  author        = "Welbl, Johannes and Glaese, Amelia and Uesato, Jonathan and
                   Dathathri, Sumanth and Mellor, John and Hendricks, Lisa Anne
                   and Anderson, Kirsty and Kohli, Pushmeet and Coppin, Ben and
                   Huang, Po-Sen",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large language models (LM) generate remarkably fluent text
                   and can be efficiently adapted across NLP tasks. Measuring
                   and guaranteeing the quality of generated text in terms of
                   safety is imperative for deploying LMs in the real world; to
                   this end, prior work often relies on automatic evaluation of
                   LM toxicity. We critically discuss this approach, evaluate
                   several toxicity mitigation strategies with respect to both
                   automatic and human evaluation, and analyze consequences of
                   toxicity mitigation in terms of model bias and LM quality. We
                   demonstrate that while basic intervention strategies can
                   effectively optimize previously established automatic metrics
                   on the RealToxicityPrompts dataset, this comes at the cost of
                   reduced LM coverage for both texts about, and dialects of,
                   marginalized groups. Additionally, we find that human raters
                   often disagree with high automatic toxicity scores after
                   strong toxicity reduction interventions -- highlighting
                   further the nuances involved in careful evaluation of LM
                   toxicity.",
  month         =  sep,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2109.07445"
}

@ARTICLE{Saad-Falcon2023-xe,
  title         = "{ARES}: An Automated evaluation framework for
                   retrieval-augmented generation systems",
  author        = "Saad-Falcon, Jon and Khattab, Omar and Potts, Christopher and
                   Zaharia, Matei",
  journal       = "arXiv [cs.CL]",
  abstract      = "Evaluating retrieval-augmented generation (RAG) systems
                   traditionally relies on hand annotations for input queries,
                   passages to retrieve, and responses to generate. We introduce
                   ARES, an Automated RAG Evaluation System, for evaluating RAG
                   systems along the dimensions of context relevance, answer
                   faithfulness, and answer relevance. Using synthetic training
                   data, ARES finetunes lightweight LM judges to assess the
                   quality of individual RAG components. To mitigate potential
                   prediction errors, ARES utilizes a small set of
                   human-annotated datapoints for prediction-powered inference
                   (PPI). Across six different knowledge-intensive tasks in KILT
                   and SuperGLUE, ARES accurately evaluates RAG systems while
                   using a few hundred human annotations during evaluation.
                   Furthermore, ARES judges remain effective across domain
                   shifts, proving accurate even after changing the type of
                   queries and/or documents used in the evaluated RAG systems.
                   We make our datasets and code for replication and deployment
                   available at https://github.com/stanford-futuredata/ARES.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2311.09476",
  doi           = "10.48550/arXiv.2311.09476"
}

@ARTICLE{Xia2022-pt,
  title         = "Training trajectories of language models across scales",
  author        = "Xia, Mengzhou and Artetxe, Mikel and Zhou, Chunting and Lin,
                   Xi Victoria and Pasunuru, Ramakanth and Chen, Danqi and
                   Zettlemoyer, Luke and Stoyanov, Ves",
  journal       = "arXiv [cs.CL]",
  abstract      = "Scaling up language models has led to unprecedented
                   performance gains, but little is understood about how the
                   training dynamics change as models get larger. How do
                   language models of different sizes learn during pre-training?
                   Why do larger language models demonstrate more desirable
                   behaviors? In this paper, we analyze the intermediate
                   training checkpoints of differently sized OPT models (Zhang
                   et al.,2022)--from 125M to 175B parameters--on next-token
                   prediction, sequence-level generation, and downstream tasks.
                   We find that 1) at a given perplexity and independent of
                   model sizes, a similar subset of training tokens see the most
                   significant reduction in loss, with the rest stagnating or
                   showing double-descent behavior; 2) early in training, all
                   models learn to reduce the perplexity of grammatical
                   sequences that contain hallucinations, with small models
                   halting at this suboptimal distribution and larger ones
                   eventually learning to assign these sequences lower
                   probabilities; 3) perplexity is a strong predictor of
                   in-context learning performance on 74 multiple-choice tasks
                   from BIG-Bench, and this holds independent of the model size.
                   Together, these results show that perplexity is more
                   predictive of model behaviors than model size or training
                   computation.",
  month         =  dec,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2212.09803"
}

@ARTICLE{Dernoncourt2017-zj,
  title     = "De-identification of patient notes with recurrent neural networks",
  author    = "Dernoncourt, Franck and Lee, Ji Young and Uzuner, Ozlem and
               Szolovits, Peter",
  journal   = "Journal of the American Medical Informatics Association: JAMIA",
  publisher = "Oxford University Press",
  volume    =  24,
  number    =  3,
  pages     = "596--606",
  abstract  = "OBJECTIVE: Patient notes in electronic health records (EHRs) may
               contain critical information for medical investigations. However,
               the vast majority of medical investigators can only access
               de-identified notes, in order to protect the confidentiality of
               patients. In the United States, the Health Insurance Portability
               and Accountability Act (HIPAA) defines 18 types of protected
               health information that needs to be removed to de-identify
               patient notes. Manual de-identification is impractical given the
               size of electronic health record databases, the limited number of
               researchers with access to non-de-identified notes, and the
               frequent mistakes of human annotators. A reliable automated
               de-identification system would consequently be of high value.
               MATERIALS AND METHODS: We introduce the first de-identification
               system based on artificial neural networks (ANNs), which requires
               no handcrafted features or rules, unlike existing systems. We
               compare the performance of the system with state-of-the-art
               systems on two datasets: the i2b2 2014 de-identification
               challenge dataset, which is the largest publicly available
               de-identification dataset, and the MIMIC de-identification
               dataset, which we assembled and is twice as large as the i2b2
               2014 dataset. RESULTS: Our ANN model outperforms the
               state-of-the-art systems. It yields an F1-score of 97.85 on the
               i2b2 2014 dataset, with a recall of 97.38 and a precision of
               98.32, and an F1-score of 99.23 on the MIMIC de-identification
               dataset, with a recall of 99.25 and a precision of 99.21.
               CONCLUSION: Our findings support the use of ANNs for
               de-identification of patient notes, as they show better
               performance than previously published systems while requiring no
               manual feature engineering.",
  month     =  may,
  year      =  2017,
  keywords  = "de-identification; medical language processing; neural networks",
  doi       = "10.1093/jamia/ocw156",
  pmc       = "PMC7787254",
  pmid      =  28040687,
  issn      = "1067-5027,1527-974X",
  language  = "en"
}

@ARTICLE{Luccioni2021-cs,
  title         = "What's in the box? A preliminary analysis of undesirable
                   content in the Common Crawl corpus",
  author        = "Luccioni, Alexandra Sasha and Viviano, Joseph D",
  journal       = "arXiv [cs.CL]",
  abstract      = "Whereas much of the success of the current generation of
                   neural language models has been driven by increasingly large
                   training corpora, relatively little research has been
                   dedicated to analyzing these massive sources of textual data.
                   In this exploratory analysis, we delve deeper into the Common
                   Crawl, a colossal web corpus that is extensively used for
                   training language models. We find that it contains a
                   significant amount of undesirable content, including hate
                   speech and sexually explicit content, even after filtering
                   procedures. We discuss the potential impacts of this content
                   on language models and conclude with future research
                   directions and a more mindful approach to corpus collection
                   and analysis.",
  month         =  may,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2105.02732"
}

@ARTICLE{Magnusson2023-sp,
  title         = "Paloma: A benchmark for evaluating language model fit",
  author        = "Magnusson, Ian and Bhagia, Akshita and Hofmann, Valentin and
                   Soldaini, Luca and Jha, Ananya Harsh and Tafjord, Oyvind and
                   Schwenk, Dustin and Walsh, Evan Pete and Elazar, Yanai and
                   Lo, Kyle and Groeneveld, Dirk and Beltagy, Iz and Hajishirzi,
                   Hannaneh and Smith, Noah A and Richardson, Kyle and Dodge,
                   Jesse",
  journal       = "arXiv [cs.CL]",
  abstract      = "Language models (LMs) commonly report perplexity on
                   monolithic data held out from training. Implicitly or
                   explicitly, this data is composed of
                   domains$\unicode{x2013}$varying distributions of language.
                   Rather than assuming perplexity on one distribution
                   extrapolates to others, Perplexity Analysis for Language
                   Model Assessment (Paloma), measures LM fit to 585 text
                   domains, ranging from nytimes.com to r/depression on Reddit.
                   We invite submissions to our benchmark and organize results
                   by comparability based on compliance with guidelines such as
                   removal of benchmark contamination from pretraining.
                   Submissions can also record parameter and training token
                   count to make comparisons of Pareto efficiency for
                   performance as a function of these measures of cost. We
                   populate our benchmark with results from 6 baselines
                   pretrained on popular corpora. In case studies, we
                   demonstrate analyses that are possible with Paloma, such as
                   finding that pretraining without data beyond Common Crawl
                   leads to inconsistent fit to many domains.",
  month         =  dec,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2312.10523"
}

@ARTICLE{Xue2020-pt,
  title         = "{mT5}: A massively multilingual pre-trained text-to-text
                   transformer",
  author        = "Xue, Linting and Constant, Noah and Roberts, Adam and Kale,
                   Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua,
                   Aditya and Raffel, Colin",
  journal       = "arXiv [cs.CL]",
  abstract      = "The recent ``Text-to-Text Transfer Transformer'' (T5)
                   leveraged a unified text-to-text format and scale to attain
                   state-of-the-art results on a wide variety of
                   English-language NLP tasks. In this paper, we introduce mT5,
                   a multilingual variant of T5 that was pre-trained on a new
                   Common Crawl-based dataset covering 101 languages. We detail
                   the design and modified training of mT5 and demonstrate its
                   state-of-the-art performance on many multilingual benchmarks.
                   We also describe a simple technique to prevent ``accidental
                   translation'' in the zero-shot setting, where a generative
                   model chooses to (partially) translate its prediction into
                   the wrong language. All of the code and model checkpoints
                   used in this work are publicly available.",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2010.11934"
}

@ARTICLE{Chronopoulou2021-kx,
  title         = "Efficient hierarchical domain adaptation for pretrained
                   language models",
  author        = "Chronopoulou, Alexandra and Peters, Matthew E and Dodge,
                   Jesse",
  journal       = "arXiv [cs.CL]",
  abstract      = "The remarkable success of large language models has been
                   driven by dense models trained on massive unlabeled,
                   unstructured corpora. These corpora typically contain text
                   from diverse, heterogeneous sources, but information about
                   the source of the text is rarely used during training.
                   Transferring their knowledge to a target domain is typically
                   done by continuing training in-domain. In this paper, we
                   introduce a method to permit domain adaptation to many
                   diverse domains using a computationally efficient adapter
                   approach. Our method is based on the observation that textual
                   domains are partially overlapping, and we represent domains
                   as a hierarchical tree structure where each node in the tree
                   is associated with a set of adapter weights. When combined
                   with a frozen pretrained language model, this approach
                   enables parameter sharing among related domains, while
                   avoiding negative interference between unrelated ones.
                   Experimental results with GPT-2 and a large fraction of the
                   100 most represented websites in C4 show across-the-board
                   improvements in-domain. We additionally provide an inference
                   time algorithm for a held-out domain and show that averaging
                   over multiple paths through the tree enables further gains in
                   generalization, while adding only a marginal cost to
                   inference.",
  month         =  dec,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2112.08786"
}

@ARTICLE{Chen2023-kp,
  title         = "Dense {X} retrieval: What retrieval granularity should we
                   use?",
  author        = "Chen, Tong and Wang, Hongwei and Chen, Sihao and Yu, Wenhao
                   and Ma, Kaixin and Zhao, Xinran and Zhang, Hongming and Yu,
                   Dong",
  journal       = "arXiv [cs.CL]",
  abstract      = "Dense retrieval has become a prominent method to obtain
                   relevant context or world knowledge in open-domain NLP tasks.
                   When we use a learned dense retriever on a retrieval corpus
                   at inference time, an often-overlooked design choice is the
                   retrieval unit in which the corpus is indexed, e.g. document,
                   passage, or sentence. We discover that the retrieval unit
                   choice significantly impacts the performance of both
                   retrieval and downstream tasks. Distinct from the typical
                   approach of using passages or sentences, we introduce a novel
                   retrieval unit, proposition, for dense retrieval.
                   Propositions are defined as atomic expressions within text,
                   each encapsulating a distinct factoid and presented in a
                   concise, self-contained natural language format. We conduct
                   an empirical comparison of different retrieval granularity.
                   Our results reveal that proposition-based retrieval
                   significantly outperforms traditional passage or
                   sentence-based methods in dense retrieval. Moreover,
                   retrieval by proposition also enhances the performance of
                   downstream QA tasks, since the retrieved texts are more
                   condensed with question-relevant information, reducing the
                   need for lengthy input tokens and minimizing the inclusion of
                   extraneous, irrelevant information.",
  month         =  dec,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2312.06648"
}

@ARTICLE{Singh2023-dk,
  title    = "The Confidence-Competence Gap in Large Language Models: A
              Cognitive Study",
  author   = "Singh, Aniket Kumar and Devkota, Suman and Lamichhane, Bishal and
              Dhakal, Uttam and Dhakal, Chandra",
  abstract = "Large Language Models (LLMs) have acquired ubiquitous attention
              for their performances across diverse domains. Our study here
              searches through LLMs' cognitive abilities and confidence
              dynamics. We dive deep into understanding the alignment between
              their self-assessed confidence and actual performance. We exploit
              these models with diverse sets of questionnaires and real-world
              scenarios and extract how LLMs exhibit confidence in their
              responses. Our findings reveal intriguing instances where models
              demonstrate high confidence even when they answer incorrectly.
              This is reminiscent of the Dunning-Kruger effect observed in human
              psychology. In contrast, there are cases where models exhibit low
              confidence with correct answers revealing potential
              underestimation biases. Our results underscore the need for a
              deeper understanding of their cognitive processes. By examining
              the nuances of LLMs' self-assessment mechanism, this investigation
              provides noteworthy revelations that serve to advance the
              functionalities and broaden the potential applications of these
              formidable language models.",
  month    =  sep,
  year     =  2023,
  eprint   = "2309.16145"
}

@ARTICLE{Lukas2023-rn,
  title         = "Analyzing leakage of Personally Identifiable Information in
                   language models",
  author        = "Lukas, Nils and Salem, Ahmed and Sim, Robert and Tople,
                   Shruti and Wutschitz, Lukas and Zanella-Béguelin, Santiago",
  journal       = "arXiv [cs.LG]",
  abstract      = "Language Models (LMs) have been shown to leak information
                   about training data through sentence-level membership
                   inference and reconstruction attacks. Understanding the risk
                   of LMs leaking Personally Identifiable Information (PII) has
                   received less attention, which can be attributed to the false
                   assumption that dataset curation techniques such as scrubbing
                   are sufficient to prevent PII leakage. Scrubbing techniques
                   reduce but do not prevent the risk of PII leakage: in
                   practice scrubbing is imperfect and must balance the
                   trade-off between minimizing disclosure and preserving the
                   utility of the dataset. On the other hand, it is unclear to
                   which extent algorithmic defenses such as differential
                   privacy, designed to guarantee sentence- or user-level
                   privacy, prevent PII disclosure. In this work, we introduce
                   rigorous game-based definitions for three types of PII
                   leakage via black-box extraction, inference, and
                   reconstruction attacks with only API access to an LM. We
                   empirically evaluate the attacks against GPT-2 models
                   fine-tuned with and without defenses in three domains: case
                   law, health care, and e-mails. Our main contributions are (i)
                   novel attacks that can extract up to 10$\times$ more PII
                   sequences than existing attacks, (ii) showing that
                   sentence-level differential privacy reduces the risk of PII
                   disclosure but still leaks about 3\% of PII sequences, and
                   (iii) a subtle connection between record-level membership
                   inference and PII reconstruction. Code to reproduce all
                   experiments in the paper is available at
                   https://github.com/microsoft/analysing\_pii\_leakage.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2302.00539"
}

@INPROCEEDINGS{Lison2021-xn,
  title     = "Anonymisation models for text data: State of the art, challenges
               and future directions",
  author    = "Lison, Pierre and Pilán, Ildikó and Sanchez, David and Batet,
               Montserrat and Øvrelid, Lilja",
  booktitle = "Proceedings of the 59th Annual Meeting of the Association for
               Computational Linguistics and the 11th International Joint
               Conference on Natural Language Processing (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "4188--4203",
  abstract  = "Pierre Lison, Ildikó Pilán, David Sanchez, Montserrat Batet,
               Lilja Øvrelid. Proceedings of the 59th Annual Meeting of the
               Association for Computational Linguistics and the 11th
               International Joint Conference on Natural Language Processing
               (Volume 1: Long Papers). 2021.",
  year      =  2021,
  doi       = "10.18653/v1/2021.acl-long.323"
}

@INPROCEEDINGS{Lo2020-db,
  title     = "{S2ORC}: The semantic scholar open research corpus",
  author    = "Lo, Kyle and Wang, Lucy Lu and Neumann, Mark and Kinney, Rodney
               and Weld, Daniel",
  booktitle = "Proceedings of the 58th Annual Meeting of the Association for
               Computational Linguistics",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  year      =  2020,
  doi       = "10.18653/v1/2020.acl-main.447"
}

@ARTICLE{Allal2023-xq,
  title         = "{SantaCoder}: don't reach for the stars!",
  author        = "Allal, Loubna Ben and Li, Raymond and Kocetkov, Denis and
                   Mou, Chenghao and Akiki, Christopher and Ferrandis, Carlos
                   Munoz and Muennighoff, Niklas and Mishra, Mayank and Gu, Alex
                   and Dey, Manan and Umapathi, Logesh Kumar and Anderson,
                   Carolyn Jane and Zi, Yangtian and Poirier, Joel Lamy and
                   Schoelkopf, Hailey and Troshin, Sergey and Abulkhanov, Dmitry
                   and Romero, Manuel and Lappert, Michael and De Toni,
                   Francesco and del Río, Bernardo García and Liu, Qian and
                   Bose, Shamik and Bhattacharyya, Urvashi and Zhuo, Terry Yue
                   and Yu, Ian and Villegas, Paulo and Zocca, Marco and
                   Mangrulkar, Sourab and Lansky, David and Nguyen, Huu and
                   Contractor, Danish and Villa, Luis and Li, Jia and Bahdanau,
                   Dzmitry and Jernite, Yacine and Hughes, Sean and Fried,
                   Daniel and Guha, Arjun and de Vries, Harm and von Werra,
                   Leandro",
  journal       = "arXiv [cs.SE]",
  abstract      = "The BigCode project is an open-scientific collaboration
                   working on the responsible development of large language
                   models for code. This tech report describes the progress of
                   the collaboration until December 2022, outlining the current
                   state of the Personally Identifiable Information (PII)
                   redaction pipeline, the experiments conducted to de-risk the
                   model architecture, and the experiments investigating better
                   preprocessing methods for the training data. We train 1.1B
                   parameter models on the Java, JavaScript, and Python subsets
                   of The Stack and evaluate them on the MultiPL-E text-to-code
                   benchmark. We find that more aggressive filtering of
                   near-duplicates can further boost performance and,
                   surprisingly, that selecting files from repositories with 5+
                   GitHub stars deteriorates performance significantly. Our best
                   model outperforms previous open-source multilingual code
                   generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in
                   both left-to-right generation and infilling on the Java,
                   JavaScript, and Python portions of MultiPL-E, despite being a
                   substantially smaller model. All models are released under an
                   OpenRAIL license at https://hf.co/bigcode.",
  month         =  jan,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.SE",
  eprint        = "2301.03988"
}

@ARTICLE{Jiang2024-zj,
  title         = "Investigating data contamination for pre-training language
                   models",
  author        = "Jiang, Minhao and Liu, Ken Ziyu and Zhong, Ming and
                   Schaeffer, Rylan and Ouyang, Siru and Han, Jiawei and Koyejo,
                   Sanmi",
  journal       = "arXiv [cs.CL]",
  abstract      = "Language models pre-trained on web-scale corpora demonstrate
                   impressive capabilities on diverse downstream tasks. However,
                   there is increasing concern whether such capabilities might
                   arise from evaluation datasets being included in the
                   pre-training corpus -- a phenomenon known as \textit{data
                   contamination} -- in a manner that artificially increases
                   performance. There has been little understanding of how this
                   potential contamination might influence LMs' performance on
                   downstream tasks. In this paper, we explore the impact of
                   data contamination at the pre-training stage by pre-training
                   a series of GPT-2 models \textit{from scratch}. We highlight
                   the effect of both text contamination (\textit{i.e.}\\ input
                   text of the evaluation samples) and ground-truth
                   contamination (\textit{i.e.}\\ the prompts asked on the input
                   and the desired outputs) from evaluation data. We also
                   investigate the effects of repeating contamination for
                   various downstream tasks. Additionally, we examine the
                   prevailing n-gram-based definitions of contamination within
                   current LLM reports, pinpointing their limitations and
                   inadequacy. Our findings offer new insights into data
                   contamination's effects on language model capabilities and
                   underscore the need for independent, comprehensive
                   contamination assessments in LLM studies.",
  month         =  jan,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2401.06059"
}

@ARTICLE{Longpre2023-vp,
  title         = "A pretrainer's guide to training data: Measuring the effects
                   of data age, domain coverage, quality, \& toxicity",
  author        = "Longpre, Shayne and Yauney, Gregory and Reif, Emily and Lee,
                   Katherine and Roberts, Adam and Zoph, Barret and Zhou, Denny
                   and Wei, Jason and Robinson, Kevin and Mimno, David and
                   Ippolito, Daphne",
  journal       = "arXiv [cs.CL]",
  abstract      = "Pretraining is the preliminary and fundamental step in
                   developing capable language models (LM). Despite this,
                   pretraining data design is critically under-documented and
                   often guided by empirically unsupported intuitions. To
                   address this, we pretrain 28 1.5B parameter decoder-only
                   models, training on data curated (1) at different times, (2)
                   with varying toxicity and quality filters, and (3) with
                   different domain compositions. First, we quantify the effect
                   of pretraining data age. A temporal shift between evaluation
                   data and pretraining data leads to performance degradation,
                   which is not overcome by finetuning. Second, we explore the
                   effect of quality and toxicity filters, showing a trade-off
                   between performance on standard benchmarks and risk of toxic
                   generations. Our findings indicate there does not exist a
                   one-size-fits-all solution to filtering training data. We
                   also find that the effects of different types of filtering
                   are not predictable from text domain characteristics. Lastly,
                   we empirically validate that the inclusion of heterogeneous
                   data sources, like books and web, is broadly beneficial and
                   warrants greater prioritization. These findings constitute
                   the largest set of experiments to validate, quantify, and
                   expose many undocumented intuitions about text pretraining,
                   which we hope will help support more informed data-centric
                   decisions in LM development.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.13169"
}

@ARTICLE{Davis2021-rg,
  title     = "Emotional consequences and attention rewards: the social effects
               of ratings on Reddit",
  author    = "Davis, Jenny L and Graham, Timothy",
  journal   = "Information, communication and society",
  publisher = "Informa UK Limited",
  volume    =  24,
  number    =  5,
  pages     = "649--666",
  month     =  apr,
  year      =  2021,
  doi       = "10.1080/1369118x.2021.1874476",
  issn      = "1369-118X,1468-4462",
  language  = "en"
}

@INPROCEEDINGS{Weninger2013-kz,
  title     = "An exploration of discussion threads in social news sites",
  author    = "Weninger, Tim and Zhu, Xihao Avi and Han, Jiawei",
  booktitle = "Proceedings of the 2013 IEEE/ACM International Conference on
               Advances in Social Networks Analysis and Mining",
  publisher = "ACM",
  address   = "New York, NY, USA",
  month     =  aug,
  year      =  2013,
  doi       = "10.1145/2492517.2492646",
  isbn      =  9781450322409
}

@ARTICLE{Gemini-Team2023-gn,
  title         = "Gemini: A family of highly capable multimodal models",
  author        = "{Gemini Team} and Anil, Rohan and Borgeaud, Sebastian and Wu,
                   Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and
                   Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and
                   Hauth, Anja and Millican, Katie and Silver, David and Petrov,
                   Slav and Johnson, Melvin and Antonoglou, Ioannis and
                   Schrittwieser, Julian and Glaese, Amelia and Chen, Jilin and
                   Pitler, Emily and Lillicrap, Timothy and Lazaridou, Angeliki
                   and Firat, Orhan and Molloy, James and Isard, Michael and
                   Barham, Paul R and Hennigan, Tom and Lee, Benjamin and Viola,
                   Fabio and Reynolds, Malcolm and Xu, Yuanzhong and Doherty,
                   Ryan and Collins, Eli and Meyer, Clemens and Rutherford,
                   Eliza and Moreira, Erica and Ayoub, Kareem and Goel, Megha
                   and Tucker, George and Piqueras, Enrique and Krikun, Maxim
                   and Barr, Iain and Savinov, Nikolay and Danihelka, Ivo and
                   Roelofs, Becca and White, Anaïs and Andreassen, Anders and
                   von Glehn, Tamara and Yagati, Lakshman and Kazemi, Mehran and
                   Gonzalez, Lucas and Khalman, Misha and Sygnowski, Jakub and
                   Frechette, Alexandre and Smith, Charlotte and Culp, Laura and
                   Proleev, Lev and Luan, Yi and Chen, Xi and Lottes, James and
                   Schucher, Nathan and Lebron, Federico and Rrustemi, Alban and
                   Clay, Natalie and Crone, Phil and Kocisky, Tomas and Zhao,
                   Jeffrey and Perz, Bartek and Yu, Dian and Howard, Heidi and
                   Bloniarz, Adam and Rae, Jack W and Lu, Han and Sifre, Laurent
                   and Maggioni, Marcello and Alcober, Fred and Garrette, Dan
                   and Barnes, Megan and Thakoor, Shantanu and Austin, Jacob and
                   Barth-Maron, Gabriel and Wong, William and Joshi, Rishabh and
                   Chaabouni, Rahma and Fatiha, Deeni and Ahuja, Arun and Liu,
                   Ruibo and Li, Yunxuan and Cogan, Sarah and Chen, Jeremy and
                   Jia, Chao and Gu, Chenjie and Zhang, Qiao and Grimstad,
                   Jordan and Hartman, Ale Jakse and Chadwick, Martin and Tomar,
                   Gaurav Singh and Garcia, Xavier and Senter, Evan and Taropa,
                   Emanuel and Pillai, Thanumalayan Sankaranarayana and Devlin,
                   Jacob and Laskin, Michael and Casas, Diego de Las and Valter,
                   Dasha and Tao, Connie and Blanco, Lorenzo and Badia, Adrià
                   Puigdomènech and Reitter, David and Chen, Mianna and Brennan,
                   Jenny and Rivera, Clara and Brin, Sergey and Iqbal, Shariq
                   and Surita, Gabriela and Labanowski, Jane and Rao, Abhi and
                   Winkler, Stephanie and Parisotto, Emilio and Gu, Yiming and
                   Olszewska, Kate and Zhang, Yujing and Addanki, Ravi and
                   Miech, Antoine and Louis, Annie and Shafey, Laurent El and
                   Teplyashin, Denis and Brown, Geoff and Catt, Elliot and
                   Attaluri, Nithya and Balaguer, Jan and Xiang, Jackie and
                   Wang, Pidong and Ashwood, Zoe and Briukhov, Anton and Webson,
                   Albert and Ganapathy, Sanjay and Sanghavi, Smit and Kannan,
                   Ajay and Chang, Ming-Wei and Stjerngren, Axel and Djolonga,
                   Josip and Sun, Yuting and Bapna, Ankur and Aitchison, Matthew
                   and Pejman, Pedram and Michalewski, Henryk and Yu, Tianhe and
                   Wang, Cindy and Love, Juliette and Ahn, Junwhan and Bloxwich,
                   Dawn and Han, Kehang and Humphreys, Peter and Sellam,
                   Thibault and Bradbury, James and Godbole, Varun and
                   Samangooei, Sina and Damoc, Bogdan and Kaskasoli, Alex and
                   Arnold, Sébastien M R and Vasudevan, Vijay and Agrawal,
                   Shubham and Riesa, Jason and Lepikhin, Dmitry and Tanburn,
                   Richard and Srinivasan, Srivatsan and Lim, Hyeontaek and
                   Hodkinson, Sarah and Shyam, Pranav and Ferret, Johan and
                   Hand, Steven and Garg, Ankush and Paine, Tom Le and Li, Jian
                   and Li, Yujia and Giang, Minh and Neitz, Alexander and Abbas,
                   Zaheer and York, Sarah and Reid, Machel and Cole, Elizabeth
                   and Chowdhery, Aakanksha and Das, Dipanjan and Rogozińska,
                   Dominika and Nikolaev, Vitaly and Sprechmann, Pablo and Nado,
                   Zachary and Zilka, Lukas and Prost, Flavien and He, Luheng
                   and Monteiro, Marianne and Mishra, Gaurav and Welty, Chris
                   and Newlan, Josh and Jia, Dawei and Allamanis, Miltiadis and
                   Hu, Clara Huiyi and de Liedekerke, Raoul and Gilmer, Justin
                   and Saroufim, Carl and Rijhwani, Shruti and Hou, Shaobo and
                   Shrivastava, Disha and Baddepudi, Anirudh and Goldin, Alex
                   and Ozturel, Adnan and Cassirer, Albin and Xu, Yunhan and
                   Sohn, Daniel and Sachan, Devendra and Amplayo, Reinald Kim
                   and Swanson, Craig and Petrova, Dessie and Narayan, Shashi
                   and Guez, Arthur and Brahma, Siddhartha and Landon, Jessica
                   and Patel, Miteyan and Zhao, Ruizhe and Villela, Kevin and
                   Wang, Luyu and Jia, Wenhao and Rahtz, Matthew and Giménez,
                   Mai and Yeung, Legg and Lin, Hanzhao and Keeling, James and
                   Georgiev, Petko and Mincu, Diana and Wu, Boxi and Haykal,
                   Salem and Saputro, Rachel and Vodrahalli, Kiran and Qin,
                   James and Cankara, Zeynep and Sharma, Abhanshu and Fernando,
                   Nick and Hawkins, Will and Neyshabur, Behnam and Kim, Solomon
                   and Hutter, Adrian and Agrawal, Priyanka and Castro-Ros, Alex
                   and van den Driessche, George and Wang, Tao and Yang, Fan and
                   Chang, Shuo-Yiin and Komarek, Paul and McIlroy, Ross and
                   Lučić, Mario and Zhang, Guodong and Farhan, Wael and Sharman,
                   Michael and Natsev, Paul and Michel, Paul and Cheng, Yong and
                   Bansal, Yamini and Qiao, Siyuan and Cao, Kris and Shakeri,
                   Siamak and Butterfield, Christina and Chung, Justin and
                   Rubenstein, Paul Kishan and Agrawal, Shivani and Mensch,
                   Arthur and Soparkar, Kedar and Lenc, Karel and Chung, Timothy
                   and Pope, Aedan and Maggiore, Loren and Kay, Jackie and
                   Jhakra, Priya and Wang, Shibo and Maynez, Joshua and Phuong,
                   Mary and Tobin, Taylor and Tacchetti, Andrea and Trebacz,
                   Maja and Robinson, Kevin and Katariya, Yash and Riedel,
                   Sebastian and Bailey, Paige and Xiao, Kefan and Ghelani,
                   Nimesh and Aroyo, Lora and Slone, Ambrose and Houlsby, Neil
                   and Xiong, Xuehan and Yang, Zhen and Gribovskaya, Elena and
                   Adler, Jonas and Wirth, Mateo and Lee, Lisa and Li, Music and
                   Kagohara, Thais and Pavagadhi, Jay and Bridgers, Sophie and
                   Bortsova, Anna and Ghemawat, Sanjay and Ahmed, Zafarali and
                   Liu, Tianqi and Powell, Richard and Bolina, Vijay and Iinuma,
                   Mariko and Zablotskaia, Polina and Besley, James and Chung,
                   Da-Woon and Dozat, Timothy and Comanescu, Ramona and Si,
                   Xiance and Greer, Jeremy and Su, Guolong and Polacek, Martin
                   and Kaufman, Raphaël Lopez and Tokumine, Simon and Hu,
                   Hexiang and Buchatskaya, Elena and Miao, Yingjie and
                   Elhawaty, Mohamed and Siddhant, Aditya and Tomasev, Nenad and
                   Xing, Jinwei and Greer, Christina and Miller, Helen and
                   Ashraf, Shereen and Roy, Aurko and Zhang, Zizhao and Ma, Ada
                   and Filos, Angelos and Besta, Milos and Blevins, Rory and
                   Klimenko, Ted and Yeh, Chih-Kuan and Changpinyo, Soravit and
                   Mu, Jiaqi and Chang, Oscar and Pajarskas, Mantas and Muir,
                   Carrie and Cohen, Vered and Lan, Charline Le and Haridasan,
                   Krishna and Marathe, Amit and Hansen, Steven and Douglas,
                   Sholto and Samuel, Rajkumar and Wang, Mingqiu and Austin,
                   Sophia and Lan, Chang and Jiang, Jiepu and Chiu, Justin and
                   Lorenzo, Jaime Alonso and Sjösund, Lars Lowe and Cevey,
                   Sébastien and Gleicher, Zach and Avrahami, Thi and Boral,
                   Anudhyan and Srinivasan, Hansa and Selo, Vittorio and May,
                   Rhys and Aisopos, Konstantinos and Hussenot, Léonard and
                   Soares, Livio Baldini and Baumli, Kate and Chang, Michael B
                   and Recasens, Adrià and Caine, Ben and Pritzel, Alexander and
                   Pavetic, Filip and Pardo, Fabio and Gergely, Anita and Frye,
                   Justin and Ramasesh, Vinay and Horgan, Dan and Badola,
                   Kartikeya and Kassner, Nora and Roy, Subhrajit and Dyer,
                   Ethan and Campos, Víctor and Tomala, Alex and Tang, Yunhao
                   and Badawy, Dalia El and White, Elspeth and Mustafa, Basil
                   and Lang, Oran and Jindal, Abhishek and Vikram, Sharad and
                   Gong, Zhitao and Caelles, Sergi and Hemsley, Ross and
                   Thornton, Gregory and Feng, Fangxiaoyu and Stokowiec,
                   Wojciech and Zheng, Ce and Thacker, Phoebe and Ünlü, Çağlar
                   and Zhang, Zhishuai and Saleh, Mohammad and Svensson, James
                   and Bileschi, Max and Patil, Piyush and Anand, Ankesh and
                   Ring, Roman and Tsihlas, Katerina and Vezer, Arpi and Selvi,
                   Marco and Shevlane, Toby and Rodriguez, Mikel and
                   Kwiatkowski, Tom and Daruki, Samira and Rong, Keran and
                   Dafoe, Allan and FitzGerald, Nicholas and Gu-Lemberg, Keren
                   and Khan, Mina and Hendricks, Lisa Anne and Pellat, Marie and
                   Feinberg, Vladimir and Cobon-Kerr, James and Sainath, Tara
                   and Rauh, Maribeth and Hashemi, Sayed Hadi and Ives, Richard
                   and Hasson, Yana and Li, Yaguang and Noland, Eric and Cao,
                   Yuan and Byrd, Nathan and Hou, Le and Wang, Qingze and
                   Sottiaux, Thibault and Paganini, Michela and Lespiau,
                   Jean-Baptiste and Moufarek, Alexandre and Hassan, Samer and
                   Shivakumar, Kaushik and van Amersfoort, Joost and Mandhane,
                   Amol and Joshi, Pratik and Goyal, Anirudh and Tung, Matthew
                   and Brock, Andrew and Sheahan, Hannah and Misra, Vedant and
                   Li, Cheng and Rakićević, Nemanja and Dehghani, Mostafa and
                   Liu, Fangyu and Mittal, Sid and Oh, Junhyuk and Noury, Seb
                   and Sezener, Eren and Huot, Fantine and Lamm, Matthew and De
                   Cao, Nicola and Chen, Charlie and Elsayed, Gamaleldin and
                   Chi, Ed and Mahdieh, Mahdis and Tenney, Ian and Hua, Nan and
                   Petrychenko, Ivan and Kane, Patrick and Scandinaro, Dylan and
                   Jain, Rishub and Uesato, Jonathan and Datta, Romina and
                   Sadovsky, Adam and Bunyan, Oskar and Rabiej, Dominik and Wu,
                   Shimu and Zhang, John and Vasudevan, Gautam and Leurent,
                   Edouard and Alnahlawi, Mahmoud and Georgescu, Ionut and Wei,
                   Nan and Zheng, Ivy and Chan, Betty and Rabinovitch, Pam G and
                   Stanczyk, Piotr and Zhang, Ye and Steiner, David and Naskar,
                   Subhajit and Azzam, Michael and Johnson, Matthew and Paszke,
                   Adam and Chiu, Chung-Cheng and Elias, Jaume Sanchez and
                   Mohiuddin, Afroz and Muhammad, Faizan and Miao, Jin and Lee,
                   Andrew and Vieillard, Nino and Potluri, Sahitya and Park,
                   Jane and Davoodi, Elnaz and Zhang, Jiageng and Stanway, Jeff
                   and Garmon, Drew and Karmarkar, Abhijit and Dong, Zhe and
                   Lee, Jong and Kumar, Aviral and Zhou, Luowei and Evens,
                   Jonathan and Isaac, William and Chen, Zhe and Jia, Johnson
                   and Levskaya, Anselm and Zhu, Zhenkai and Gorgolewski, Chris
                   and Grabowski, Peter and Mao, Yu and Magni, Alberto and Yao,
                   Kaisheng and Snaider, Javier and Casagrande, Norman and
                   Suganthan, Paul and Palmer, Evan and Irving, Geoffrey and
                   Loper, Edward and Faruqui, Manaal and Arkatkar, Isha and
                   Chen, Nanxin and Shafran, Izhak and Fink, Michael and
                   Castaño, Alfonso and Giannoumis, Irene and Kim, Wooyeol and
                   Rybiński, Mikołaj and Sreevatsa, Ashwin and Prendki, Jennifer
                   and Soergel, David and Goedeckemeyer, Adrian and Gierke,
                   Willi and Jafari, Mohsen and Gaba, Meenu and Wiesner, Jeremy
                   and Wright, Diana Gage and Wei, Yawen and Vashisht, Harsha
                   and Kulizhskaya, Yana and Hoover, Jay and Le, Maigo and Li,
                   Lu and Iwuanyanwu, Chimezie and Liu, Lu and Ramirez, Kevin
                   and Khorlin, Andrey and Cui, Albert and Lin, Tian and
                   Georgiev, Marin and Wu, Marcus and Aguilar, Ricardo and
                   Pallo, Keith and Chakladar, Abhishek and Repina, Alena and
                   Wu, Xihui and van der Weide, Tom and Ponnapalli, Priya and
                   Kaplan, Caroline and Simsa, Jiri and Li, Shuangfeng and
                   Dousse, Olivier and Yang, Fan and Piper, Jeff and Ie, Nathan
                   and Lui, Minnie and Pasumarthi, Rama and Lintz, Nathan and
                   Vijayakumar, Anitha and Thiet, Lam Nguyen and Andor, Daniel
                   and Valenzuela, Pedro and Paduraru, Cosmin and Peng, Daiyi
                   and Lee, Katherine and Zhang, Shuyuan and Greene, Somer and
                   Nguyen, Duc Dung and Kurylowicz, Paula and Velury, Sarmishta
                   and Krause, Sebastian and Hardin, Cassidy and Dixon, Lucas
                   and Janzer, Lili and Choo, Kiam and Feng, Ziqiang and Zhang,
                   Biao and Singhal, Achintya and Latkar, Tejasi and Zhang,
                   Mingyang and Le, Quoc and Abellan, Elena Allica and Du, Dayou
                   and McKinnon, Dan and Antropova, Natasha and Bolukbasi, Tolga
                   and Keller, Orgad and Reid, David and Finchelstein, Daniel
                   and Raad, Maria Abi and Crocker, Remi and Hawkins, Peter and
                   Dadashi, Robert and Gaffney, Colin and Lall, Sid and Franko,
                   Ken and Filonov, Egor and Bulanova, Anna and Leblond, Rémi
                   and Yadav, Vikas and Chung, Shirley and Askham, Harry and
                   Cobo, Luis C and Xu, Kelvin and Fischer, Felix and Xu, Jun
                   and Sorokin, Christina and Alberti, Chris and Lin, Chu-Cheng
                   and Evans, Colin and Zhou, Hao and Dimitriev, Alek and
                   Forbes, Hannah and Banarse, Dylan and Tung, Zora and Liu,
                   Jeremiah and Omernick, Mark and Bishop, Colton and Kumar,
                   Chintu and Sterneck, Rachel and Foley, Ryan and Jain, Rohan
                   and Mishra, Swaroop and Xia, Jiawei and Bos, Taylor and
                   Cideron, Geoffrey and Amid, Ehsan and Piccinno, Francesco and
                   Wang, Xingyu and Banzal, Praseem and Gurita, Petru and Noga,
                   Hila and Shah, Premal and Mankowitz, Daniel J and Polozov,
                   Alex and Kushman, Nate and Krakovna, Victoria and Brown,
                   Sasha and Bateni, Mohammadhossein and Duan, Dennis and
                   Firoiu, Vlad and Thotakuri, Meghana and Natan, Tom and
                   Mohananey, Anhad and Geist, Matthieu and Mudgal, Sidharth and
                   Girgin, Sertan and Li, Hui and Ye, Jiayu and Roval, Ofir and
                   Tojo, Reiko and Kwong, Michael and Lee-Thorp, James and Yew,
                   Christopher and Yuan, Quan and Bagri, Sumit and Sinopalnikov,
                   Danila and Ramos, Sabela and Mellor, John and Sharma,
                   Abhishek and Severyn, Aliaksei and Lai, Jonathan and Wu,
                   Kathy and Cheng, Heng-Tze and Miller, David and Sonnerat,
                   Nicolas and Vnukov, Denis and Greig, Rory and Beattie,
                   Jennifer and Caveness, Emily and Bai, Libin and Eisenschlos,
                   Julian and Korchemniy, Alex and Tsai, Tomy and Jasarevic,
                   Mimi and Kong, Weize and Dao, Phuong and Zheng, Zeyu and Liu,
                   Frederick and Yang, Fan and Zhu, Rui and Geller, Mark and
                   Teh, Tian Huey and Sanmiya, Jason and Gladchenko, Evgeny and
                   Trdin, Nejc and Sozanschi, Andrei and Toyama, Daniel and
                   Rosen, Evan and Tavakkol, Sasan and Xue, Linting and Elkind,
                   Chen and Woodman, Oliver and Carpenter, John and
                   Papamakarios, George and Kemp, Rupert and Kafle, Sushant and
                   Grunina, Tanya and Sinha, Rishika and Talbert, Alice and
                   Goyal, Abhimanyu and Wu, Diane and Owusu-Afriyie, Denese and
                   Du, Cosmo and Thornton, Chloe and Pont-Tuset, Jordi and
                   Narayana, Pradyumna and Li, Jing and Fatehi, Sabaer and
                   Wieting, John and Ajmeri, Omar and Uria, Benigno and Zhu, Tao
                   and Ko, Yeongil and Knight, Laura and Héliou, Amélie and Niu,
                   Ning and Gu, Shane and Pang, Chenxi and Tran, Dustin and Li,
                   Yeqing and Levine, Nir and Stolovich, Ariel and Kalb, Norbert
                   and Santamaria-Fernandez, Rebeca and Goenka, Sonam and
                   Yustalim, Wenny and Strudel, Robin and Elqursh, Ali and
                   Lakshminarayanan, Balaji and Deck, Charlie and Upadhyay,
                   Shyam and Lee, Hyo and Dusenberry, Mike and Li, Zonglin and
                   Wang, Xuezhi and Levin, Kyle and Hoffmann, Raphael and
                   Holtmann-Rice, Dan and Bachem, Olivier and Yue, Summer and
                   Arora, Sho and Malmi, Eric and Mirylenka, Daniil and Tan,
                   Qijun and Koh, Christy and Yeganeh, Soheil Hassas and Põder,
                   Siim and Zheng, Steven and Pongetti, Francesco and Tariq,
                   Mukarram and Sun, Yanhua and Ionita, Lucian and
                   Seyedhosseini, Mojtaba and Tafti, Pouya and Kotikalapudi,
                   Ragha and Liu, Zhiyu and Gulati, Anmol and Liu, Jasmine and
                   Ye, Xinyu and Chrzaszcz, Bart and Wang, Lily and Sethi,
                   Nikhil and Li, Tianrun and Brown, Ben and Singh, Shreya and
                   Fan, Wei and Parisi, Aaron and Stanton, Joe and Kuang,
                   Chenkai and Koverkathu, Vinod and Choquette-Choo, Christopher
                   A and Li, Yunjie and Lu, T J and Ittycheriah, Abe and Shroff,
                   Prakash and Sun, Pei and Varadarajan, Mani and Bahargam,
                   Sanaz and Willoughby, Rob and Gaddy, David and Dasgupta,
                   Ishita and Desjardins, Guillaume and Cornero, Marco and
                   Robenek, Brona and Mittal, Bhavishya and Albrecht, Ben and
                   Shenoy, Ashish and Moiseev, Fedor and Jacobsson, Henrik and
                   Ghaffarkhah, Alireza and Rivière, Morgane and Walton, Alanna
                   and Crepy, Clément and Parrish, Alicia and Liu, Yuan and
                   Zhou, Zongwei and Farabet, Clement and Radebaugh, Carey and
                   Srinivasan, Praveen and van der Salm, Claudia and Fidjeland,
                   Andreas and Scellato, Salvatore and Latorre-Chimoto, Eri and
                   Klimczak-Plucińska, Hanna and Bridson, David and de Cesare,
                   Dario and Hudson, Tom and Mendolicchio, Piermaria and Walker,
                   Lexi and Morris, Alex and Penchev, Ivo and Mauger, Matthew
                   and Guseynov, Alexey and Reid, Alison and Odoom, Seth and
                   Loher, Lucia and Cotruta, Victor and Yenugula, Madhavi and
                   Grewe, Dominik and Petrushkina, Anastasia and Duerig, Tom and
                   Sanchez, Antonio and Yadlowsky, Steve and Shen, Amy and
                   Globerson, Amir and Kurzrok, Adam and Webb, Lynette and Dua,
                   Sahil and Li, Dong and Lahoti, Preethi and Bhupatiraju, Surya
                   and Hurt, Dan and Qureshi, Haroon and Agarwal, Ananth and
                   Shani, Tomer and Eyal, Matan and Khare, Anuj and Belle,
                   Shreyas Rammohan and Wang, Lei and Tekur, Chetan and Kale,
                   Mihir Sanjay and Wei, Jinliang and Sang, Ruoxin and Saeta,
                   Brennan and Liechty, Tyler and Sun, Yi and Zhao, Yao and Lee,
                   Stephan and Nayak, Pandu and Fritz, Doug and Vuyyuru, Manish
                   Reddy and Aslanides, John and Vyas, Nidhi and Wicke, Martin
                   and Ma, Xiao and Bilal, Taylan and Eltyshev, Evgenii and
                   Balle, Daniel and Martin, Nina and Cate, Hardie and Manyika,
                   James and Amiri, Keyvan and Kim, Yelin and Xiong, Xi and
                   Kang, Kai and Luisier, Florian and Tripuraneni, Nilesh and
                   Madras, David and Guo, Mandy and Waters, Austin and Wang,
                   Oliver and Ainslie, Joshua and Baldridge, Jason and Zhang,
                   Han and Pruthi, Garima and Bauer, Jakob and Yang, Feng and
                   Mansour, Riham and Gelman, Jason and Xu, Yang and Polovets,
                   George and Liu, Ji and Cai, Honglong and Chen, Warren and
                   Sheng, Xianghai and Xue, Emily and Ozair, Sherjil and Yu,
                   Adams and Angermueller, Christof and Li, Xiaowei and Wang,
                   Weiren and Wiesinger, Julia and Koukoumidis, Emmanouil and
                   Tian, Yuan and Iyer, Anand and Gurumurthy, Madhu and
                   Goldenson, Mark and Shah, Parashar and Blake, M K and Yu,
                   Hongkun and Urbanowicz, Anthony and Palomaki, Jennimaria and
                   Fernando, Chrisantha and Brooks, Kevin and Durden, Ken and
                   Mehta, Harsh and Momchev, Nikola and Rahimtoroghi, Elahe and
                   Georgaki, Maria and Raul, Amit and Ruder, Sebastian and
                   Redshaw, Morgan and Lee, Jinhyuk and Jalan, Komal and Li,
                   Dinghua and Perng, Ginger and Hechtman, Blake and Schuh,
                   Parker and Nasr, Milad and Chen, Mia and Milan, Kieran and
                   Mikulik, Vladimir and Strohman, Trevor and Franco, Juliana
                   and Green, Tim and Hassabis, Demis and Kavukcuoglu, Koray and
                   Dean, Jeffrey and Vinyals, Oriol",
  journal       = "arXiv [cs.CL]",
  abstract      = "This report introduces a new family of multimodal models,
                   Gemini, that exhibit remarkable capabilities across image,
                   audio, video, and text understanding. The Gemini family
                   consists of Ultra, Pro, and Nano sizes, suitable for
                   applications ranging from complex reasoning tasks to
                   on-device memory-constrained use-cases. Evaluation on a broad
                   range of benchmarks shows that our most-capable Gemini Ultra
                   model advances the state of the art in 30 of 32 of these
                   benchmarks - notably being the first model to achieve
                   human-expert performance on the well-studied exam benchmark
                   MMLU, and improving the state of the art in every one of the
                   20 multimodal benchmarks we examined. We believe that the new
                   capabilities of Gemini models in cross-modal reasoning and
                   language understanding will enable a wide variety of use
                   cases and we discuss our approach toward deploying them
                   responsibly to users.",
  month         =  dec,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2312.11805"
}

@ARTICLE{Bragg2021-vt,
  title     = "Flex: Unifying evaluation for few-shot nlp",
  author    = "Bragg, J and Cohan, A and Lo, K and Beltagy, I",
  journal   = "Thirty-Fifth Conference on Neural",
  publisher = "papers.nips.cc",
  abstract  = "Few-shot NLP research is highly active, yet conducted in disjoint
               research threads with evaluation suites that lack
               challenging-yet-realistic testing setups and fail to employ
               careful experimental design. Consequently, the community does not
               know which techniques perform best or even if they outperform
               simple baselines. In response, we formulate the FLEX Principles,
               a set of requirements and best practices for unified, rigorous,
               valid, and cost-sensitive few-shot NLP evaluation. These
               principles include Sample Size Design, a …",
  year      =  2021
}

@ARTICLE{Piktus2021-ew,
  title         = "The Web Is Your Oyster -- Knowledge-Intensive {NLP} against a
                   Very Large Web Corpus",
  author        = "Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir
                   and Okhonko, Dmytro and Broscheit, Samuel and Izacard,
                   Gautier and Lewis, Patrick and Oğuz, Barlas and Grave,
                   Edouard and Yih, Wen-Tau and Riedel, Sebastian",
  journal       = "arXiv [cs.CL]",
  abstract      = "In order to address the increasing demands of real-world
                   applications, the research for knowledge-intensive NLP
                   (KI-NLP) should advance by capturing the challenges of a
                   truly open-domain environment: web scale knowledge, lack of
                   structure, inconsistent quality, and noise. To this end, we
                   propose a new setup for evaluating existing KI-NLP tasks in
                   which we generalize the background corpus to a universal web
                   snapshot. We repurpose KILT, a standard KI-NLP benchmark
                   initially developed for Wikipedia, and ask systems to use a
                   subset of CCNet - the Sphere corpus - as a knowledge source.
                   In contrast to Wikipedia, Sphere is orders of magnitude
                   larger and better reflects the full diversity of knowledge on
                   the Internet. We find that despite potential gaps of
                   coverage, challenges of scale, lack of structure and lower
                   quality, retrieval from Sphere enables a state-of-the-art
                   retrieve-and-read system to match and even outperform
                   Wikipedia-based models on several KILT tasks - even if we
                   aggressively filter content that looks like Wikipedia. We
                   also observe that while a single dense passage index over
                   Wikipedia can outperform a sparse BM25 version, on Sphere
                   this is not yet possible. To facilitate further research into
                   this area, and minimise the community's reliance on
                   proprietary black box search engines, we will share our
                   indices, evaluation metrics and infrastructure.",
  month         =  dec,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2112.09924"
}

@ARTICLE{Mielke2021-et,
  title         = "Between words and characters: A Brief History of
                   Open-Vocabulary Modeling and Tokenization in {NLP}",
  author        = "Mielke, Sabrina J and Alyafeai, Zaid and Salesky, Elizabeth
                   and Raffel, Colin and Dey, Manan and Gallé, Matthias and
                   Raja, Arun and Si, Chenglei and Lee, Wilson Y and Sagot,
                   Benoît and Tan, Samson",
  journal       = "arXiv [cs.CL]",
  abstract      = "What are the units of text that we want to model? From bytes
                   to multi-word expressions, text can be analyzed and generated
                   at many granularities. Until recently, most natural language
                   processing (NLP) models operated over words, treating those
                   as discrete and atomic tokens, but starting with byte-pair
                   encoding (BPE), subword-based approaches have become dominant
                   in many areas, enabling small vocabularies while still
                   allowing for fast inference. Is the end of the road
                   character-level model or byte-level processing? In this
                   survey, we connect several lines of work from the pre-neural
                   and neural era, by showing how hybrid approaches of words and
                   characters as well as subword-based approaches based on
                   learned segmentation have been proposed and evaluated. We
                   conclude that there is and likely will never be a silver
                   bullet singular solution for all applications and that
                   thinking seriously about tokenization remains important for
                   many applications.",
  month         =  dec,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2112.10508"
}

@ARTICLE{Lin2021-ht,
  title         = "Few-shot Learning with Multilingual Language Models",
  author        = "Lin, Xi Victoria and Mihaylov, Todor and Artetxe, Mikel and
                   Wang, Tianlu and Chen, Shuohui and Simig, Daniel and Ott,
                   Myle and Goyal, Naman and Bhosale, Shruti and Du, Jingfei and
                   Pasunuru, Ramakanth and Shleifer, Sam and Koura, Punit Singh
                   and Chaudhary, Vishrav and O'Horo, Brian and Wang, Jeff and
                   Zettlemoyer, Luke and Kozareva, Zornitsa and Diab, Mona and
                   Stoyanov, Veselin and Li, Xian",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large-scale autoregressive language models such as GPT-3 are
                   few-shot learners that can perform a wide range of language
                   tasks without fine-tuning. While these models are known to be
                   able to jointly represent many different languages, their
                   training data is dominated by English, potentially limiting
                   their cross-lingual generalization. In this work, we train
                   multilingual autoregressive language models on a balanced
                   corpus covering a diverse set of languages, and study their
                   few- and zero-shot learning capabilities in a wide range of
                   tasks. Our largest model with 7.5 billion parameters sets new
                   state of the art in few-shot learning in more than 20
                   representative languages, outperforming GPT-3 of comparable
                   size in multilingual commonsense reasoning (with +7.4\%
                   absolute accuracy improvement in 0-shot settings and +9.4\%
                   in 4-shot settings) and natural language inference (+5.4\% in
                   each of 0-shot and 4-shot settings). On the FLORES-101
                   machine translation benchmark, our model outperforms GPT-3 on
                   171 out of 182 translation directions with 32 training
                   examples, while surpassing the official supervised baseline
                   in 45 directions. We present a detailed analysis of where the
                   model succeeds and fails, showing in particular that it
                   enables cross-lingual in-context learning on some tasks,
                   while there is still room for improvement on surface form
                   robustness and adaptation to tasks that do not have a natural
                   cloze form. Finally, we evaluate our models in social value
                   tasks such as hate speech detection in five languages and
                   find it has limitations similar to comparable sized GPT-3
                   models.",
  month         =  dec,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2112.10668"
}

@ARTICLE{Choi2021-ww,
  title     = "Decontextualization: Making sentences stand-alone",
  author    = "Choi, Eunsol and Palomaki, Jennimaria and Lamm, Matthew and
               Kwiatkowski, Tom and Das, Dipanjan and Collins, Michael",
  journal   = "Transactions of the Association for Computational Linguistics",
  publisher = "MIT Press - Journals",
  volume    =  9,
  pages     = "447--461",
  abstract  = "Abstract Models for question answering, dialogue agents, and
               summarization often interpret the meaning of a sentence in a rich
               context and use that meaning in a new context. Taking excerpts of
               text can be problematic, as key pieces may not be explicit in a
               local window. We isolate and define the problem of sentence
               decontextualization: taking a sentence together with its context
               and rewriting it to be interpretable out of context, while
               preserving its meaning. We describe an annotation procedure,
               collect data on the Wikipedia corpus, and use the data to train
               models to automatically decontextualize sentences. We present
               preliminary studies that show the value of sentence
               decontextualization in a user-facing task, and as preprocessing
               for systems that perform document understanding. We argue that
               decontextualization is an important subtask in many downstream
               applications, and that the definitions and resources provided can
               benefit tasks that operate on sentences that occur in a richer
               context.",
  month     =  apr,
  year      =  2021,
  doi       = "10.1162/tacl\_a\_00377",
  issn      = "2307-387X",
  language  = "en"
}

@ARTICLE{Dasigi2017-al,
  title         = "Experiment Segmentation in Scientific Discourse as
                   Clause-level Structured Prediction using Recurrent Neural
                   Networks",
  author        = "Dasigi, Pradeep and Burns, Gully A P and Hovy, Eduard and de
                   Waard, Anita",
  journal       = "arXiv [cs.CL]",
  abstract      = "We propose a deep learning model for identifying structure
                   within experiment narratives in scientific literature. We
                   take a sequence labeling approach to this problem, and label
                   clauses within experiment narratives to identify the
                   different parts of the experiment. Our dataset consists of
                   paragraphs taken from open access PubMed papers labeled with
                   rhetorical information as a result of our pilot annotation.
                   Our model is a Recurrent Neural Network (RNN) with Long
                   Short-Term Memory (LSTM) cells that labels clauses. The
                   clause representations are computed by combining word
                   representations using a novel attention mechanism that
                   involves a separate RNN. We compare this model against LSTMs
                   where the input layer has simple or no attention and a
                   feature rich CRF model. Furthermore, we describe how our work
                   could be useful for information extraction from scientific
                   literature.",
  month         =  feb,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1702.05398"
}

@ARTICLE{Brack2021-ca,
  title    = "Sequential Sentence Classification in Research Papers using
              Cross-Domain Multi-Task Learning",
  author   = "Brack, Arthur and Hoppe, Anett and Buschermöhle, Pascal and
              Ewerth, R",
  journal  = "undefined",
  abstract = "It is demonstrated that models, which are trained on datasets from
              different scientific domains, benefit from one another when using
              the proposed multi-task learning architecture, and the approach
              outperforms the state of the art on three benchmark datasets. The
              task of sequential sentence classification enables the semantic
              structuring of research papers. This can enhance academic search
              engines to support researchers in finding and exploring research
              literature more effectively. However, previous work has not
              investigated the potential of transfer learning with datasets from
              different scientific domains for this task yet. We propose a
              uniform deep learning architecture and multi-task learning to
              improve sequential sentence classification in scientific texts
              across domains by exploiting training data from multiple domains.
              Our contributions can be summarised as follows: (1) We tailor two
              common transfer learning methods, sequential transfer learning and
              multi-task learning, and evaluate their performance for sequential
              sentence classification; (2) The presented multi-task model is
              able to recognise semantically related classes from different
              datasets and thus supports manual comparison and assessment of
              different annotation schemes; (3) The unified approach is capable
              of handling datasets that contain either only abstracts or full
              papers without further feature engineering. We demonstrate that
              models, which are trained on datasets from different scientific
              domains, benefit from one another when using the proposed
              multi-task learning architecture. Our approach outperforms the
              state of the art on three benchmark datasets. Arthur Brack E-mail:
              arthur.brack@tib.eu Anett Hoppe E-mail: anett.hoppe@tib.eu Pascal
              Buschermöhle E-mail: pascal.buschermoehle@gmail.com Ralph Ewerth
              E-mail: ralph.ewerth@tib.eu 1TIB – Leibniz Information Centre for
              Science and Technology, Hannover, Germany 2L3S Research Center,
              Leibniz University, Hannover, Germany ar X iv :2 10 2. 06 00 8v 1
              [ cs .C L ] 1 1 Fe b 20 21",
  year     =  2021,
  eprint   = "2102.06008",
  language = "en"
}

@ARTICLE{Dernoncourt2017-sw,
  title         = "{PubMed} {200k} {RCT}: a Dataset for Sequential Sentence
                   Classification in Medical Abstracts",
  author        = "Dernoncourt, Franck and Lee, Ji Young",
  journal       = "arXiv [cs.CL]",
  abstract      = "We present PubMed 200k RCT, a new dataset based on PubMed for
                   sequential sentence classification. The dataset consists of
                   approximately 200,000 abstracts of randomized controlled
                   trials, totaling 2.3 million sentences. Each sentence of each
                   abstract is labeled with their role in the abstract using one
                   of the following classes: background, objective, method,
                   result, or conclusion. The purpose of releasing this dataset
                   is twofold. First, the majority of datasets for sequential
                   short-text classification (i.e., classification of short
                   texts that appear in sequences) are small: we hope that
                   releasing a new large dataset will help develop more accurate
                   algorithms for this task. Second, from an application
                   perspective, researchers need better tools to efficiently
                   skim through the literature. Automatically classifying each
                   sentence in an abstract would help researchers read abstracts
                   more efficiently, especially in fields where abstracts may be
                   long, such as the medical field.",
  month         =  oct,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1710.06071"
}

@ARTICLE{Sciavolino2021-xf,
  title         = "Simple Entity-Centric Questions Challenge Dense Retrievers",
  author        = "Sciavolino, Christopher and Zhong, Zexuan and Lee, Jinhyuk
                   and Chen, Danqi",
  journal       = "arXiv [cs.CL]",
  abstract      = "Open-domain question answering has exploded in popularity
                   recently due to the success of dense retrieval models, which
                   have surpassed sparse models using only a few supervised
                   training examples. However, in this paper, we demonstrate
                   current dense models are not yet the holy grail of retrieval.
                   We first construct EntityQuestions, a set of simple,
                   entity-rich questions based on facts from Wikidata (e.g.,
                   ``Where was Arve Furset born?''), and observe that dense
                   retrievers drastically underperform sparse methods. We
                   investigate this issue and uncover that dense retrievers can
                   only generalize to common entities unless the question
                   pattern is explicitly observed during training. We discuss
                   two simple solutions towards addressing this critical
                   problem. First, we demonstrate that data augmentation is
                   unable to fix the generalization problem. Second, we argue a
                   more robust passage encoder helps facilitate better question
                   adaptation using specialized question encoders. We hope our
                   work can shed light on the challenges in creating a robust,
                   universal dense retriever that works well across different
                   input distributions.",
  month         =  sep,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2109.08535"
}

@INPROCEEDINGS{Kanakarajan2021-hf,
  title     = "{BioELECTRA}:Pretrained Biomedical text Encoder using
               Discriminators",
  author    = "Kanakarajan, Kamal Raj and Kundumani, Bhuvana and Sankarasubbu,
               Malaikannan",
  booktitle = "Proceedings of the 20th Workshop on Biomedical Language
               Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "143--154",
  abstract  = "Kamal raj Kanakarajan, Bhuvana Kundumani, Malaikannan
               Sankarasubbu. Proceedings of the 20th Workshop on Biomedical
               Language Processing. 2021.",
  year      =  2021,
  doi       = "10.18653/v1/2021.bionlp-1.16"
}

@INPROCEEDINGS{Mahajan2021-ia,
  title     = "{IBMResearch} at {MEDIQA} 2021: Toward improving factual
               correctness of radiology report abstractive summarization",
  author    = "Mahajan, Diwakar and Tsou, Ching-Huei and Liang, Jennifer J",
  booktitle = "Proceedings of the 20th Workshop on Biomedical Language
               Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "302--310",
  abstract  = "Diwakar Mahajan, Ching-Huei Tsou, Jennifer J Liang. Proceedings
               of the 20th Workshop on Biomedical Language Processing. 2021.",
  year      =  2021,
  doi       = "10.18653/v1/2021.bionlp-1.35"
}

@ARTICLE{Li2022-un,
  title         = "{CORWA}: A Citation-Oriented Related Work Annotation dataset",
  author        = "Li, Xiangci and Mandal, Biswadip and Ouyang, Jessica",
  journal       = "arXiv [cs.CL]",
  abstract      = "Academic research is an exploratory activity to discover new
                   solutions to problems. By this nature, academic research
                   works perform literature reviews to distinguish their
                   novelties from prior work. In natural language processing,
                   this literature review is usually conducted under the
                   ``Related Work'' section. The task of related work generation
                   aims to automatically generate the related work section given
                   the rest of the research paper and a list of papers to cite.
                   Prior work on this task has focused on the sentence as the
                   basic unit of generation, neglecting the fact that related
                   work sections consist of variable length text fragments
                   derived from different information sources. As a first step
                   toward a linguistically-motivated related work generation
                   framework, we present a Citation Oriented Related Work
                   Annotation (CORWA) dataset that labels different types of
                   citation text fragments from different information sources.
                   We train a strong baseline model that automatically tags the
                   CORWA labels on massive unlabeled related work section texts.
                   We further suggest a novel framework for human-in-the-loop,
                   iterative, abstractive related work generation.",
  month         =  may,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2205.03512"
}

@ARTICLE{Lewis2020-jy,
  title         = "Question and answer test-train overlap in open-Domain
                   Question Answering datasets",
  author        = "Lewis, Patrick and Stenetorp, Pontus and Riedel, Sebastian",
  journal       = "arXiv [cs.CL]",
  abstract      = "Ideally Open-Domain Question Answering models should exhibit
                   a number of competencies, ranging from simply memorizing
                   questions seen at training time, to answering novel question
                   formulations with answers seen during training, to
                   generalizing to completely novel questions with novel
                   answers. However, single aggregated test set scores do not
                   show the full picture of what capabilities models truly have.
                   In this work, we perform a detailed study of the test sets of
                   three popular open-domain benchmark datasets with respect to
                   these competencies. We find that 60-70\% of test-time answers
                   are also present somewhere in the training sets. We also find
                   that 30\% of test-set questions have a near-duplicate
                   paraphrase in their corresponding training sets. Using these
                   findings, we evaluate a variety of popular open-domain models
                   to obtain greater insight into what extent they can actually
                   generalize, and what drives their overall performance. We
                   find that all models perform dramatically worse on questions
                   that cannot be memorized from training sets, with a mean
                   absolute performance difference of 63\% between repeated and
                   non-repeated data. Finally we show that simple
                   nearest-neighbor models out-perform a BART closed-book QA
                   model, further highlighting the role that training set
                   memorization plays in these benchmarks",
  month         =  aug,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2008.02637"
}

@ARTICLE{Wahle2022-mi,
  title         = "{D3}: A massive dataset of scholarly metadata for analyzing
                   the state of computer science research",
  author        = "Wahle, Jan Philip and Ruas, Terry and Mohammad, Saif M and
                   Gipp, Bela",
  journal       = "arXiv [cs.DL]",
  abstract      = "DBLP is the largest open-access repository of scientific
                   articles on computer science and provides metadata associated
                   with publications, authors, and venues. We retrieved more
                   than 6 million publications from DBLP and extracted pertinent
                   metadata (e.g., abstracts, author affiliations, citations)
                   from the publication texts to create the DBLP Discovery
                   Dataset (D3). D3 can be used to identify trends in research
                   activity, productivity, focus, bias, accessibility, and
                   impact of computer science research. We present an initial
                   analysis focused on the volume of computer science research
                   (e.g., number of papers, authors, research activity), trends
                   in topics of interest, and citation patterns. Our findings
                   show that computer science is a growing research field
                   (approx. 15\% annually), with an active and collaborative
                   researcher community. While papers in recent years present
                   more bibliographical entries in comparison to previous
                   decades, the average number of citations has been declining.
                   Investigating papers' abstracts reveals that recent topic
                   trends are clearly reflected in D3. Finally, we list further
                   applications of D3 and pose supplemental research questions.
                   The D3 dataset, our findings, and source code are publicly
                   available for research purposes.",
  month         =  apr,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.DL",
  eprint        = "2204.13384"
}

@ARTICLE{Wang2022-gh,
  title         = "{KECP}: Knowledge enhanced contrastive prompting for few-shot
                   extractive Question Answering",
  author        = "Wang, Jianing and Wang, Chengyu and Qiu, Minghui and Shi,
                   Qiuhui and Wang, Hongbin and Huang, Jun and Gao, Ming",
  journal       = "arXiv [cs.CL]",
  abstract      = "Extractive Question Answering (EQA) is one of the most
                   important tasks in Machine Reading Comprehension (MRC), which
                   can be solved by fine-tuning the span selecting heads of
                   Pre-trained Language Models (PLMs). However, most existing
                   approaches for MRC may perform poorly in the few-shot
                   learning scenario. To solve this issue, we propose a novel
                   framework named Knowledge Enhanced Contrastive Prompt-tuning
                   (KECP). Instead of adding pointer heads to PLMs, we introduce
                   a seminal paradigm for EQA that transform the task into a
                   non-autoregressive Masked Language Modeling (MLM) generation
                   problem. Simultaneously, rich semantics from the external
                   knowledge base (KB) and the passage context are support for
                   enhancing the representations of the query. In addition, to
                   boost the performance of PLMs, we jointly train the model by
                   the MLM and contrastive learning objectives. Experiments on
                   multiple benchmarks demonstrate that our method consistently
                   outperforms state-of-the-art approaches in few-shot settings
                   by a large margin.",
  month         =  may,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2205.03071"
}

@ARTICLE{Rubin2021-zr,
  title         = "Learning to retrieve prompts for in-context learning",
  author        = "Rubin, Ohad and Herzig, Jonathan and Berant, Jonathan",
  journal       = "arXiv [cs.CL]",
  abstract      = "In-context learning is a recent paradigm in natural language
                   understanding, where a large pre-trained language model (LM)
                   observes a test instance and a few training examples as its
                   input, and directly decodes the output without any update to
                   its parameters. However, performance has been shown to
                   strongly depend on the selected training examples (termed
                   prompt). In this work, we propose an efficient method for
                   retrieving prompts for in-context learning using annotated
                   data and a LM. Given an input-output pair, we estimate the
                   probability of the output given the input and a candidate
                   training example as the prompt, and label training examples
                   as positive or negative based on this probability. We then
                   train an efficient dense retriever from this data, which is
                   used to retrieve training examples as prompts at test time.
                   We evaluate our approach on three sequence-to-sequence tasks
                   where language utterances are mapped to meaning
                   representations, and find that it substantially outperforms
                   prior work and multiple baselines across the board.",
  month         =  dec,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2112.08633"
}

@ARTICLE{An2021-dg,
  title         = "{RetrievalSum}: A Retrieval Enhanced Framework for
                   Abstractive Summarization",
  author        = "An, Chenxin and Zhong, Ming and Geng, Zhichao and Yang,
                   Jianqiang and Qiu, Xipeng",
  journal       = "arXiv [cs.CL]",
  abstract      = "Existing summarization systems mostly generate summaries
                   purely relying on the content of the source document.
                   However, even for humans, we usually need some references or
                   exemplars to help us fully understand the source document and
                   write summaries in a particular format. But how to find the
                   high-quality exemplars and incorporate them into
                   summarization systems is still challenging and worth
                   exploring. In this paper, we propose RetrievalSum, a novel
                   retrieval enhanced abstractive summarization framework
                   consisting of a dense Retriever and a Summarizer. At first,
                   several closely related exemplars are retrieved as
                   supplementary input to help the generation model understand
                   the text more comprehensively. Furthermore, retrieved
                   exemplars can also play a role in guiding the model to
                   capture the writing style of a specific corpus. We validate
                   our method on a wide range of summarization datasets across
                   multiple domains and two backbone models: BERT and BART.
                   Results show that our framework obtains significant
                   improvement by 1.38~4.66 in ROUGE-1 score when compared with
                   the powerful pre-trained models, and achieve new
                   state-of-the-art on BillSum. Human evaluation demonstrates
                   that our retrieval enhanced model can better capture the
                   domain-specific writing style.",
  month         =  sep,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2109.07943"
}

@ARTICLE{Kulkarni2020-nh,
  title         = "{AQuaMuSe}: Automatically Generating Datasets for Query-Based
                   Multi-Document Summarization",
  author        = "Kulkarni, Sayali and Chammas, Sheide and Zhu, Wan and Sha,
                   Fei and Ie, Eugene",
  journal       = "arXiv [cs.CL]",
  abstract      = "Summarization is the task of compressing source document(s)
                   into coherent and succinct passages. This is a valuable tool
                   to present users with concise and accurate sketch of the top
                   ranked documents related to their queries. Query-based
                   multi-document summarization (qMDS) addresses this pervasive
                   need, but the research is severely limited due to lack of
                   training and evaluation datasets as existing single-document
                   and multi-document summarization datasets are inadequate in
                   form and scale. We propose a scalable approach called
                   AQuaMuSe to automatically mine qMDS examples from question
                   answering datasets and large document corpora. Our approach
                   is unique in the sense that it can general a dual dataset --
                   for extractive and abstractive summaries both. We publicly
                   release a specific instance of an AQuaMuSe dataset with 5,519
                   query-based summaries, each associated with an average of 6
                   input documents selected from an index of 355M documents from
                   Common Crawl. Extensive evaluation of the dataset along with
                   baseline summarization model experiments are provided.",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2010.12694"
}

@ARTICLE{Wang2022-up,
  title         = "Training {datA} is more valuable than you think: A simple and
                   effective method by retrieving from {traINing} {datA}",
  author        = "Wang, Shuohang and Xu, Yichong and Fang, Yuwei and Liu, Yang
                   and Sun, Siqi and Xu, Ruochen and Zhu, Chenguang and Zeng,
                   Michael",
  journal       = "arXiv [cs.CL]",
  abstract      = "Retrieval-based methods have been shown to be effective in
                   NLP tasks via introducing external knowledge. However, the
                   indexing and retrieving of large-scale corpora bring
                   considerable computational cost. Surprisingly, we found that
                   REtrieving from the traINing datA (REINA) only can lead to
                   significant gains on multiple NLG and NLU tasks. We retrieve
                   the labeled training instances most similar to the input text
                   and then concatenate them with the input to feed into the
                   model to generate the output. Experimental results show that
                   this simple method can achieve significantly better
                   performance on a variety of NLU and NLG tasks, including
                   summarization, machine translation, language modeling, and
                   question answering tasks. For instance, our proposed method
                   achieved state-of-the-art results on XSum, BigPatent, and
                   CommonsenseQA. Our code is released,
                   https://github.com/microsoft/REINA .",
  month         =  mar,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2203.08773"
}

@ARTICLE{Cao2020-es,
  title         = "{DeFormer}: Decomposing Pre-trained Transformers for Faster
                   Question Answering",
  author        = "Cao, Qingqing and Trivedi, Harsh and Balasubramanian, Aruna
                   and Balasubramanian, Niranjan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Transformer-based QA models use input-wide self-attention --
                   i.e. across both the question and the input passage -- at all
                   layers, causing them to be slow and memory-intensive. It
                   turns out that we can get by without input-wide
                   self-attention at all layers, especially in the lower layers.
                   We introduce DeFormer, a decomposed transformer, which
                   substitutes the full self-attention with question-wide and
                   passage-wide self-attentions in the lower layers. This allows
                   for question-independent processing of the input text
                   representations, which in turn enables pre-computing passage
                   representations reducing runtime compute drastically.
                   Furthermore, because DeFormer is largely similar to the
                   original model, we can initialize DeFormer with the
                   pre-training weights of a standard transformer, and directly
                   fine-tune on the target QA dataset. We show DeFormer versions
                   of BERT and XLNet can be used to speed up QA by over 4.3x and
                   with simple distillation-based losses they incur only a 1\%
                   drop in accuracy. We open source the code at
                   https://github.com/StonyBrookNLP/deformer.",
  month         =  may,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2005.00697"
}

@ARTICLE{Mavi2022-ku,
  title         = "A survey on multi-hop Question Answering and generation",
  author        = "Mavi, Vaibhav and Jangra, Anubhav and Jatowt, Adam",
  journal       = "arXiv [cs.CL]",
  publisher     = "arXiv",
  abstract      = "The problem of Question Answering (QA) has attracted
                   significant research interest for long. Its relevance to
                   language understanding and knowledge retrieval tasks, along
                   with the simple setting makes the task of QA crucial for
                   strong AI systems. Recent success on simple QA tasks has
                   shifted the focus to more complex settings. Among these,
                   Multi-Hop QA (MHQA) is one of the most researched tasks over
                   the recent years. The ability to answer multi-hop questions
                   and perform multi step reasoning can significantly improve
                   the utility of NLP systems. Consequently, the field has seen
                   a sudden surge with high quality datasets, models and
                   evaluation strategies. The notion of `multiple hops' is
                   somewhat abstract which results in a large variety of tasks
                   that require multi-hop reasoning. This implies that different
                   datasets and models differ significantly which makes the
                   field challenging to generalize and survey. This work aims to
                   provide a general and formal definition of MHQA task, and
                   organize and summarize existing MHQA frameworks. We also
                   outline the best methods to create MHQA datasets. The paper
                   provides a systematic and thorough introduction as well as
                   the structuring of the existing attempts to this highly
                   interesting, yet quite challenging task.",
  month         =  apr,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2204.09140",
  doi           = "10.48550/ARXIV.2204.09140"
}

@MISC{noauthor_undated-kc,
  title        = "No title",
  howpublished = "\url{https://academic.oup.com/ser/article/21/2/1217/7030814}",
  note         = "Accessed: 2023-6-13"
}

@MISC{noauthor_undated-mz,
  howpublished = "\url{https://aclanthology.org/2022.amta-research.9}",
  note         = "Accessed: 2023-6-11"
}

@ARTICLE{Singh2023-sf,
  title         = "Enhancing textbooks with visuals from the web for improved
                   learning",
  author        = "Singh, Janvijay and Zouhar, Vilém and Sachan, Mrinmaya",
  journal       = "arXiv [cs.CV]",
  abstract      = "Textbooks are one of the main mediums for delivering
                   high-quality education to students. In particular,
                   explanatory and illustrative visuals play a key role in
                   retention, comprehension and general transfer of knowledge.
                   However, many textbooks lack these interesting visuals to
                   support student learning. In this paper, we investigate the
                   effectiveness of vision-language models to automatically
                   enhance textbooks with images from the web. We collect a
                   dataset of e-textbooks in the math, science, social science
                   and business domains. We then set up a text-image matching
                   task that involves retrieving and appropriately assigning web
                   images to textbooks, which we frame as a matching
                   optimization problem. Through a crowd-sourced evaluation, we
                   verify that (1) while the original textbook images are rated
                   higher, automatically assigned ones are not far behind, and
                   (2) the precise formulation of the optimization problem
                   matters. We release the dataset of textbooks with an
                   associated image bank to inspire further research in this
                   intersectional area of computer vision and NLP for education.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2304.08931"
}

@ARTICLE{Stengel-Eskin2022-uh,
  title         = "When more data hurts: A troubling quirk in developing
                   broad-coverage natural language understanding systems",
  author        = "Stengel-Eskin, Elias and Platanios, Emmanouil Antonios and
                   Pauls, Adam and Thomson, Sam and Fang, Hao and Van Durme,
                   Benjamin and Eisner, Jason and Su, Yu",
  journal       = "arXiv [cs.CL]",
  abstract      = "In natural language understanding (NLU) production systems,
                   users' evolving needs necessitate the addition of new
                   features over time, indexed by new symbols added to the
                   meaning representation space. This requires additional
                   training data and results in ever-growing datasets. We
                   present the first systematic investigation of this
                   incremental symbol learning scenario. Our analysis reveals a
                   troubling quirk in building broad-coverage NLU systems: as
                   the training dataset grows, performance on the new symbol
                   often decreases if we do not accordingly increase its
                   training data. This suggests that it becomes more difficult
                   to learn new symbols with a larger training dataset. We show
                   that this trend holds for multiple mainstream models on two
                   common NLU tasks: intent recognition and semantic parsing.
                   Rejecting class imbalance as the sole culprit, we reveal that
                   the trend is closely associated with an effect we call source
                   signal dilution, where strong lexical cues for the new symbol
                   become diluted as the training dataset grows. Selectively
                   dropping training examples to prevent dilution often reverses
                   the trend, showing the over-reliance of mainstream neural NLU
                   models on simple lexical cues. Code, models, and data are
                   available at https://aka.ms/nlu-incremental-symbol-learning",
  month         =  may,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2205.12228"
}

@ARTICLE{Zeng2022-jc,
  title         = "{GLM}-{130B}: An Open Bilingual Pre-trained Model",
  author        = "Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan
                   and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan
                   and Zheng, Wendi and Xia, Xiao and Tam, Weng Lam and Ma,
                   Zixuan and Xue, Yufei and Zhai, Jidong and Chen, Wenguang and
                   Zhang, Peng and Dong, Yuxiao and Tang, Jie",
  journal       = "arXiv [cs.CL]",
  abstract      = "We introduce GLM-130B, a bilingual (English and Chinese)
                   pre-trained language model with 130 billion parameters. It is
                   an attempt to open-source a 100B-scale model at least as good
                   as GPT-3 and unveil how models of such a scale can be
                   successfully pre-trained. Over the course of this effort, we
                   face numerous unexpected technical and engineering
                   challenges, particularly on loss spikes and disconvergence.
                   In this paper, we introduce the training process of GLM-130B
                   including its design choices, training strategies for both
                   efficiency and stability, and engineering efforts. The
                   resultant GLM-130B model offers significant outperformance
                   over GPT-3 175B on a wide range of popular English benchmarks
                   while the performance advantage is not observed in OPT-175B
                   and BLOOM-176B. It also consistently and significantly
                   outperforms ERNIE TITAN 3.0 260B -- the largest Chinese
                   language model -- across related benchmarks. Finally, we
                   leverage a unique scaling property of GLM-130B to reach INT4
                   quantization, without quantization aware training and with
                   almost no performance loss, making it the first among
                   100B-scale models. More importantly, the property allows its
                   effective inference on 4$\times$RTX 3090 (24G) or
                   8$\times$RTX 2080 Ti (11G) GPUs, the most ever affordable
                   GPUs required for using 100B-scale models. The GLM-130B model
                   weights are publicly accessible and its code, training logs,
                   related toolkit, and lessons learned are open-sourced at
                   https://github.com/THUDM/GLM-130B .",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2210.02414"
}

@INPROCEEDINGS{Caciularu2022-du,
  title     = "Long Context Question Answering via Supervised Contrastive
               Learning",
  author    = "Caciularu, Avi and Dagan, Ido and Goldberger, Jacob and Cohan,
               Arman",
  booktitle = "Proceedings of the 2022 Conference of the North American Chapter
               of the Association for Computational Linguistics: Human Language
               Technologies",
  publisher = "Association for Computational Linguistics",
  address   = "Seattle, United States",
  pages     = "2872--2879",
  abstract  = "Long-context question answering (QA) tasks require reasoning over
               a long document or multiple documents. Addressing these tasks
               often benefits from identifying a set of evidence spans (e.g.,
               sentences), which provide supporting evidence for answering the
               question.In this work, we propose a novel method for equipping
               long-context QA models with an additional sequence-level
               objective for better identification of the supporting evidence.We
               achieve this via an additional contrastive supervision signal in
               finetuning, where the model is encouraged to explicitly
               discriminate supporting evidence sentences from negative ones by
               maximizing question-evidence similarity. The proposed additional
               loss exhibits consistent improvements on three different strong
               long-context transformer models, across two challenging question
               answering benchmarks -- HotpotQA and QAsper.",
  month     =  jul,
  year      =  2022,
  doi       = "10.18653/v1/2022.naacl-main.207"
}

@INPROCEEDINGS{Jain2020-jn,
  title     = "{SciREX}: A Challenge Dataset for Document-Level Information
               Extraction",
  author    = "Jain, Sarthak and van Zuylen, Madeleine and Hajishirzi, Hannaneh
               and Beltagy, Iz",
  booktitle = "Proceedings of the 58th Annual Meeting of the Association for
               Computational Linguistics",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  year      =  2020,
  doi       = "10.18653/v1/2020.acl-main.670"
}

@ARTICLE{Groeneveld2020-qz,
  title         = "A Simple Yet Strong Pipeline for {HotpotQA}",
  author        = "Groeneveld, Dirk and Khot, Tushar and {Mausam} and Sabharwal,
                   Ashish",
  journal       = "arXiv [cs.CL]",
  abstract      = "State-of-the-art models for multi-hop question answering
                   typically augment large-scale language models like BERT with
                   additional, intuitively useful capabilities such as named
                   entity recognition, graph-based reasoning, and question
                   decomposition. However, does their strong performance on
                   popular multi-hop datasets really justify this added design
                   complexity? Our results suggest that the answer may be no,
                   because even our simple pipeline based on BERT, named Quark,
                   performs surprisingly well. Specifically, on HotpotQA, Quark
                   outperforms these models on both question answering and
                   support identification (and achieves performance very close
                   to a RoBERTa model). Our pipeline has three steps: 1) use
                   BERT to identify potentially relevant sentences independently
                   of each other; 2) feed the set of selected sentences as
                   context into a standard BERT span prediction model to choose
                   an answer; and 3) use the sentence selection model, now with
                   the chosen answer, to produce supporting sentences. The
                   strong performance of Quark resurfaces the importance of
                   carefully exploring simple model designs before using popular
                   benchmarks to justify the value of complex techniques.",
  month         =  apr,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2004.06753"
}

@ARTICLE{Liu2021-pp,
  title         = "Bridging Subword Gaps in Pretrain-Finetune Paradigm for
                   Natural Language Generation",
  author        = "Liu, Xin and Yang, Baosong and Liu, Dayiheng and Zhang, Haibo
                   and Luo, Weihua and Zhang, Min and Zhang, Haiying and Su,
                   Jinsong",
  journal       = "arXiv [cs.CL]",
  abstract      = "A well-known limitation in pretrain-finetune paradigm lies in
                   its inflexibility caused by the one-size-fits-all vocabulary.
                   This potentially weakens the effect when applying pretrained
                   models into natural language generation (NLG) tasks,
                   especially for the subword distributions between upstream and
                   downstream tasks with significant discrepancy. Towards
                   approaching this problem, we extend the vanilla
                   pretrain-finetune pipeline with an extra embedding transfer
                   step. Specifically, a plug-and-play embedding generator is
                   introduced to produce the representation of any input token,
                   according to pre-trained embeddings of its morphologically
                   similar ones. Thus, embeddings of mismatch tokens in
                   downstream tasks can also be efficiently initialized. We
                   conduct experiments on a variety of NLG tasks under the
                   pretrain-finetune fashion. Experimental results and extensive
                   analyses show that the proposed strategy offers us
                   opportunities to feel free to transfer the vocabulary,
                   leading to more efficient and better performed downstream NLG
                   models.",
  month         =  jun,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2106.06125v1"
}

@INPROCEEDINGS{Wadden2022-zm,
  title     = "{MultiVerS}: Improving scientific claim verification with weak
               supervision and full-document context",
  author    = "Wadden, David and Lo, Kyle and Wang, Lucy and Cohan, Arman and
               Beltagy, Iz and Hajishirzi, Hannaneh",
  booktitle = "Findings of the Association for Computational Linguistics: NAACL
               2022",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "61--76",
  abstract  = "The scientific claim verification task requires an NLP system to
               label scientific documents which Support or Refute an input
               claim, and to select evidentiary sentences (or rationales)
               justifying each predicted label. In this work, we present
               MultiVerS, which predicts a fact-checking label and identifies
               rationales in a multitask fashion based on a shared encoding of
               the claim and full document context. This approach accomplishes
               two key modeling goals. First, it ensures that all relevant
               contextual information is incorporated into each labeling
               decision. Second, it enables the model to learn from instances
               annotated with a document-level fact-checking label, but lacking
               sentence-level rationales. This allows MultiVerS to perform
               weakly-supervised domain adaptation by training on scientific
               documents labeled using high-precision heuristics. Our approach
               outperforms two competitive baselines on three scientific claim
               verification datasets, with particularly strong performance in
               zero / few-shot domain adaptation experiments. Our code and data
               are available at https://github.com/dwadden/multivers.",
  month     =  jul,
  year      =  2022,
  doi       = "10.18653/v1/2022.findings-naacl.6"
}

@ARTICLE{Birhane2021-xl,
  title         = "The Values Encoded in Machine Learning Research",
  author        = "Birhane, Abeba and Kalluri, Pratyusha and Card, Dallas and
                   Agnew, William and Dotan, Ravit and Bao, Michelle",
  journal       = "arXiv [cs.LG]",
  abstract      = "Machine learning currently exerts an outsized influence on
                   the world, increasingly affecting institutional practices and
                   impacted communities. It is therefore critical that we
                   question vague conceptions of the field as value-neutral or
                   universally beneficial, and investigate what specific values
                   the field is advancing. In this paper, we first introduce a
                   method and annotation scheme for studying the values encoded
                   in documents such as research papers. Applying the scheme, we
                   analyze 100 highly cited machine learning papers published at
                   premier machine learning conferences, ICML and NeurIPS. We
                   annotate key features of papers which reveal their values:
                   their justification for their choice of project, which
                   attributes of their project they uplift, their consideration
                   of potential negative consequences, and their institutional
                   affiliations and funding sources. We find that few of the
                   papers justify how their project connects to a societal need
                   (15\%) and far fewer discuss negative potential (1\%).
                   Through line-by-line content analysis, we identify 59 values
                   that are uplifted in ML research, and, of these, we find that
                   the papers most frequently justify and assess themselves
                   based on Performance, Generalization, Quantitative evidence,
                   Efficiency, Building on past work, and Novelty. We present
                   extensive textual evidence and identify key themes in the
                   definitions and operationalization of these values. Notably,
                   we find systematic textual evidence that these top values are
                   being defined and applied with assumptions and implications
                   generally supporting the centralization of power.Finally, we
                   find increasingly close ties between these highly cited
                   papers and tech companies and elite universities.",
  month         =  jun,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2106.15590"
}

@ARTICLE{Ko2021-ii,
  title    = "Discourse Comprehension: A Question Answering Framework to
              Represent Sentence Connections",
  author   = "Ko, Wei-Jen and Dalton, Cutter and Simmons, M and Fisher, Eliza
              and Durrett, Greg and Li, Junyi Jessy",
  journal  = "ArXiv",
  abstract = "A novel paradigm that enables scalable data collection targeting
              the comprehension of news documents, viewing these questions
              through the lens of discourse, and shows that DCQA provides
              valuable supervision for answering open-ended questions. While
              there has been substantial progress in text comprehension through
              simple factoid question answering, more holistic comprehension of
              a discourse still presents a major challenge (Dunietz et al.,
              2020). Someone critically reﬂecting on a text as they read it will
              pose curiosity-driven, often open-ended questions, which reﬂect
              deep understanding of the content and require complex reasoning to
              answer (Ko et al., 2020; Westera et al., 2020). A key challenge in
              building and evaluating models for this type of discourse
              comprehension is the lack of annotated data, especially since
              ﬁnding answers to such questions (which may not be answered at
              all) requires high cognitive load for annotators over long
              documents. This paper presents a novel paradigm that enables
              scalable data collection targeting the comprehension of news
              documents, viewing these questions through the lens of discourse.
              The resulting corpus, DCQA ( D iscourse C omprehension by Q
              uestion A nswering), consists of 22,394 question-answer pairs
              across 606 English documents. DCQA captures both discourse and
              semantic links between sentences in the form of free-form,
              open-ended questions. On an evaluation set that we annotated on
              questions from Ko et al. (2020), we show that DCQA provides
              valuable supervision for answering open-ended questions. We
              additionally design pre-training methods utilizing existing
              question-answering resources, and use synthetic data to
              accommodate unanswerable questions. We release DCQA",
  year     =  2021,
  eprint   = "2111.00701",
  language = "en"
}

@INPROCEEDINGS{King2020-rx,
  title     = "High-Precision Extraction of Emerging Concepts from Scientific
               Literature",
  author    = "King, Daniel and Downey, Doug and Weld, Daniel S",
  booktitle = "Proceedings of the 43rd International ACM SIGIR Conference on
               Research and Development in Information Retrieval",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1549--1552",
  abstract  = "Identification of new concepts in scientific literature can help
               power faceted search, scientific trend analysis, knowledge-base
               construction, and more, but current methods are lacking. Manual
               identification can't keep up with the torrent of new
               publications, while the precision of existing automatic
               techniques is too low for many applications. We present an
               unsupervised concept extraction method for scientific literature
               that achieves much higher precision than previous work. Our
               approach relies on a simple but novel intuition: each scientific
               concept is likely to be introduced or popularized by a single
               paper that is disproportionately cited by subsequent papers
               mentioning the concept. From a corpus of computer science papers
               on arXiv, we find that our method achieves a Precision@1000 of
               99\%, compared to 86\% for prior work, and a substantially better
               precision-yield trade-off across the top 15,000 extractions. To
               stimulate research in this area, we release our code and data.",
  series    = "SIGIR '20",
  month     =  jul,
  year      =  2020,
  keywords  = "concept extraction, scientific literature, citation graph",
  doi       = "10.1145/3397271.3401235",
  isbn      =  9781450380164
}

@ARTICLE{Dasigi2021-tv,
  title         = "A Dataset of Information-Seeking Questions and Answers
                   Anchored in Research Papers",
  author        = "Dasigi, Pradeep and Lo, Kyle and Beltagy, Iz and Cohan, Arman
                   and Smith, Noah A and Gardner, Matt",
  journal       = "arXiv [cs.CL]",
  abstract      = "Readers of academic research papers often read with the goal
                   of answering specific questions. Question Answering systems
                   that can answer those questions can make consumption of the
                   content much more efficient. However, building such tools
                   requires data that reflect the difficulty of the task arising
                   from complex reasoning about claims made in multiple parts of
                   a paper. In contrast, existing information-seeking question
                   answering datasets usually contain questions about generic
                   factoid-type information. We therefore present QASPER, a
                   dataset of 5,049 questions over 1,585 Natural Language
                   Processing papers. Each question is written by an NLP
                   practitioner who read only the title and abstract of the
                   corresponding paper, and the question seeks information
                   present in the full text. The questions are then answered by
                   a separate set of NLP practitioners who also provide
                   supporting evidence to answers. We find that existing models
                   that do well on other QA tasks do not perform well on
                   answering these questions, underperforming humans by at least
                   27 F1 points when answering them from entire papers,
                   motivating further research in document-grounded,
                   information-seeking QA, which our dataset is designed to
                   facilitate.",
  month         =  may,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2105.03011"
}

@ARTICLE{DeYoung2021-ft,
  title         = "{MS2}: Multi-Document Summarization of Medical Studies",
  author        = "DeYoung, Jay and Beltagy, Iz and van Zuylen, Madeleine and
                   Kuehl, Bailey and Wang, Lucy Lu",
  journal       = "arXiv [cs.CL]",
  abstract      = "To assess the effectiveness of any medical intervention,
                   researchers must conduct a time-intensive and highly manual
                   literature review. NLP systems can help to automate or assist
                   in parts of this expensive process. In support of this goal,
                   we release MS\textasciicircum2 (Multi-Document Summarization
                   of Medical Studies), a dataset of over 470k documents and 20k
                   summaries derived from the scientific literature. This
                   dataset facilitates the development of systems that can
                   assess and aggregate contradictory evidence across multiple
                   studies, and is the first large-scale, publicly available
                   multi-document summarization dataset in the biomedical
                   domain. We experiment with a summarization system based on
                   BART, with promising early results. We formulate our
                   summarization inputs and targets in both free text and
                   structured forms and modify a recently proposed metric to
                   assess the quality of our system's generated summaries. Data
                   and models are available at https://github.com/allenai/ms2",
  month         =  apr,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2104.06486"
}

@ARTICLE{Chen2022-mh,
  title         = "Augmenting Pre-trained Language Models with {QA}-Memory for
                   Open-Domain Question Answering",
  author        = "Chen, Wenhu and Verga, Pat and de Jong, Michiel and Wieting,
                   John and Cohen, William",
  journal       = "arXiv [cs.CL]",
  abstract      = "Retrieval augmented language models have recently become the
                   standard for knowledge intensive tasks. Rather than relying
                   purely on latent semantics within the parameters of large
                   neural models, these methods enlist a semi-parametric memory
                   to encode an index of knowledge for the model to retrieve
                   over. Most prior work has employed text passages as the unit
                   of knowledge, which has high coverage at the cost of
                   interpretability, controllability, and efficiency. The
                   opposite properties arise in other methods which have instead
                   relied on knowledge base (KB) facts. At the same time, more
                   recent work has demonstrated the effectiveness of storing and
                   retrieving from an index of Q-A pairs derived from text
                   \citep{lewis2021paq}. This approach yields a high coverage
                   knowledge representation that maintains KB-like properties
                   due to its representations being more atomic units of
                   information. In this work we push this line of research
                   further by proposing a question-answer augmented
                   encoder-decoder model and accompanying pretraining strategy.
                   This yields an end-to-end system that not only outperforms
                   prior QA retrieval methods on single-hop QA tasks but also
                   enables compositional reasoning, as demonstrated by strong
                   performance on two multi-hop QA datasets. Together, these
                   methods improve the ability to interpret and control the
                   model while narrowing the performance gap with passage
                   retrieval systems.",
  month         =  apr,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2204.04581"
}

@ARTICLE{Chen2022-hg,
  title         = "{MuRAG}: Multimodal Retrieval-Augmented Generator for Open
                   Question Answering over Images and Text",
  author        = "Chen, Wenhu and Hu, Hexiang and Chen, Xi and Verga, Pat and
                   Cohen, William W",
  journal       = "arXiv [cs.CL]",
  abstract      = "While language Models store a massive amount of world
                   knowledge implicitly in their parameters, even very large
                   models often fail to encode information about rare entities
                   and events, while incurring huge computational costs.
                   Recently, retrieval-augmented models, such as REALM, RAG, and
                   RETRO, have incorporated world knowledge into language
                   generation by leveraging an external non-parametric index and
                   have demonstrated impressive performance with constrained
                   model sizes. However, these methods are restricted to
                   retrieving only textual knowledge, neglecting the ubiquitous
                   amount of knowledge in other modalities like images -- much
                   of which contains information not covered by any text. To
                   address this limitation, we propose the first Multimodal
                   Retrieval-Augmented Transformer (MuRAG), which accesses an
                   external non-parametric multimodal memory to augment language
                   generation. MuRAG is pre-trained with a mixture of
                   large-scale image-text and text-only corpora using a joint
                   contrastive and generative loss. We perform experiments on
                   two different datasets that require retrieving and reasoning
                   over both images and text to answer a given query: WebQA, and
                   MultimodalQA. Our results show that MuRAG achieves
                   state-of-the-art accuracy, outperforming existing models by
                   10-20\% absolute on both datasets and under both distractor
                   and full-wiki settings.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2210.02928"
}

@ARTICLE{Wu2022-qu,
  title         = "An Efficient Memory-Augmented Transformer for
                   Knowledge-Intensive {NLP} Tasks",
  author        = "Wu, Yuxiang and Zhao, Yu and Hu, Baotian and Minervini,
                   Pasquale and Stenetorp, Pontus and Riedel, Sebastian",
  journal       = "arXiv [cs.CL]",
  abstract      = "Access to external knowledge is essential for many natural
                   language processing tasks, such as question answering and
                   dialogue. Existing methods often rely on a parametric model
                   that stores knowledge in its parameters, or use a
                   retrieval-augmented model that has access to an external
                   knowledge source. Parametric and retrieval-augmented models
                   have complementary strengths in terms of computational
                   efficiency and predictive accuracy. To combine the strength
                   of both approaches, we propose the Efficient Memory-Augmented
                   Transformer (EMAT) -- it encodes external knowledge into a
                   key-value memory and exploits the fast maximum inner product
                   search for memory querying. We also introduce pre-training
                   tasks that allow EMAT to encode informative key-value
                   representations, and to learn an implicit strategy to
                   integrate multiple memory slots into the transformer.
                   Experiments on various knowledge-intensive tasks such as
                   question answering and dialogue datasets show that, simply
                   augmenting parametric models (T5-base) using our method
                   produces more accurate results (e.g., 25.8 -> 44.3 EM on NQ)
                   while retaining a high throughput (e.g., 1000 queries/s on
                   NQ). Compared to retrieval-augmented models, EMAT runs
                   substantially faster across the board and produces more
                   accurate results on WoW and ELI5. Our code and datasets are
                   available at https://github. com/uclnlp/EMAT.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2210.16773"
}

@ARTICLE{Lewis2021-mc,
  title     = "Paq: 65 million probably-asked questions and what you can do with
               them",
  author    = "Lewis, Patrick and Wu, Yuxiang and Liu, Linqing and Minervini,
               Pasquale and Küttler, Heinrich and Piktus, Aleksandra and
               Stenetorp, Pontus and Riedel, Sebastian",
  journal   = "Transactions of the Association for Computational Linguistics",
  publisher = "MIT Press",
  volume    =  9,
  pages     = "1098--1115",
  year      =  2021
}

@ARTICLE{Zhao2022-lc,
  title         = "On Measuring the Intrinsic Few-Shot Hardness of Datasets",
  author        = "Zhao, Xinran and Murty, Shikhar and Manning, Christopher D",
  journal       = "arXiv [cs.CL]",
  abstract      = "While advances in pre-training have led to dramatic
                   improvements in few-shot learning of NLP tasks, there is
                   limited understanding of what drives successful few-shot
                   adaptation in datasets. In particular, given a new dataset
                   and a pre-trained model, what properties of the dataset make
                   it \emph{few-shot learnable} and are these properties
                   independent of the specific adaptation techniques used? We
                   consider an extensive set of recent few-shot learning
                   methods, and show that their performance across a large
                   number of datasets is highly correlated, showing that
                   few-shot hardness may be intrinsic to datasets, for a given
                   pre-trained model. To estimate intrinsic few-shot hardness,
                   we then propose a simple and lightweight metric called
                   ``Spread'' that captures the intuition that few-shot learning
                   is made possible by exploiting feature-space invariances
                   between training and test samples. Our metric better accounts
                   for few-shot hardness compared to existing notions of
                   hardness, and is ~8-100x faster to compute.",
  month         =  nov,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2211.09113"
}

@ARTICLE{Boytsov2023-gs,
  title         = "{InPars}-light: Cost-effective unsupervised training of
                   efficient rankers",
  author        = "Boytsov, Leonid and Patel, Preksha and Sourabh, Vivek and
                   Nisar, Riddhi and Kundu, Sayani and Ramanathan, Ramya and
                   Nyberg, Eric",
  journal       = "arXiv [cs.IR]",
  publisher     = "arXiv",
  abstract      = "We carried out a reproducibility study of InPars recipe for
                   unsupervised training of neural rankers. As a by-product of
                   this study, we developed a simple-yet-effective modification
                   of InPars, which we called InPars-light. Unlike InPars,
                   InPars-light uses only a freely available language model
                   BLOOM and 7x-100x smaller ranking models. On all five English
                   retrieval collections (used in the original InPars study) we
                   obtained substantial (7-30\%) and statistically significant
                   improvements over BM25 in nDCG or MRR using only a 30M
                   parameter six-layer MiniLM ranker. In contrast, in the InPars
                   study only a 100x larger MonoT5-3B model consistently
                   outperformed BM25, whereas their smaller MonoT5-220M model
                   (which is still 7x larger than our MiniLM ranker),
                   outperformed BM25 only on MS MARCO and TREC DL 2020. In a
                   purely unsupervised setting, our 435M parameter DeBERTA v3
                   ranker was roughly at par with the 7x larger MonoT5-3B: In
                   fact, on three out of five datasets, it slightly outperformed
                   MonoT5-3B. Finally, these good results were achieved by
                   re-ranking only 100 candidate documents compared to 1000 used
                   in InPars. We believe that InPars-light is the first truly
                   cost-effective prompt-based unsupervised recipe to train and
                   deploy neural ranking models that outperform BM25.",
  month         =  jan,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "2301.02998",
  doi           = "10.48550/ARXIV.2301.02998"
}

@ARTICLE{Bonifacio2022-wa,
  title         = "{InPars}: Data augmentation for information retrieval using
                   large language models",
  author        = "Bonifacio, Luiz and Abonizio, Hugo and Fadaee, Marzieh and
                   Nogueira, Rodrigo",
  journal       = "arXiv [cs.CL]",
  publisher     = "arXiv",
  abstract      = "The information retrieval community has recently witnessed a
                   revolution due to large pretrained transformer models.
                   Another key ingredient for this revolution was the MS MARCO
                   dataset, whose scale and diversity has enabled zero-shot
                   transfer learning to various tasks. However, not all IR tasks
                   and domains can benefit from one single dataset equally.
                   Extensive research in various NLP tasks has shown that using
                   domain-specific training data, as opposed to a
                   general-purpose one, improves the performance of neural
                   models. In this work, we harness the few-shot capabilities of
                   large pretrained language models as synthetic data generators
                   for IR tasks. We show that models finetuned solely on our
                   unsupervised dataset outperform strong baselines such as BM25
                   as well as recently proposed self-supervised dense retrieval
                   methods. Furthermore, retrievers finetuned on both supervised
                   and our synthetic data achieve better zero-shot transfer than
                   models finetuned only on supervised data. Code, models, and
                   data are available at
                   https://github.com/zetaalphavector/inpars .",
  month         =  feb,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2202.05144",
  doi           = "10.48550/ARXIV.2202.05144"
}

@ARTICLE{Wei2022-lo,
  title         = "Chain-of-Thought Prompting Elicits Reasoning in Large
                   Language Models",
  author        = "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma,
                   Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le,
                   Quoc and Zhou, Denny",
  journal       = "arXiv [cs.CL]",
  abstract      = "We explore how generating a chain of thought -- a series of
                   intermediate reasoning steps -- significantly improves the
                   ability of large language models to perform complex
                   reasoning. In particular, we show how such reasoning
                   abilities emerge naturally in sufficiently large language
                   models via a simple method called chain of thought prompting,
                   where a few chain of thought demonstrations are provided as
                   exemplars in prompting. Experiments on three large language
                   models show that chain of thought prompting improves
                   performance on a range of arithmetic, commonsense, and
                   symbolic reasoning tasks. The empirical gains can be
                   striking. For instance, prompting a 540B-parameter language
                   model with just eight chain of thought exemplars achieves
                   state of the art accuracy on the GSM8K benchmark of math word
                   problems, surpassing even finetuned GPT-3 with a verifier.",
  month         =  jan,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2201.11903"
}

@INPROCEEDINGS{Yang2015-kp,
  title     = "Leveraging Procedural Knowledge for Task-oriented Search",
  author    = "Yang, Zi and Nyberg, Eric",
  booktitle = "Proceedings of the 38th International ACM SIGIR Conference on
               Research and Development in Information Retrieval",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "513--522",
  abstract  = "Many search engine users attempt to satisfy an information need
               by issuing multiple queries, with the expectation that each
               result will contribute some portion of the required information.
               Previous research has shown that structured or semi-structured
               descriptive knowledge bases (such as Wikipedia) can be used to
               improve search quality and experience for general or
               entity-centric queries. However, such resources do not have
               sufficient coverage of procedural knowledge, i.e. what actions
               should be performed and what factors should be considered to
               achieve some goal; such procedural knowledge is crucial when
               responding to task-oriented search queries. This paper provides a
               first attempt to bridge the gap between two evolving research
               areas: development of procedural knowledge bases (such as
               wikiHow) and task-oriented search. We investigate whether
               task-oriented search can benefit from existing procedural
               knowledge (search task suggestion) and whether automatic
               procedural knowledge construction can benefit from users' search
               activities (automatic procedural knowledge base construction). We
               propose to create a three-way parallel corpus of queries, query
               contexts, and task descriptions, and reduce both problems to
               sequence labeling tasks. We propose a set of textual features and
               structural features to identify key search phrases from task
               descriptions, and then adapt similar features to extract
               wikiHow-style procedural knowledge descriptions from search
               queries and relevant text snippets. We compare our proposed
               solution with baseline algorithms, commercial search engines, and
               the (manually-curated) wikiHow procedural knowledge; experimental
               results show an improvement of +0.28 to +0.41 in terms of
               Precision@8 and mean average precision (MAP).",
  series    = "SIGIR '15",
  month     =  aug,
  year      =  2015,
  keywords  = "search intent, query suggestion, wikihow, search log, procedural
               knowledge base",
  doi       = "10.1145/2766462.2767744",
  isbn      =  9781450336215
}

@INPROCEEDINGS{Rajagopal2020-js,
  title     = "What-if {I} ask you to explain: Explaining the effects of
               perturbations in procedural text",
  author    = "Rajagopal, Dheeraj and Tandon, Niket and Clark, Peter and Dalvi,
               Bhavana and Hovy, Eduard",
  booktitle = "Findings of the Association for Computational Linguistics: EMNLP
               2020",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  abstract  = "QUARTET, a system that constructs explanations from the sentences
               in the procedural text, achieves ~18 points better on explanation
               accuracy compared to several strong baselines on a recent process
               comprehension benchmark, and shows a surprising finding that good
               explanations do not have to come at the expense of end task
               performance. Our goal is to explain the effects of perturbations
               in procedural text, e.g., given a passage describing a rabbit’s
               life cycle, explain why illness (the perturbation) may reduce the
               rabbit population (the effect). Although modern systems are able
               to solve the original prediction task well (e.g., illness results
               in less rabbits), the explanation task - identifying the causal
               chain of events from perturbation to effect - remains largely
               unaddressed, and is the goal of this research. We present
               QUARTET, a system that constructs such explanations from
               paragraphs, by modeling the explanation task as a multitask
               learning problem. QUARTET constructs explanations from the
               sentences in the procedural text, achieving ~18 points better on
               explanation accuracy compared to several strong baselines on a
               recent process comprehension benchmark. On an end task on this
               benchmark, we show a surprising finding that good explanations do
               not have to come at the expense of end task performance, in fact
               leading to a 7\% F1 improvement over SOTA.",
  year      =  2020,
  doi       = "10.18653/v1/2020.findings-emnlp.300",
  language  = "en"
}

@ARTICLE{Gao2023-oz,
  title         = "Enabling large language models to generate text with
                   citations",
  author        = "Gao, Tianyu and Yen, Howard and Yu, Jiatong and Chen, Danqi",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large language models (LLMs) have emerged as a widely-used
                   tool for information seeking, but their generated outputs are
                   prone to hallucination. In this work, we aim to enable LLMs
                   to generate text with citations, improving their factual
                   correctness and verifiability. Existing work mainly relies on
                   commercial search engines and human evaluation, making it
                   challenging to reproduce and compare with different modeling
                   approaches. We propose ALCE, the first benchmark for
                   Automatic LLMs' Citation Evaluation. ALCE collects a diverse
                   set of questions and retrieval corpora and requires building
                   end-to-end systems to retrieve supporting evidence and
                   generate answers with citations. We build automatic metrics
                   along three dimensions -- fluency, correctness, and citation
                   quality -- and demonstrate their strong correlation with
                   human judgements. Our experiments with state-of-the-art LLMs
                   and novel prompting strategies show that current systems have
                   considerable room for improvements -- for example, on the
                   ELI5 dataset, even the best model has 49\% of its generations
                   lacking complete citation support. Our extensive analyses
                   further highlight promising future directions, including
                   developing better retrievers, advancing long-context LLMs,
                   and improving the ability to synthesize information from
                   multiple sources.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.14627"
}

@ARTICLE{Gao2022-jc,
  title         = "{RARR}: Researching and revising what language models say,
                   using language models",
  author        = "Gao, Luyu and Dai, Zhuyun and Pasupat, Panupong and Chen,
                   Anthony and Chaganty, Arun Tejasvi and Fan, Yicheng and Zhao,
                   Vincent Y and Lao, Ni and Lee, Hongrae and Juan, Da-Cheng and
                   Guu, Kelvin",
  journal       = "arXiv [cs.CL]",
  abstract      = "Language models (LMs) now excel at many tasks such as
                   few-shot learning, question answering, reasoning, and dialog.
                   However, they sometimes generate unsupported or misleading
                   content. A user cannot easily determine whether their outputs
                   are trustworthy or not, because most LMs do not have any
                   built-in mechanism for attribution to external evidence. To
                   enable attribution while still preserving all the powerful
                   advantages of recent generation models, we propose RARR
                   (Retrofit Attribution using Research and Revision), a system
                   that 1) automatically finds attribution for the output of any
                   text generation model and 2) post-edits the output to fix
                   unsupported content while preserving the original output as
                   much as possible. When applied to the output of several
                   state-of-the-art LMs on a diverse set of generation tasks, we
                   find that RARR significantly improves attribution while
                   otherwise preserving the original input to a much greater
                   degree than previously explored edit models. Furthermore, the
                   implementation of RARR requires only a handful of training
                   examples, a large language model, and standard web search.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2210.08726"
}

@ARTICLE{Ravfogel2023-bj,
  title         = "Retrieving Texts based on Abstract Descriptions",
  author        = "Ravfogel, Shauli and Pyatkin, Valentina and Cohen, Amir D N
                   and Manevich, Avshalom and Goldberg, Yoav",
  journal       = "arXiv [cs.CL]",
  abstract      = "In this work, we aim to connect two research areas:
                   instruction models and retrieval-based models. While
                   instruction-tuned Large Language Models (LLMs) excel at
                   extracting information from text, they are not suitable for
                   semantic retrieval. Similarity search over embedding vectors
                   allows to index and query vectors, but the similarity
                   reflected in the embedding is sub-optimal for many use cases.
                   We identify the task of retrieving sentences based on
                   abstract descriptions of their content. We demonstrate the
                   inadequacy of current text embeddings and propose an
                   alternative model that significantly improves when used in
                   standard nearest neighbor search. The model is trained using
                   positive and negative pairs sourced through prompting an a
                   large language model (LLM). While it is easy to source the
                   training material from an LLM, the retrieval task cannot be
                   performed by the LLM directly. This demonstrates that data
                   from LLMs can be used not only for distilling more efficient
                   specialized models than the original LLM, but also for
                   creating new capabilities not immediately possible using the
                   original model.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.12517"
}

@ARTICLE{Gupta2023-ky,
  title         = "Instruction Tuned Models are Quick Learners",
  author        = "Gupta, Himanshu and Sawant, Saurabh Arjun and Mishra, Swaroop
                   and Nakamura, Mutsumi and Mitra, Arindam and Mashetty,
                   Santosh and Baral, Chitta",
  journal       = "arXiv [cs.CL]",
  abstract      = "Instruction tuning of language models has demonstrated the
                   ability to enhance model generalization to unseen tasks via
                   in-context learning using a few examples. However, typical
                   supervised learning still requires a plethora of downstream
                   training data for finetuning. Often in real-world situations,
                   there is a scarcity of data available for finetuning, falling
                   somewhere between few shot inference and fully supervised
                   finetuning. In this work, we demonstrate the sample
                   efficiency of instruction tuned models over various tasks by
                   estimating the minimal downstream training data required by
                   them to perform transfer learning and match the performance
                   of state-of-the-art (SOTA) supervised models. We conduct
                   experiments on 119 tasks from Super Natural Instructions
                   (SuperNI) in both the single task learning (STL) and multi
                   task learning (MTL) settings. Our findings reveal that, in
                   the STL setting, instruction tuned models equipped with 25\%
                   of the downstream train data surpass the SOTA performance on
                   the downstream tasks. In the MTL setting, an instruction
                   tuned model trained on only 6\% of downstream training data
                   achieve SOTA, while using 100\% of the training data results
                   in a 3.69\% points improvement (ROUGE-L 74.68) over the
                   previous SOTA. We conduct an analysis on T5 vs Tk-Instruct by
                   developing several baselines to demonstrate that instruction
                   tuning aids in increasing both sample efficiency and transfer
                   learning. Additionally, we observe a consistent ~4\%
                   performance increase in both settings when pre-finetuning is
                   performed with instructions. Finally, we conduct a
                   categorical study and find that contrary to previous results,
                   tasks in the question rewriting and title generation
                   categories suffer from instruction tuning.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2306.05539"
}

@ARTICLE{Gudibande2023-op,
  title         = "The false promise of imitating proprietary {LLMs}",
  author        = "Gudibande, Arnav and Wallace, Eric and Snell, Charlie and
                   Geng, Xinyang and Liu, Hao and Abbeel, Pieter and Levine,
                   Sergey and Song, Dawn",
  journal       = "arXiv [cs.CL]",
  abstract      = "An emerging method to cheaply improve a weaker language model
                   is to finetune it on outputs from a stronger model, such as a
                   proprietary system like ChatGPT (e.g., Alpaca, Self-Instruct,
                   and others). This approach looks to cheaply imitate the
                   proprietary model's capabilities using a weaker open-source
                   model. In this work, we critically analyze this approach. We
                   first finetune a series of LMs that imitate ChatGPT using
                   varying base model sizes (1.5B--13B), data sources, and
                   imitation data amounts (0.3M--150M tokens). We then evaluate
                   the models using crowd raters and canonical NLP benchmarks.
                   Initially, we were surprised by the output quality of our
                   imitation models -- they appear far better at following
                   instructions, and crowd workers rate their outputs as
                   competitive with ChatGPT. However, when conducting more
                   targeted automatic evaluations, we find that imitation models
                   close little to none of the gap from the base LM to ChatGPT
                   on tasks that are not heavily supported in the imitation
                   data. We show that these performance discrepancies may slip
                   past human raters because imitation models are adept at
                   mimicking ChatGPT's style but not its factuality. Overall, we
                   conclude that model imitation is a false promise: there
                   exists a substantial capabilities gap between open and closed
                   LMs that, with current methods, can only be bridged using an
                   unwieldy amount of imitation data or by using more capable
                   base LMs. In turn, we argue that the highest leverage action
                   for improving open-source models is to tackle the difficult
                   challenge of developing better base LMs, rather than taking
                   the shortcut of imitating proprietary systems.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.15717"
}

@ARTICLE{Saad-Falcon2023-gg,
  title         = "{UDAPDR}: Unsupervised domain adaptation via {LLM} prompting
                   and distillation of rerankers",
  author        = "Saad-Falcon, Jon and Khattab, Omar and Santhanam, Keshav and
                   Florian, Radu and Franz, Martin and Roukos, Salim and Sil,
                   Avirup and Sultan, Md Arafat and Potts, Christopher",
  journal       = "arXiv [cs.IR]",
  abstract      = "Many information retrieval tasks require large labeled
                   datasets for fine-tuning. However, such datasets are often
                   unavailable, and their utility for real-world applications
                   can diminish quickly due to domain shifts. To address this
                   challenge, we develop and motivate a method for using large
                   language models (LLMs) to generate large numbers of synthetic
                   queries cheaply. The method begins by generating a small
                   number of synthetic queries using an expensive LLM. After
                   that, a much less expensive one is used to create large
                   numbers of synthetic queries, which are used to fine-tune a
                   family of reranker models. These rerankers are then distilled
                   into a single efficient retriever for use in the target
                   domain. We show that this technique boosts zero-shot accuracy
                   in long-tail domains, even where only 2K synthetic queries
                   are used for fine-tuning, and that it achieves substantially
                   lower latency than standard reranking methods. We make our
                   end-to-end approach, including our synthetic datasets and
                   replication code, publicly available on Github:
                   https://github.com/primeqa/primeqa.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "2303.00807"
}

@ARTICLE{Widder2022-sn,
  title         = "Dislocated accountabilities in the {AI} supply chain:
                   Modularity and developers' notions of responsibility",
  author        = "Widder, David Gray and Nafus, Dawn",
  journal       = "arXiv [cs.CY]",
  abstract      = "Responsible AI guidelines often ask engineers to consider how
                   their systems might harm. However, contemporary AI systems
                   are built by composing many preexisting software modules that
                   pass through many hands before becoming a finished product or
                   service. How does this shape responsible AI practice? In
                   interviews with 27 AI engineers across industry, open source,
                   and academia, our participants often did not see the
                   questions posed in responsible AI guidelines to be within
                   their agency, capability, or responsibility to address. We
                   use Lucy Suchman's notion of located accountability to show
                   how responsible AI labor is currently organized, and to
                   explore how it could be done differently. We identify
                   cross-cutting social logics, like modularizability, scale,
                   reputation, and customer orientation, that organize which
                   responsible AI actions do take place, and which are relegated
                   to low status staff or believed to be the work of the next or
                   previous person in the chain. We argue that current
                   responsible AI interventions, like ethics checklists and
                   guidelines that assume panoptical knowledge and control over
                   systems, could improve by taking a located accountability
                   approach, where relations and obligations intertwine and
                   incrementally add value in the process. This would constitute
                   a shift from ``supply chain' thinking to ''value chain``
                   thinking.",
  month         =  sep,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY",
  eprint        = "2209.09780"
}

@ARTICLE{Hedderich2020-bt,
  title         = "A survey on recent approaches for natural language processing
                   in low-resource scenarios",
  author        = "Hedderich, Michael A and Lange, Lukas and Adel, Heike and
                   Strötgen, Jannik and Klakow, Dietrich",
  journal       = "arXiv [cs.CL]",
  abstract      = "Current developments in natural language processing offer
                   challenges and opportunities for low-resource languages and
                   domains. Deep neural networks are known for requiring large
                   amounts of training data which might not be available in
                   resource-lean scenarios. However, there is also a growing
                   body of works to improve the performance in low-resource
                   settings. Motivated by fundamental changes towards neural
                   models and the currently popular pre-train and fine-tune
                   paradigm, we give an overview of promising approaches for
                   low-resource natural language processing. After a discussion
                   about the definition of low-resource scenarios and the
                   different dimensions of data availability, we then examine
                   methods that enable learning when training data is sparse.
                   This includes mechanisms to create additional labeled data
                   like data augmentation and distant supervision as well as
                   transfer learning settings that reduce the need for target
                   supervision. The survey closes with a brief look into methods
                   suggested in non-NLP machine learning communities, which
                   might be beneficial for NLP in low-resource scenarios",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2010.12309"
}

@ARTICLE{Agrawal2023-xk,
  title         = "Do language models know when they're hallucinating
                   references?",
  author        = "Agrawal, Ayush and Mackey, Lester and Kalai, Adam Tauman",
  journal       = "arXiv [cs.CL]",
  abstract      = "Current state-of-the-art language models (LMs) are notorious
                   for generating text with ``hallucinations,'' a primary
                   example being book and paper references that lack any solid
                   basis in their training data. However, we find that many of
                   these fabrications can be identified using the same LM, using
                   only black-box queries without consulting any external
                   resources. Consistency checks done with direct queries about
                   whether the generated reference title is real (inspired by
                   Kadavath et al. 2022, Lin et al. 2022, Manakul et al. 2023)
                   are compared to consistency checks with indirect queries
                   which ask for ancillary details such as the authors of the
                   work. These consistency checks are found to be partially
                   reliable indicators of whether or not the reference is a
                   hallucination. In particular, we find that LMs in the
                   GPT-series will hallucinate differing authors of hallucinated
                   references when queried in independent sessions, while it
                   will consistently identify authors of real references. This
                   suggests that the hallucination may be more a result of
                   generation techniques than the underlying representation.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.18248"
}

@ARTICLE{Saikh2022-tu,
  title     = "{ScienceQA}: a novel resource for question answering on scholarly
               articles",
  author    = "Saikh, Tanik and Ghosal, Tirthankar and Mittal, Amish and Ekbal,
               Asif and Bhattacharyya, Pushpak",
  journal   = "International journal on digital libraries",
  publisher = "Springer Science and Business Media LLC",
  volume    =  23,
  number    =  3,
  pages     = "289--301",
  abstract  = "Machine Reading Comprehension (MRC) of a document is a
               challenging problem that requires discourse-level understanding.
               Information extraction from scholarly articles nowadays is a
               critical use case for researchers to understand the underlying
               research quickly and move forward, especially in this age of
               infodemic. MRC on research articles can also provide helpful
               information to the reviewers and editors. However, the main
               bottleneck in building such models is the availability of
               human-annotated data. In this paper, firstly, we introduce a
               dataset to facilitate question answering (QA) on scientific
               articles. We prepare the dataset in a semi-automated fashion
               having more than 100k human-annotated context-question-answer
               triples. Secondly, we implement one baseline QA model based on
               Bidirectional Encoder Representations from Transformers (BERT).
               Additionally, we implement two models: the first one is based on
               Science BERT (SciBERT), and the second is the combination of
               SciBERT and Bi-Directional Attention Flow (Bi-DAF). The best
               model (i.e., SciBERT) obtains an F1 score of 75.46\%. Our dataset
               is novel, and our work opens up a new avenue for scholarly
               document processing research by providing a benchmark QA dataset
               and standard baseline. We make our dataset and codes available
               here at
               https://github.com/TanikSaikh/Scientific-Question-Answering.",
  month     =  jul,
  year      =  2022,
  keywords  = "Automatic article review system; BERT; BiDAF; Machine reading
               comprehension; Question answering; Scholarly articles; SciBERT",
  doi       = "10.1007/s00799-022-00329-y",
  pmc       = "PMC9297303",
  pmid      =  35873651,
  issn      = "1432-5012,1432-1300",
  language  = "en"
}

@ARTICLE{Ni2021-ku,
  title         = "Large Dual Encoders Are Generalizable Retrievers",
  author        = "Ni, Jianmo and Qu, Chen and Lu, Jing and Dai, Zhuyun and
                   Ábrego, Gustavo Hernández and Ma, Ji and Zhao, Vincent Y and
                   Luan, Yi and Hall, Keith B and Chang, Ming-Wei and Yang,
                   Yinfei",
  journal       = "arXiv [cs.IR]",
  abstract      = "It has been shown that dual encoders trained on one domain
                   often fail to generalize to other domains for retrieval
                   tasks. One widespread belief is that the bottleneck layer of
                   a dual encoder, where the final score is simply a dot-product
                   between a query vector and a passage vector, is too limited
                   to make dual encoders an effective retrieval model for
                   out-of-domain generalization. In this paper, we challenge
                   this belief by scaling up the size of the dual encoder model
                   {\em while keeping the bottleneck embedding size fixed.} With
                   multi-stage training, surprisingly, scaling up the model size
                   brings significant improvement on a variety of retrieval
                   tasks, especially for out-of-domain generalization.
                   Experimental results show that our dual encoders,
                   \textbf{G}eneralizable \textbf{T}5-based dense
                   \textbf{R}etrievers (GTR), outperform
                   \%ColBERT~\cite{khattab2020colbert} and existing sparse and
                   dense retrievers on the BEIR dataset~\cite{thakur2021beir}
                   significantly. Most surprisingly, our ablation study finds
                   that GTR is very data efficient, as it only needs 10\% of MS
                   Marco supervised data to achieve the best out-of-domain
                   performance. All the GTR models are released at
                   https://tfhub.dev/google/collections/gtr/1.",
  month         =  dec,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "2112.07899"
}

@ARTICLE{Fu2023-ts,
  title         = "Specializing smaller language models towards multi-step
                   reasoning",
  author        = "Fu, Yao and Peng, Hao and Ou, Litu and Sabharwal, Ashish and
                   Khot, Tushar",
  journal       = "arXiv [cs.CL]",
  abstract      = "The surprising ability of Large Language Models (LLMs) to
                   perform well on complex reasoning with only few-shot
                   chain-of-thought prompts is believed to emerge only in very
                   large-scale models (100+ billion parameters). We show that
                   such abilities can, in fact, be distilled down from GPT-3.5
                   ($\ge$ 175B) to T5 variants ($\le$ 11B). We propose model
                   specialization, to specialize the model's ability towards a
                   target task. The hypothesis is that large models (commonly
                   viewed as larger than 100B) have strong modeling power, but
                   are spread on a large spectrum of tasks. Small models
                   (commonly viewed as smaller than 10B) have limited model
                   capacity, but if we concentrate their capacity on a specific
                   target task, the model can achieve a decent improved
                   performance. We use multi-step math reasoning as our testbed
                   because it is a very typical emergent ability. We show two
                   important aspects of model abilities: (1). there exists a
                   very complex balance/ tradeoff between language models'
                   multi-dimensional abilities; (2). by paying the price of
                   decreased generic ability, we can clearly lift up the scaling
                   curve of models smaller than 10B towards a specialized
                   multi-step math reasoning ability. We further give
                   comprehensive discussions about important design choices for
                   better generalization, including the tuning data format, the
                   start model checkpoint, and a new model selection method. We
                   hope our practice and discoveries can serve as an important
                   attempt towards specialized smaller models in the new
                   research paradigm set by LLMs.",
  month         =  jan,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2301.12726"
}

@INPROCEEDINGS{Choshen2018-ps,
  title     = "Inherent biases in reference-based evaluation for grammatical
               error correction",
  author    = "Choshen, Leshem and Abend, Omri",
  booktitle = "Proceedings of the 56th Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  abstract  = "It is shown that overcoming LCB in Grammatical Error Correction
               (GEC) evaluation cannot be attained by re-scaling or by
               increasing the number of references in any feasible range,
               contrary to previous suggestions. The prevalent use of too few
               references for evaluating text-to-text generation is known to
               bias estimates of their quality ({\it low coverage bias} or LCB).
               This paper shows that overcoming LCB in Grammatical Error
               Correction (GEC) evaluation cannot be attained by re-scaling or
               by increasing the number of references in any feasible range,
               contrary to previous suggestions. This is due to the long-tailed
               distribution of valid corrections for a sentence. Concretely, we
               show that LCB incentivizes GEC systems to avoid correcting even
               when they can generate a valid correction. Consequently, existing
               systems obtain comparable or superior performance compared to
               humans, by making few but targeted changes to the input. Similar
               effects on Text Simplification further support our claims.",
  year      =  2018,
  doi       = "10.18653/v1/p18-1059",
  language  = "en"
}

@ARTICLE{Eldan2023-vp,
  title         = "{TinyStories}: How small can language models be and still
                   speak coherent English?",
  author        = "Eldan, Ronen and Li, Yuanzhi",
  journal       = "arXiv [cs.CL]",
  abstract      = "Language models (LMs) are powerful tools for natural language
                   processing, but they often struggle to produce coherent and
                   fluent text when they are small. Models with around 125M
                   parameters such as GPT-Neo (small) or GPT-2 (small) can
                   rarely generate coherent and consistent English text beyond a
                   few words even after extensive training. This raises the
                   question of whether the emergence of the ability to produce
                   coherent English text only occurs at larger scales (with
                   hundreds of millions of parameters or more) and complex
                   architectures (with many layers of global attention). In this
                   work, we introduce TinyStories, a synthetic dataset of short
                   stories that only contain words that a typical 3 to
                   4-year-olds usually understand, generated by GPT-3.5 and
                   GPT-4. We show that TinyStories can be used to train and
                   evaluate LMs that are much smaller than the state-of-the-art
                   models (below 10 million total parameters), or have much
                   simpler architectures (with only one transformer block), yet
                   still produce fluent and consistent stories with several
                   paragraphs that are diverse and have almost perfect grammar,
                   and demonstrate reasoning capabilities. We also introduce a
                   new paradigm for the evaluation of language models: We
                   suggest a framework which uses GPT-4 to grade the content
                   generated by these models as if those were stories written by
                   students and graded by a (human) teacher. This new paradigm
                   overcomes the flaws of standard benchmarks which often
                   requires the model's output to be very structures, and
                   moreover provides a multidimensional score for the model,
                   providing scores for different capabilities such as grammar,
                   creativity and consistency. We hope that TinyStories can
                   facilitate the development, analysis and research of LMs,
                   especially for low-resource or specialized domains, and shed
                   light on the emergence of language capabilities in LMs.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.07759"
}

@ARTICLE{Jung2023-yy,
  title         = "Impossible Distillation: From low-quality model to
                   high-quality dataset \& model for summarization and
                   paraphrasing",
  author        = "Jung, Jaehun and West, Peter and Jiang, Liwei and Brahman,
                   Faeze and Lu, Ximing and Fisher, Jillian and Sorensen, Taylor
                   and Choi, Yejin",
  journal       = "arXiv [cs.CL]",
  abstract      = "It is commonly perceived that the strongest language models
                   (LMs) rely on a combination of massive scale, instruction
                   data, and human feedback to perform specialized tasks -- e.g.
                   summarization and paraphrasing, without supervision. In this
                   paper, we propose that language models can learn to summarize
                   and paraphrase sentences, with none of these 3 factors. We
                   present Impossible Distillation, a framework that distills a
                   task-specific dataset directly from an off-the-shelf LM, even
                   when it is impossible for the LM itself to reliably solve the
                   task. By training a student model on the generated dataset
                   and amplifying its capability through self-distillation, our
                   method yields a high-quality model and dataset from a
                   low-quality teacher model, without the need for scale or
                   supervision. Using Impossible Distillation, we are able to
                   distill an order of magnitude smaller model (with only 770M
                   parameters) that outperforms 175B parameter GPT-3, in both
                   quality and controllability, as confirmed by automatic and
                   human evaluations. Furthermore, as a useful byproduct of our
                   approach, we obtain DIMSUM+, a high-quality dataset with 3.4M
                   sentence summaries and paraphrases. Our analyses show that
                   this dataset, as a purely LM-generated corpus, is more
                   diverse and more effective for generalization to unseen
                   domains than all human-authored datasets -- including
                   Gigaword with 4M samples.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.16635"
}

@ARTICLE{DiPaolo2022-wu,
  title     = "What's wrong with epistemic trespassing?",
  author    = "DiPaolo, Joshua",
  journal   = "Philosophical studies",
  publisher = "Springer Science and Business Media LLC",
  volume    =  179,
  number    =  1,
  pages     = "223--243",
  abstract  = "Epistemic trespassers are experts who pass judgment on questions
               in fields where they lack expertise. What's wrong with epistemic
               trespassing? I identify several limitations with a seminal
               analysis to isolate three desiderata on an answer to this
               question and motivate my own answer. An answer (i) should explain
               what's wrong in the cases that motivate inquiry into epistemic
               trespassing, (ii) should explain what's wrong with epistemic
               trespassing even if trespassers do not acknowledge their
               trespassing, and (iii) these explanations should not be
               independent of the fact that epistemic trespassing involves
               expertise. I also independently motivate a fourth desideratum:
               (iv) this account should explain the evaluative difference
               between different kinds of trespassing. To satisfy these
               desiderata, I develop a social analysis: epistemic trespassing is
               wrong because it is an abuse of expert authority that neglects
               novice vulnerabilities.",
  year      =  2022,
  keywords  = "Assertion; Epistemic dependence; Epistemic trespassing;
               Expertise; Higher-order evidence",
  doi       = "10.1007/s11098-021-01657-6",
  pmc       = "PMC8131877",
  pmid      =  34024942,
  issn      = "0031-8116,1573-0883",
  language  = "en"
}

@ARTICLE{McCoy2023-pk,
  title     = "How much do language models copy from their training data?
               Evaluating linguistic novelty in text generation using {RAVEN}",
  author    = "McCoy, R Thomas and Smolensky, Paul and Linzen, Tal and Gao,
               Jianfeng and Celikyilmaz, Asli",
  journal   = "Transactions of the Association for Computational Linguistics",
  publisher = "MIT Press",
  volume    =  11,
  pages     = "652--670",
  abstract  = "Abstract Current language models can generate high-quality text.
               Are they simply copying text they have seen before, or have they
               learned generalizable linguistic abstractions? To tease apart
               these possibilities, we introduce RAVEN, a suite of analyses for
               assessing the novelty of generated text, focusing on sequential
               structure (n-grams) and syntactic structure. We apply these
               analyses to four neural language models trained on English (an
               LSTM, a Transformer, Transformer-XL, and GPT-2). For local
               structure—e.g., individual dependencies—text generated with a
               standard sampling scheme is substantially less novel than our
               baseline of human-generated text from each model’s test set. For
               larger-scale structure—e.g., overall sentence
               structure—model-generated text is as novel or even more novel
               than the human-generated baseline, but models still sometimes
               copy substantially, in some cases duplicating passages over 1,000
               words long from the training set. We also perform extensive
               manual analysis, finding evidence that GPT-2 uses both
               compositional and analogical generalization mechanisms and
               showing that GPT-2’s novel text is usually well-formed
               morphologically and syntactically but has reasonably frequent
               semantic issues (e.g., being self-contradictory).",
  month     =  jun,
  year      =  2023,
  doi       = "10.1162/tacl\_a\_00567",
  issn      = "2307-387X",
  language  = "en"
}

@ARTICLE{Kirk2023-zc,
  title         = "Understanding the effects of {RLHF} on {LLM} generalisation
                   and diversity",
  author        = "Kirk, Robert and Mediratta, Ishita and Nalmpantis,
                   Christoforos and Luketina, Jelena and Hambro, Eric and
                   Grefenstette, Edward and Raileanu, Roberta",
  journal       = "arXiv [cs.LG]",
  abstract      = "Large language models (LLMs) fine-tuned with reinforcement
                   learning from human feedback (RLHF) have been used in some of
                   the most widely deployed AI models to date, such as OpenAI's
                   ChatGPT, Anthropic's Claude, or Meta's LLaMA-2. While there
                   has been significant work developing these methods, our
                   understanding of the benefits and downsides of each stage in
                   RLHF is still limited. To fill this gap, we present an
                   extensive analysis of how each stage of the process (i.e.
                   supervised fine-tuning (SFT), reward modelling, and RLHF)
                   affects two key properties: out-of-distribution (OOD)
                   generalisation and output diversity. OOD generalisation is
                   crucial given the wide range of real-world scenarios in which
                   these models are being used, while output diversity refers to
                   the model's ability to generate varied outputs and is
                   important for a variety of use cases. We perform our analysis
                   across two base models on both summarisation and instruction
                   following tasks, the latter being highly relevant for current
                   LLM use cases. We find that RLHF generalises better than SFT
                   to new inputs, particularly as the distribution shift between
                   train and test becomes larger. However, RLHF significantly
                   reduces output diversity compared to SFT across a variety of
                   measures, implying a tradeoff in current LLM fine-tuning
                   methods between generalisation and diversity. Our results
                   provide guidance on which fine-tuning method should be used
                   depending on the application, and show that more research is
                   needed to improve the trade-off between generalisation and
                   diversity.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2310.06452"
}
