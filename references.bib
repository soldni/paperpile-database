@ARTICLE{Rosa2022-sv,
  title     = "No parameter left behind: How distillation and model size affect
               zero-shot retrieval",
  author    = "Rosa, Guilherme Moraes and Bonifacio, Luiz and Jeronymo, Vitor
               and Abonizio, Hugo and Fadaee, Marzieh and Lotufo, Roberto and
               Nogueira, Rodrigo",
  journal   = "arXiv.org",
  publisher = "arXiv",
  abstract  = "Recent work has shown that small distilled language models are
               strong competitors to models that are orders of magnitude larger
               and slower in a wide range of information retrieval tasks. This
               has made distilled and dense models, due to latency constraints,
               the go-to choice for deployment in real-world retrieval
               applications. In this work, we question this practice by showing
               that the number of parameters and early query-document
               interaction play a significant role in the generalization ability
               of retrieval models. Our experiments show that increasing model
               size results in marginal gains on in-domain test sets, but much
               larger gains in new domains never seen during fine-tuning.
               Furthermore, we show that rerankers largely outperform dense ones
               of similar size in several tasks. Our largest reranker reaches
               the state of the art in 12 of the 18 datasets of the Benchmark-IR
               (BEIR) and surpasses the previous state of the art by 3 average
               points. Finally, we confirm that in-domain effectiveness is not a
               good indicator of zero-shot effectiveness. Code is available at
               https://github.com/guilhermemr04/scaling-zero-shot-retrieval.git",
  year      =  2022,
  eprint    = "2206.02873",
  doi       = "10.48550/ARXIV.2206.02873",
  language  = "en"
}

@INPROCEEDINGS{Kandpal2023-rv,
  title     = "Large language models struggle to learn long-tail knowledge",
  author    = "Kandpal, Nikhil and Deng, Haikang and Roberts, Adam and Wallace,
               Eric and Raffel, Colin",
  editor    = "Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and
               Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan",
  booktitle = "Proceedings of the 40th International Conference on Machine
               Learning",
  publisher = "PMLR",
  volume    =  202,
  pages     = "15696--15707",
  abstract  = "The Internet contains a wealth of knowledge—from the birthdays of
               historical figures to tutorials on how to code—all of which may
               be learned by language models. However, while certain pieces of
               information are ubiquitous on the web, others appear extremely
               rarely. In this paper, we study the relationship between the
               knowledge memorized by large language models and the information
               in pre-training datasets scraped from the web. In particular, we
               show that a language model’s ability to answer a fact-based
               question relates to how many documents associated with that
               question were seen during pre-training. We identify these
               relevant documents by entity linking pre-training datasets and
               counting documents that contain the same entities as a given
               question-answer pair. Our results demonstrate strong
               correlational and causal relationships between accuracy and
               relevant document count for numerous question answering datasets
               (e.g., TriviaQA), pre-training corpora (e.g., ROOTS), and model
               sizes (e.g., 176B parameters). Moreover, while larger models are
               better at learning long-tail knowledge, we estimate that today’s
               models must be scaled by many orders of magnitude to reach
               competitive QA performance on questions with little support in
               the pre-training data. Finally, we show that
               retrieval-augmentation can reduce the dependence on relevant
               pre-training information, presenting a promising approach for
               capturing the long-tail.",
  series    = "Proceedings of Machine Learning Research",
  year      =  2023
}

@ARTICLE{MacAvaney2022-tn,
  title     = "{ABNIRML}: Analyzing the Behavior of neural {IR} models",
  author    = "MacAvaney, Sean and Feldman, Sergey and Goharian, Nazli and
               Downey, Doug and Cohan, Arman",
  journal   = "Transactions of the Association for Computational Linguistics",
  publisher = "MIT Press - Journals",
  volume    =  10,
  pages     = "224--239",
  abstract  = "Abstract Pretrained contextualized language models such as BERT
               and T5 have established a new state-of-the-art for ad-hoc search.
               However, it is not yet well understood why these methods are so
               effective, what makes some variants more effective than others,
               and what pitfalls they may have. We present a new comprehensive
               framework for Analyzing the Behavior of Neural IR ModeLs
               (ABNIRML), which includes new types of diagnostic probes that
               allow us to test several characteristics—such as writing styles,
               factuality, sensitivity to paraphrasing and word order—that are
               not addressed by previous techniques. To demonstrate the value of
               the framework, we conduct an extensive empirical study that
               yields insights into the factors that contribute to the neural
               model’s gains, and identify potential unintended biases the
               models exhibit. Some of our results confirm conventional wisdom,
               for example, that recent neural ranking models rely less on exact
               term overlap with the query, and instead leverage richer
               linguistic information, evidenced by their higher sensitivity to
               word and sentence order. Other results are more surprising, such
               as that some models (e.g., T5 and ColBERT) are biased towards
               factually correct (rather than simply relevant) texts. Further,
               some characteristics vary even for the same base language model,
               and other characteristics can appear due to random variations
               during model training.1",
  month     =  mar,
  year      =  2022,
  doi       = "10.1162/tacl\_a\_00457",
  issn      = "2307-387X",
  language  = "en"
}

@ARTICLE{Lo2023-td,
  title         = "The Semantic Reader Project: Augmenting Scholarly Documents
                   through {AI}-Powered Interactive Reading Interfaces",
  author        = "Lo, Kyle and Chang, Joseph Chee and Head, Andrew and Bragg,
                   Jonathan and Zhang, Amy X and Trier, Cassidy and
                   Anastasiades, Chloe and August, Tal and Authur, Russell and
                   Bragg, Danielle and Bransom, Erin and Cachola, Isabel and
                   Candra, Stefan and Chandrasekhar, Yoganand and Chen, Yen-Sung
                   and Cheng, Evie Yu-Yen and Chou, Yvonne and Downey, Doug and
                   Evans, Rob and Fok, Raymond and Hu, Fangzhou and Huff, Regan
                   and Kang, Dongyeop and Kim, Tae Soo and Kinney, Rodney and
                   Kittur, Aniket and Kang, Hyeonsu and Klevak, Egor and Kuehl,
                   Bailey and Langan, Michael and Latzke, Matt and Lochner,
                   Jaron and MacMillan, Kelsey and Marsh, Eric and Murray, Tyler
                   and Naik, Aakanksha and Nguyen, Ngoc-Uyen and Palani, Srishti
                   and Park, Soya and Paulic, Caroline and Rachatasumrit, Napol
                   and Rao, Smita and Sayre, Paul and Shen, Zejiang and
                   Siangliulue, Pao and Soldaini, Luca and Tran, Huy and van
                   Zuylen, Madeleine and Wang, Lucy Lu and Wilhelm, Christopher
                   and Wu, Caroline and Yang, Jiangjiang and Zamarron, Angele
                   and Hearst, Marti A and Weld, Daniel S",
  abstract      = "Scholarly publications are key to the transfer of knowledge
                   from scholars to others. However, research papers are
                   information-dense, and as the volume of the scientific
                   literature grows, the need for new technology to support the
                   reading process grows. In contrast to the process of finding
                   papers, which has been transformed by Internet technology,
                   the experience of reading research papers has changed little
                   in decades. The PDF format for sharing research papers is
                   widely used due to its portability, but it has significant
                   downsides including: static content, poor accessibility for
                   low-vision readers, and difficulty reading on mobile devices.
                   This paper explores the question ``Can recent advances in AI
                   and HCI power intelligent, interactive, and accessible
                   reading interfaces -- even for legacy PDFs?'' We describe the
                   Semantic Reader Project, a collaborative effort across
                   multiple institutions to explore automatic creation of
                   dynamic reading interfaces for research papers. Through this
                   project, we've developed ten research prototype interfaces
                   and conducted usability studies with more than 300
                   participants and real-world users showing improved reading
                   experiences for scholars. We've also released a production
                   reading interface for research papers that will incorporate
                   the best features as they mature. We structure this paper
                   around challenges scholars and the public face when reading
                   research papers -- Discovery, Efficiency, Comprehension,
                   Synthesis, and Accessibility -- and present an overview of
                   our progress and remaining open challenges.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC",
  eprint        = "2303.14334"
}

@INPROCEEDINGS{MacAvaney2021-ex,
  title     = "Simplified Data Wrangling with ir\_datasets",
  author    = "MacAvaney, Sean and Yates, Andrew and Feldman, Sergey and Downey,
               Doug and Cohan, Arman and Goharian, Nazli",
  booktitle = "Proceedings of the 44th International ACM SIGIR Conference on
               Research and Development in Information Retrieval",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "2429--2436",
  abstract  = "Managing the data for Information Retrieval (IR) experiments can
               be challenging. Dataset documentation is scattered across the
               Internet and once one obtains a copy of the data, there are
               numerous different data formats to work with. Even basic formats
               can have subtle dataset-specific nuances that need to be
               considered for proper use. To help mitigate these challenges, we
               introduce a new robust and lightweight tool (ir\_datasets) for
               acquiring, managing, and performing typical operations over
               datasets used in IR. We primarily focus on textual datasets used
               for ad-hoc search. This tool provides both a Python and command
               line interface to numerous IR datasets and benchmarks. To our
               knowledge, this is the most extensive tool of its kind.
               Integrations with popular IR indexing and experimentation
               toolkits demonstrate the tool's utility. We also provide
               documentation of these datasets through the \sys catalog:
               https://ir-datasets.com/. The catalog acts as a hub for
               information on datasets used in IR, providing core information
               about what data each benchmark provides as well as links to more
               detailed information. We welcome community contributions and
               intend to continue to maintain and grow this tool.",
  series    = "SIGIR '21",
  month     =  jul,
  year      =  2021,
  keywords  = "information retrieval, benchmarks, datasets",
  doi       = "10.1145/3404835.3463254",
  isbn      =  9781450380379
}

@ARTICLE{MacAvaney2023-da,
  title         = "One-Shot Labeling for Automatic Relevance Estimation",
  author        = "MacAvaney, Sean and Soldaini, Luca",
  abstract      = "Dealing with unjudged documents (``holes'') in relevance
                   assessments is a perennial problem when evaluating search
                   systems with offline experiments. Holes can reduce the
                   apparent effectiveness of retrieval systems during evaluation
                   and introduce biases in models trained with incomplete data.
                   In this work, we explore whether large language models can
                   help us fill such holes to improve offline evaluations. We
                   examine an extreme, albeit common, evaluation setting wherein
                   only a single known relevant document per query is available
                   for evaluation. We then explore various approaches for
                   predicting the relevance of unjudged documents with respect
                   to a query and the known relevant document, including nearest
                   neighbor, supervised, and prompting techniques. We find that
                   although the predictions of these One-Shot Labelers (1SL)
                   frequently disagree with human assessments, the labels they
                   produce yield a far more reliable ranking of systems than the
                   single labels do alone. Specifically, the strongest
                   approaches can consistently reach system ranking correlations
                   of over 0.86 with the full rankings over a variety of
                   measures. Meanwhile, the approach substantially increases the
                   reliability of t-tests due to filling holes in relevance
                   assessments, giving researchers more confidence in results
                   they find to be significant. Alongside this work, we release
                   an easy-to-use software package to enable the use of 1SL for
                   evaluation of other ad-hoc collections or systems.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "2302.11266"
}

@INPROCEEDINGS{Nguyen2023-ty,
  title     = "A Unified Framework for Learned Sparse Retrieval",
  author    = "Nguyen, Thong and MacAvaney, Sean and Yates, Andrew",
  booktitle = "Advances in Information Retrieval",
  publisher = "Springer Nature Switzerland",
  pages     = "101--116",
  abstract  = "Learned sparse retrieval (LSR) is a family of first-stage
               retrieval methods that are trained to generate sparse lexical
               representations of queries and documents for use with an inverted
               index. Many LSR methods have been recently introduced, with
               Splade models achieving state-of-the-art performance on MSMarco.
               Despite similarities in their model architectures, many LSR
               methods show substantial differences in effectiveness and
               efficiency. Differences in the experimental setups and
               configurations used make it difficult to compare the methods and
               derive insights. In this work, we analyze existing LSR methods
               and identify key components to establish an LSR framework that
               unifies all LSR methods under the same perspective. We then
               reproduce all prominent methods using a common codebase and
               re-train them in the same environment, which allows us to
               quantify how components of the framework affect effectiveness and
               efficiency. We find that (1) including document term weighting is
               most important for a method’s effectiveness, (2) including query
               weighting has a small positive impact, and (3) document expansion
               and query expansion have a cancellation effect. As a result, we
               show how removing query expansion from a state-of-the-art model
               can reduce latency significantly while maintaining effectiveness
               on MSMarco and TripClick benchmarks. Our code is publicly
               available (Code:
               https://github.com/thongnt99/learned-sparse-retrieval).",
  year      =  2023,
  doi       = "10.1007/978-3-031-28241-6\_7"
}

@ARTICLE{Nguyen2023-yh,
  title         = "Adapting Learned Sparse Retrieval for Long Documents",
  author        = "Nguyen, Thong and MacAvaney, Sean and Yates, Andrew",
  abstract      = "Learned sparse retrieval (LSR) is a family of neural
                   retrieval methods that transform queries and documents into
                   sparse weight vectors aligned with a vocabulary. While LSR
                   approaches like Splade work well for short passages, it is
                   unclear how well they handle longer documents. We investigate
                   existing aggregation approaches for adapting LSR to longer
                   documents and find that proximal scoring is crucial for LSR
                   to handle long documents. To leverage this property, we
                   proposed two adaptations of the Sequential Dependence Model
                   (SDM) to LSR: ExactSDM and SoftSDM. ExactSDM assumes only
                   exact query term dependence, while SoftSDM uses potential
                   functions that model the dependence of query terms and their
                   expansion terms (i.e., terms identified using a transformer's
                   masked language modeling head). Experiments on the MSMARCO
                   Document and TREC Robust04 datasets demonstrate that both
                   ExactSDM and SoftSDM outperform existing LSR aggregation
                   approaches for different document length constraints.
                   Surprisingly, SoftSDM does not provide any performance
                   benefits over ExactSDM. This suggests that soft proximity
                   matching is not necessary for modeling term dependence in
                   LSR. Overall, this study provides insights into handling long
                   documents with LSR, proposing adaptations that improve its
                   performance.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "2305.18494"
}

@INPROCEEDINGS{Gospodinov2023-sn,
  title     = "{Doc2Query–}: When Less is More",
  author    = "Gospodinov, Mitko and MacAvaney, Sean and Macdonald, Craig",
  booktitle = "Advances in Information Retrieval",
  publisher = "Springer Nature Switzerland",
  pages     = "414--422",
  abstract  = "Doc2Query—the process of expanding the content of a document
               before indexing using a sequence-to-sequence model—has emerged as
               a prominent technique for improving the first-stage retrieval
               effectiveness of search engines. However, sequence-to-sequence
               models are known to be prone to “hallucinating” content that is
               not present in the source text. We argue that Doc2Query is indeed
               prone to hallucination, which ultimately harms retrieval
               effectiveness and inflates the index size. In this work, we
               explore techniques for filtering out these harmful queries prior
               to indexing. We find that using a relevance model to remove
               poor-quality queries can improve the retrieval effectiveness of
               Doc2Query by up to 16\%, while simultaneously reducing mean query
               execution time by 30\% and cutting the index size by 48\%. We
               release the code, data, and a live demonstration to facilitate
               reproduction and further exploration
               (https://github.com/terrierteam/pyterrier\_doc2query).",
  year      =  2023,
  doi       = "10.1007/978-3-031-28238-6\_31"
}

@INPROCEEDINGS{MacAvaney2023-be,
  title     = "One-Shot Labeling for Automatic Relevance Estimation",
  author    = "MacAvaney, Sean and Soldaini, Luca",
  booktitle = "Proceedings of the 46th International ACM SIGIR Conference on
               Research and Development in Information Retrieval",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "2230--2235",
  abstract  = "Dealing with unjudged documents (``holes'') in relevance
               assessments is a perennial problem when evaluating search systems
               with offline experiments. Holes can reduce the apparent
               effectiveness of retrieval systems during evaluation and
               introduce biases in models trained with incomplete data. In this
               work, we explore whether large language models can help us fill
               such holes to improve offline evaluations. We examine an extreme,
               albeit common, evaluation setting wherein only a single known
               relevant document per query is available for evaluation. We then
               explore various approaches for predicting the relevance of
               unjudged documents with respect to a query and the known relevant
               document, including nearest neighbor, supervised, and prompting
               techniques. We find that although the predictions of these
               One-Shot Labelers (1SL) frequently disagree with human
               assessments, the labels they produce yield a far more reliable
               ranking of systems than the single labels do alone. Specifically,
               the strongest approaches can consistently reach system ranking
               correlations of over 0.86 with the full rankings over a variety
               of measures. Meanwhile, the approach substantially increases the
               reliability of t-tests due to filling holes in relevance
               assessments, giving researchers more confidence in results they
               find to be significant. Alongside this work, we release an
               easy-to-use software package to enable the use of 1SL for
               evaluation of other ad-hoc collections or systems.",
  series    = "SIGIR '23",
  month     =  jul,
  year      =  2023,
  keywords  = "few-shot learning, relevance assessments, neural networks",
  doi       = "10.1145/3539618.3592032",
  isbn      =  9781450394086
}

@INPROCEEDINGS{Nguyen2023-hw,
  title     = "Adapting Learned Sparse Retrieval for Long Documents",
  author    = "Nguyen, Thong and MacAvaney, Sean and Yates, Andrew",
  booktitle = "Proceedings of the 46th International ACM SIGIR Conference on
               Research and Development in Information Retrieval",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1781--1785",
  abstract  = "Learned sparse retrieval (LSR) is a family of neural retrieval
               methods that transform queries and documents into sparse weight
               vectors aligned with a vocabulary. While LSR approaches like
               Splade work well for short passages, it is unclear how well they
               handle longer documents. We investigate existing aggregation
               approaches for adapting LSR to longer documents and find that
               proximal scoring is crucial for LSR to handle long documents. To
               leverage this property, we proposed two adaptations of the
               Sequential Dependence Model (SDM) to LSR: ExactSDM and SoftSDM.
               ExactSDM assumes only exact query term dependence, while SoftSDM
               uses potential functions that model the dependence of query terms
               and their expansion terms (i.e., terms identified using a
               transformer's masked language modeling head).Experiments on the
               MSMARCO Document and TREC Robust04 datasets demonstrate that both
               ExactSDM and SoftSDM outperform existing LSR aggregation
               approaches for different document length constraints.
               Surprisingly, SoftSDM does not provide any performance benefits
               over ExactSDM. This suggests that soft proximity matching is not
               necessary for modeling term dependence in LSR. Overall, this
               study provides insights into handling long documents with LSR,
               proposing adaptations that improve its performance.",
  series    = "SIGIR '23",
  month     =  jul,
  year      =  2023,
  keywords  = "learned sparse retrieval, long documents, term proximity",
  doi       = "10.1145/3539618.3591943",
  isbn      =  9781450394086
}

@ARTICLE{Li2023-ix,
  title         = "{FLM}-{101B}: An open {LLM} and how to train it with \${100K}
                   budget",
  author        = "Li, Xiang and Yao, Yiqun and Jiang, Xin and Fang, Xuezhi and
                   Meng, Xuying and Fan, Siqi and Han, Peng and Li, Jing and Du,
                   Li and Qin, Bowen and Zhang, Zheng and Sun, Aixin and Wang,
                   Yequan",
  abstract      = "Large language models (LLMs) have achieved remarkable success
                   in NLP and multimodal tasks. Despite these successes, their
                   development faces two main challenges: (i) high computational
                   cost; and (ii) difficulty in conducting fair and objective
                   evaluations. LLMs are prohibitively expensive, making it
                   feasible for only a few major players to undertake their
                   training, thereby constraining both research and application
                   opportunities. This underscores the importance of
                   cost-effective LLM training. In this paper, we utilize a
                   growth strategy to significantly reduce LLM training cost. We
                   demonstrate that an LLM with 101B parameters and 0.31TB
                   tokens can be trained on a $100K budget. We also adopt a
                   systematic evaluation paradigm for the IQ evaluation of LLMs,
                   in complement to existing evaluations that focus more on
                   knowledge-oriented abilities. We introduce our benchmark
                   including evaluations on important aspects of intelligence
                   including symbolic mapping, itrule understanding, pattern
                   mining, and anti-interference. Such evaluations minimize the
                   potential impact of memorization. Experimental results show
                   that our model FLM-101B, trained with a budget of $100K,
                   achieves comparable performance to powerful and well-known
                   models, eg GPT-3 and GLM-130B, especially in the IQ benchmark
                   evaluations with contexts unseen in training data. The
                   checkpoint of FLM-101B will be open-sourced at
                   https://huggingface.co/CofeAI/FLM-101B.",
  month         =  sep,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2309.03852"
}

@ARTICLE{Zhu2023-zl,
  title     = "Large language models for information retrieval: A survey",
  author    = "Zhu, Yutao and Yuan, Huaying and Wang, Shuting and Liu, Jiongnan
               and Liu, Wenhan and Deng, Chenlong and Dou, Zhicheng and Wen,
               Ji-Rong",
  journal   = "arXiv.org",
  publisher = "arXiv",
  abstract  = "As a primary means of information acquisition, information
               retrieval (IR) systems, such as search engines, have integrated
               themselves into our daily lives. These systems also serve as
               components of dialogue, question-answering, and recommender
               systems. The trajectory of IR has evolved dynamically from its
               origins in term-based methods to its integration with advanced
               neural models. While the neural models excel at capturing complex
               contextual signals and semantic nuances, thereby reshaping the IR
               landscape, they still face challenges such as data scarcity,
               interpretability, and the generation of contextually plausible
               yet potentially inaccurate responses. This evolution requires a
               combination of both traditional methods (such as term-based
               sparse retrieval methods with rapid response) and modern neural
               architectures (such as language models with powerful language
               understanding capacity). Meanwhile, the emergence of large
               language models (LLMs), typified by ChatGPT and GPT-4, has
               revolutionized natural language processing due to their
               remarkable language understanding, generation, generalization,
               and reasoning abilities. Consequently, recent research has sought
               to leverage LLMs to improve IR systems. Given the rapid evolution
               of this research trajectory, it is necessary to consolidate
               existing methodologies and provide nuanced insights through a
               comprehensive overview. In this survey, we delve into the
               confluence of LLMs and IR systems, including crucial aspects such
               as query rewriters, retrievers, rerankers, and readers.
               Additionally, we explore promising directions within this
               expanding field.",
  year      =  2023,
  eprint    = "2308.07107",
  doi       = "10.48550/ARXIV.2308.07107",
  language  = "en"
}

@ARTICLE{Izacard2020-zn,
  title         = "Leveraging Passage Retrieval with Generative Models for Open
                   Domain Question Answering",
  author        = "Izacard, Gautier and Grave, Edouard",
  abstract      = "Generative models for open domain question answering have
                   proven to be competitive, without resorting to external
                   knowledge. While promising, this approach requires to use
                   models with billions of parameters, which are expensive to
                   train and query. In this paper, we investigate how much these
                   models can benefit from retrieving text passages, potentially
                   containing evidence. We obtain state-of-the-art results on
                   the Natural Questions and TriviaQA open benchmarks.
                   Interestingly, we observe that the performance of this method
                   significantly improves when increasing the number of
                   retrieved passages. This is evidence that generative models
                   are good at aggregating and combining evidence from multiple
                   passages.",
  month         =  jul,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2007.01282"
}

@MISC{Dror2019-ko,
  title   = "Deep Dominance - How to Properly Compare Deep Neural Models",
  author  = "Dror, Rotem and Shlomov, Segev and Reichart, Roi",
  journal = "Proceedings of the 57th Annual Meeting of the Association for
             Computational Linguistics",
  year    =  2019,
  doi     = "10.18653/v1/p19-1266"
}

@ARTICLE{Haddad2019-lc,
  title         = "Learning More From Less: Towards Strengthening Weak
                   Supervision for Ad-Hoc Retrieval",
  author        = "Haddad, Dany and Ghosh, Joydeep",
  abstract      = "The limited availability of ground truth relevance labels has
                   been a major impediment to the application of supervised
                   methods to ad-hoc retrieval. As a result, unsupervised
                   scoring methods, such as BM25, remain strong competitors to
                   deep learning techniques which have brought on dramatic
                   improvements in other domains, such as computer vision and
                   natural language processing. Recent works have shown that it
                   is possible to take advantage of the performance of these
                   unsupervised methods to generate training data for
                   learning-to-rank models. The key limitation to this line of
                   work is the size of the training set required to surpass the
                   performance of the original unsupervised method, which can be
                   as large as $10^{13}$ training examples. Building on these
                   insights, we propose two methods to reduce the amount of
                   training data required. The first method takes inspiration
                   from crowdsourcing, and leverages multiple unsupervised
                   rankers to generate soft, or noise-aware, training labels.
                   The second identifies harmful, or mislabeled, training
                   examples and removes them from the training set. We show that
                   our methods allow us to surpass the performance of the
                   unsupervised baseline with far fewer training examples than
                   previous works.",
  month         =  jul,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "1907.08657",
  doi           = "10.1145/3331184.3331272"
}

@ARTICLE{Chen2020-vj,
  title         = "{AdaBERT}: Task-Adaptive {BERT} Compression with
                   Differentiable Neural Architecture Search",
  author        = "Chen, Daoyuan and Li, Yaliang and Qiu, Minghui and Wang, Zhen
                   and Li, Bofang and Ding, Bolin and Deng, Hongbo and Huang,
                   Jun and Lin, Wei and Zhou, Jingren",
  abstract      = "Large pre-trained language models such as BERT have shown
                   their effectiveness in various natural language processing
                   tasks. However, the huge parameter size makes them difficult
                   to be deployed in real-time applications that require quick
                   inference with limited resources. Existing methods compress
                   BERT into small models while such compression is
                   task-independent, i.e., the same compressed BERT for all
                   different downstream tasks. Motivated by the necessity and
                   benefits of task-oriented BERT compression, we propose a
                   novel compression method, AdaBERT, that leverages
                   differentiable Neural Architecture Search to automatically
                   compress BERT into task-adaptive small models for specific
                   tasks. We incorporate a task-oriented knowledge distillation
                   loss to provide search hints and an efficiency-aware loss as
                   search constraints, which enables a good trade-off between
                   efficiency and effectiveness for task-adaptive BERT
                   compression. We evaluate AdaBERT on several NLP tasks, and
                   the results demonstrate that those task-adaptive compressed
                   models are 12.7x to 29.3x faster than BERT in inference time
                   and 11.5x to 17.0x smaller in terms of parameter size, while
                   comparable performance is maintained.",
  month         =  jan,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2001.04246"
}

@ARTICLE{Conneau2019-ft,
  title         = "Unsupervised Cross-lingual Representation Learning at Scale",
  author        = "Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and
                   Chaudhary, Vishrav and Wenzek, Guillaume and Guzmán,
                   Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer,
                   Luke and Stoyanov, Veselin",
  abstract      = "This paper shows that pretraining multilingual language
                   models at scale leads to significant performance gains for a
                   wide range of cross-lingual transfer tasks. We train a
                   Transformer-based masked language model on one hundred
                   languages, using more than two terabytes of filtered
                   CommonCrawl data. Our model, dubbed XLM-R, significantly
                   outperforms multilingual BERT (mBERT) on a variety of
                   cross-lingual benchmarks, including +13.8\% average accuracy
                   on XNLI, +12.3\% average F1 score on MLQA, and +2.1\% average
                   F1 score on NER. XLM-R performs particularly well on
                   low-resource languages, improving 11.8\% in XNLI accuracy for
                   Swahili and 9.2\% for Urdu over the previous XLM model. We
                   also present a detailed empirical evaluation of the key
                   factors that are required to achieve these gains, including
                   the trade-offs between (1) positive transfer and capacity
                   dilution and (2) the performance of high and low resource
                   languages at scale. Finally, we show, for the first time, the
                   possibility of multilingual modeling without sacrificing
                   per-language performance; XLM-Ris very competitive with
                   strong monolingual models on the GLUE and XNLI benchmarks. We
                   will make XLM-R code, data, and models publicly available.",
  month         =  nov,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1911.02116"
}

@ARTICLE{Jiao2019-gl,
  title         = "{TinyBERT}: Distilling {BERT} for Natural Language
                   Understanding",
  author        = "Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin
                   and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun",
  abstract      = "Language model pre-training, such as BERT, has significantly
                   improved the performances of many natural language processing
                   tasks. However, pre-trained language models are usually
                   computationally expensive and memory intensive, so it is
                   difficult to effectively execute them on some
                   resource-restricted devices. To accelerate inference and
                   reduce model size while maintaining accuracy, we firstly
                   propose a novel transformer distillation method that is a
                   specially designed knowledge distillation (KD) method for
                   transformer-based models. By leveraging this new KD method,
                   the plenty of knowledge encoded in a large teacher BERT can
                   be well transferred to a small student TinyBERT. Moreover, we
                   introduce a new two-stage learning framework for TinyBERT,
                   which performs transformer distillation at both the
                   pre-training and task-specific learning stages. This
                   framework ensures that TinyBERT can capture both the
                   general-domain and task-specific knowledge of the teacher
                   BERT. TinyBERT is empirically effective and achieves
                   comparable results with BERT in GLUE datasets, while being
                   7.5x smaller and 9.4x faster on inference. TinyBERT is also
                   significantly better than state-of-the-art baselines, even
                   with only about 28\% parameters and 31\% inference time of
                   baselines.",
  month         =  sep,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1909.10351"
}

@ARTICLE{Lan2019-on,
  title         = "{ALBERT}: A Lite {BERT} for Self-supervised Learning of
                   Language Representations",
  author        = "Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and
                   Gimpel, Kevin and Sharma, Piyush and Soricut, Radu",
  abstract      = "Increasing model size when pretraining natural language
                   representations often results in improved performance on
                   downstream tasks. However, at some point further model
                   increases become harder due to GPU/TPU memory limitations,
                   longer training times, and unexpected model degradation. To
                   address these problems, we present two parameter-reduction
                   techniques to lower memory consumption and increase the
                   training speed of BERT. Comprehensive empirical evidence
                   shows that our proposed methods lead to models that scale
                   much better compared to the original BERT. We also use a
                   self-supervised loss that focuses on modeling inter-sentence
                   coherence, and show it consistently helps downstream tasks
                   with multi-sentence inputs. As a result, our best model
                   establishes new state-of-the-art results on the GLUE, RACE,
                   and SQuAD benchmarks while having fewer parameters compared
                   to BERT-large.The code and the pretrained models are
                   available at
                   https://github.com/google-research/google-research/tree/master/albert.",
  month         =  sep,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1909.11942"
}

@ARTICLE{Li2019-rs,
  title         = "Dice Loss for Data-imbalanced {NLP} Tasks",
  author        = "Li, Xiaoya and Sun, Xiaofei and Meng, Yuxian and Liang,
                   Junjun and Wu, Fei and Li, Jiwei",
  abstract      = "Many NLP tasks such as tagging and machine reading
                   comprehension are faced with the severe data imbalance issue:
                   negative examples significantly outnumber positive examples,
                   and the huge number of background examples (or easy-negative
                   examples) overwhelms the training. The most commonly used
                   cross entropy (CE) criteria is actually an accuracy-oriented
                   objective, and thus creates a discrepancy between training
                   and test: at training time, each training instance
                   contributes equally to the objective function, while at test
                   time F1 score concerns more about positive examples. In this
                   paper, we propose to use dice loss in replacement of the
                   standard cross-entropy objective for data-imbalanced NLP
                   tasks. Dice loss is based on the Sorensen-Dice coefficient or
                   Tversky index, which attaches similar importance to false
                   positives and false negatives, and is more immune to the
                   data-imbalance issue. To further alleviate the dominating
                   influence from easy-negative examples in training, we propose
                   to associate training examples with dynamically adjusted
                   weights to deemphasize easy-negative examples.Theoretical
                   analysis shows that this strategy narrows down the gap
                   between the F1 score in evaluation and the dice loss in
                   training. With the proposed training objective, we observe
                   significant performance boost on a wide range of data
                   imbalanced NLP tasks. Notably, we are able to achieve SOTA
                   results on CTB5, CTB6 and UD1.4 for the part of speech
                   tagging task; SOTA results on CoNLL03, OntoNotes5.0, MSRA and
                   OntoNotes4.0 for the named entity recognition task; along
                   with competitive results on the tasks of machine reading
                   comprehension and paraphrase identification.",
  month         =  nov,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1911.02855"
}

@ARTICLE{Sil2017-ii,
  title         = "One for All: Towards Language Independent Named Entity
                   Linking",
  author        = "Sil, Avirup and Florian, Radu",
  abstract      = "Entity linking (EL) is the task of disambiguating mentions in
                   text by associating them with entries in a predefined
                   database of mentions (persons, organizations, etc). Most
                   previous EL research has focused mainly on one language,
                   English, with less attention being paid to other languages,
                   such as Spanish or Chinese. In this paper, we introduce LIEL,
                   a Language Independent Entity Linking system, which provides
                   an EL framework which, once trained on one language, works
                   remarkably well on a number of different languages without
                   change. LIEL makes a joint global prediction over the entire
                   document, employing a discriminative reranking framework with
                   many domain and language-independent feature functions.
                   Experiments on numerous benchmark datasets, show that the
                   proposed system, once trained on one language, English,
                   outperforms several state-of-the-art systems in English (by 4
                   points) and the trained model also works very well on Spanish
                   (14 points better than a competitor system), demonstrating
                   the viability of the approach.",
  month         =  dec,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1712.01797"
}

@ARTICLE{Chauhan2019-ko,
  title         = "{REflex}: Flexible Framework for Relation Extraction in
                   Multiple Domains",
  author        = "Chauhan, Geeticka and McDermott, Matthew B A and Szolovits,
                   Peter",
  abstract      = "Systematic comparison of methods for relation extraction (RE)
                   is difficult because many experiments in the field are not
                   described precisely enough to be completely reproducible and
                   many papers fail to report ablation studies that would
                   highlight the relative contributions of their various
                   combined techniques. In this work, we build a unifying
                   framework for RE, applying this on three highly used datasets
                   (from the general, biomedical and clinical domains) with the
                   ability to be extendable to new datasets. By performing a
                   systematic exploration of modeling, pre-processing and
                   training methodologies, we find that choices of
                   pre-processing are a large contributor performance and that
                   omission of such information can further hinder fair
                   comparison. Other insights from our exploration allow us to
                   provide recommendations for future research in this area.",
  month         =  jun,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1906.08318"
}

@ARTICLE{Collobert2019-zt,
  title         = "A Fully Differentiable Beam Search Decoder",
  author        = "Collobert, Ronan and Hannun, Awni and Synnaeve, Gabriel",
  abstract      = "We introduce a new beam search decoder that is fully
                   differentiable, making it possible to optimize at training
                   time through the inference procedure. Our decoder allows us
                   to combine models which operate at different granularities
                   (e.g. acoustic and language models). It can be used when
                   target sequences are not aligned to input sequences by
                   considering all possible alignments between the two. We
                   demonstrate our approach scales by applying it to speech
                   recognition, jointly training acoustic and word-level
                   language models. The system is end-to-end, with gradients
                   flowing through the whole architecture from the word-level
                   transcriptions. Recent research efforts have shown that deep
                   neural networks with attention-based mechanisms are powerful
                   enough to successfully train an acoustic model from the final
                   transcription, while implicitly learning a language model.
                   Instead, we show that it is possible to discriminatively
                   train an acoustic model jointly with an explicit and possibly
                   pre-trained language model.",
  month         =  feb,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1902.06022"
}

@ARTICLE{Talmor2018-tb,
  title         = "{CommonsenseQA}: A Question Answering Challenge Targeting
                   Commonsense Knowledge",
  author        = "Talmor, Alon and Herzig, Jonathan and Lourie, Nicholas and
                   Berant, Jonathan",
  abstract      = "When answering a question, people often draw upon their rich
                   world knowledge in addition to the particular context. Recent
                   work has focused primarily on answering questions given some
                   relevant document or context, and required very little
                   general background. To investigate question answering with
                   prior knowledge, we present CommonsenseQA: a challenging new
                   dataset for commonsense question answering. To capture common
                   sense beyond associations, we extract from ConceptNet (Speer
                   et al., 2017) multiple target concepts that have the same
                   semantic relation to a single source concept. Crowd-workers
                   are asked to author multiple-choice questions that mention
                   the source concept and discriminate in turn between each of
                   the target concepts. This encourages workers to create
                   questions with complex semantics that often require prior
                   knowledge. We create 12,247 questions through this procedure
                   and demonstrate the difficulty of our task with a large
                   number of strong baselines. Our best baseline is based on
                   BERT-large (Devlin et al., 2018) and obtains 56\% accuracy,
                   well below human performance, which is 89\%.",
  month         =  nov,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1811.00937"
}

@ARTICLE{Feng2018-ci,
  title    = "Pathologies of Neural Models Make Interpretation Difficult",
  author   = "Feng, Shi and Wallace, Eric and Grissom, Alvin and Iyyer, Mohit
              and Rodriguez, Pedro and Boyd-Graber, Jordan L",
  journal  = "undefined",
  abstract = "One way to interpret neural model predictions is to highlight the
              most important input features—for example, a heatmap visualization
              over the words in an input sentence. In existing interpretation
              methods for NLP, a word’s importance is determined by either input
              perturbation—measuring the decrease in model confidence when that
              word is removed—or by the gradient with respect to that word. To
              understand the limitations of these methods, we use input
              reduction, which iteratively removes the least important word from
              the input. This exposes pathological behaviors of neural models:
              the remaining words appear nonsensical to humans and do not match
              the words interpretation methods deem important. As we confirm
              with human experiments, the reduced examples lack information to
              support the prediction of any label, but models still make the
              same predictions with high confidence. To explain these
              counterintuitive results, we draw connections to adversarial
              examples and confidence calibration: pathological behaviors reveal
              difficulties in interpreting neural models trained with maximum
              likelihood. To mitigate their deficiencies, we fine-tune the
              models by encouraging high entropy outputs on reduced examples.
              Fine-tuned models become more interpretable under input reduction
              without accuracy loss on regular examples.",
  year     =  2018,
  eprint   = "1804.07781"
}

@ARTICLE{Gonen2019-yy,
  title         = "Lipstick on a Pig: Debiasing Methods Cover up Systematic
                   Gender Biases in Word Embeddings But do not Remove Them",
  author        = "Gonen, Hila and Goldberg, Yoav",
  abstract      = "Word embeddings are widely used in NLP for a vast range of
                   tasks. It was shown that word embeddings derived from text
                   corpora reflect gender biases in society. This phenomenon is
                   pervasive and consistent across different word embedding
                   models, causing serious concern. Several recent works tackle
                   this problem, and propose methods for significantly reducing
                   this gender bias in word embeddings, demonstrating convincing
                   results. However, we argue that this removal is superficial.
                   While the bias is indeed substantially reduced according to
                   the provided bias definition, the actual effect is mostly
                   hiding the bias, not removing it. The gender bias information
                   is still reflected in the distances between
                   ``gender-neutralized'' words in the debiased embeddings, and
                   can be recovered from them. We present a series of
                   experiments to support this claim, for two debiasing methods.
                   We conclude that existing bias removal techniques are
                   insufficient, and should not be trusted for providing
                   gender-neutral modeling.",
  month         =  mar,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1903.03862"
}

@ARTICLE{Jimmy2018-bl,
  title    = "Payoffs and pitfalls in using knowledge-bases for consumer health
              search",
  author   = "{Jimmy} and Zuccon, Guido and Koopman, Bevan",
  journal  = "Information Retrieval Journal",
  abstract = "Consumer health search (CHS) is a challenging domain with
              vocabulary mismatch and considerable domain expertise hampering
              peoples’ ability to formulate effective queries. We posit that
              using knowledge bases for query reformulation may help alleviate
              this problem. How to exploit knowledge bases for effective CHS is
              nontrivial, involving a swathe of key choices and design decisions
              (many of which are not explored in the literature). Here we
              rigorously empirically evaluate the impact these different choices
              have on retrieval effectiveness.A state-of-the-art knowledge-base
              retrieval model—the Entity Query Feature Expansion model—was used
              to evaluate these choices, which include: which knowledge base to
              use (specialised vs. general purpose), how to construct the
              knowledge base, how to extract entities from queries and map them
              to entities in the knowledge base, what part of the knowledge base
              to use for query expansion, and if to augment the knowledge base
              search process with relevance feedback.While knowledge base
              retrieval has been proposed as a solution for CHS, this paper
              delves into the finer details of doing this effectively,
              highlighting both payoffs and pitfalls. It aims to provide some
              lessons to others in advancing the state-of-the-art in CHS.",
  month    =  nov,
  year     =  2018,
  doi      = "10.1007/s10791-018-9344-z",
  issn     = "1573-7659"
}

@INPROCEEDINGS{Zamani2017-px,
  title     = "Relevance-based Word Embedding",
  author    = "Zamani, Hamed and Croft, W Bruce",
  booktitle = "Proceedings of the 40th International ACM SIGIR Conference on
               Research and Development in Information Retrieval",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "505--514",
  abstract  = "Learning a high-dimensional dense representation for vocabulary
               terms, also known as a word embedding, has recently attracted
               much attention in natural language processing and information
               retrieval tasks. The embedding vectors are typically learned
               based on term proximity in a large corpus. This means that the
               objective in well-known word embedding algorithms, e.g.,
               word2vec, is to accurately predict adjacent word(s) for a given
               word or context. However, this objective is not necessarily
               equivalent to the goal of many information retrieval (IR) tasks.
               The primary objective in various IR tasks is to capture relevance
               instead of term proximity, syntactic, or even semantic
               similarity. This is the motivation for developing unsupervised
               relevance-based word embedding models that learn word
               representations based on query-document relevance information. In
               this paper, we propose two learning models with different
               objective functions; one learns a relevance distribution over the
               vocabulary set for each query, and the other classifies each term
               as belonging to the relevant or non-relevant class for each
               query. To train our models, we used over six million unique
               queries and the top ranked documents retrieved in response to
               each query, which are assumed to be relevant to the query. We
               extrinsically evaluate our learned word representation models
               using two IR tasks: query expansion and query classification.
               Both query expansion experiments on four TREC collections and
               query classification experiments on the KDD Cup 2005 dataset
               suggest that the relevance-based word embedding models
               significantly outperform state-of-the-art proximity-based
               embedding models, such as word2vec and GloVe.",
  series    = "SIGIR '17",
  year      =  2017,
  keywords  = "embedding vector, neural network, query classification, query
               expansion, word representation",
  doi       = "10.1145/3077136.3080831",
  isbn      =  9781450350228
}

@INPROCEEDINGS{Goodwin2016-ga,
  title     = "Medical Question Answering for Clinical Decision Support",
  author    = "Goodwin, Travis R and Harabagiu, Sanda M",
  booktitle = "Proceedings of the 25th ACM International on Conference on
               Information and Knowledge Management",
  publisher = "ACM",
  pages     = "297–306",
  month     =  oct,
  year      =  2016
}

@INPROCEEDINGS{Paul2015-qy,
  title     = "Diagnoses, Decisions, and Outcomes: Web Search as Decision
               Support for Cancer",
  author    = "Paul, Michael J and White, Ryen W and Horvitz, Eric",
  booktitle = "Proceedings of the 24th International Conference on World Wide
               Web",
  publisher = "International World Wide Web Conferences Steering Committee",
  pages     = "831–841",
  month     =  may,
  year      =  2015
}

@INPROCEEDINGS{Alsulmi2016-eo,
  title     = "Learning to predict the performance of clinical queries using an
               integrated approach",
  author    = "Alsulmi, M and Carterette, B",
  booktitle = "2016 IEEE International Conference on Bioinformatics and
               Biomedicine (BIBM)",
  pages     = "930–937",
  abstract  = "Several query performance prediction approaches have been
               proposed to estimate the retrieval effectiveness of user queries.
               One limitation in these approaches is that they estimate query
               performance without any consideration of the type of features
               implemented in the retrieval systems used to answer those
               queries. In this work, aiming to address this challenge, we use a
               learning based approach that combines several query predictors as
               well as some system features to predict the performance of a
               given query that is submitted to a certain retrieval system. We
               apply the result of cross-validated training to several retrieval
               systems submitted to TREC Clinical Decision Support (CDS) track,
               and show that our approach can estimate retrieval effectiveness
               values with high accuracy.",
  month     =  dec,
  year      =  2016
}

@INCOLLECTION{Whitehill2009-ka,
  title     = "Whose Vote Should Count More: Optimal Integration of Labels from
               Labelers of Unknown Expertise",
  author    = "Whitehill, Jacob and Wu, Ting-Fan and Bergsma, Jacob and
               Movellan, Javier R and Ruvolo, Paul L",
  editor    = "Bengio, Y and Schuurmans, D and Lafferty, J D and Williams, C K I
               and Culotta, A",
  booktitle = "Advances in Neural Information Processing Systems 22",
  publisher = "Curran Associates, Inc.",
  pages     = "2035–2043",
  year      =  2009
}

@ARTICLE{Nie2015-of,
  title    = "Disease Inference from Health-Related Questions via Sparse Deep
              Learning",
  author   = "Nie, L and Wang, M and Zhang, L and Yan, S and Zhang, B and Chua,
              T",
  journal  = "IEEE transactions on knowledge and data engineering",
  volume   =  27,
  number   =  8,
  pages    = "2107--2119",
  abstract = "Automatic disease inference is of importance to bridge the gap
              between what online health seekers with unusual symptoms need and
              what busy human doctors with biased expertise can offer. However,
              accurately and efficiently inferring diseases is non-trivial,
              especially for community-based health services due to the
              vocabulary gap, incomplete information, correlated medical
              concepts, and limited high quality training samples. In this
              paper, we first report a user study on the information needs of
              health seekers in terms of questions and then select those that
              ask for possible diseases of their manifested symptoms for further
              analytic. We next propose a novel deep learning scheme to infer
              the possible diseases given the questions of health seekers. The
              proposed scheme is comprised of two key components. The first
              globally mines the discriminant medical signatures from raw
              features. The second deems the raw features and their signatures
              as input nodes in one layer and hidden nodes in the subsequent
              layer, respectively. Meanwhile, it learns the inter-relations
              between these two layers via pre-training with pseudo-labeled
              data. Following that, the hidden nodes serve as raw features for
              the more abstract signature mining. With incremental and
              alternative repeating of these two components, our scheme builds a
              sparsely connected deep architecture with three hidden layers.
              Overall, it well fits specific tasks with fine-tuning. Extensive
              experiments on a real-world dataset labeled by online doctors show
              the significant performance gains of our scheme.",
  month    =  aug,
  year     =  2015,
  keywords = "data mining;diseases;health care;inference mechanisms;information
              needs;learning (artificial intelligence);medical information
              systems;pseudolabeled data;abstract signature mining;sparsely
              connected deep architecture;discriminant medical
              signatures;information needs;community-based health
              services;online health seekers;automatic disease inference;sparse
              deep learning;health-related questions;Diseases;Medical diagnostic
              imaging;Cancer;Educational institutions;Training;Community-based
              Health Services;Question Answering;Disease Inference;Deep
              Learning;Community-based health services;question
              answering;disease inference;deep learning",
  doi      = "10.1109/TKDE.2015.2399298",
  issn     = "1041-4347"
}

@INCOLLECTION{Soldaini2015-hs,
  title     = "Retrieving Medical Literature for Clinical Decision Support",
  author    = "Soldaini, Luca and Cohan, Arman and Yates, Andrew and Goharian,
               Nazli and Frieder, Ophir",
  booktitle = "Lecture Notes in Computer Science",
  pages     = "538–549",
  year      =  2015
}

@INPROCEEDINGS{Yu2016-qk,
  title     = "Retrofitting Word Vectors of {MeSH} Terms to Improve Semantic
               Similarity Measures",
  author    = "Yu, Zhiguo and Cohen, Trevor and Wallace, Byron and Bernstam,
               Elmer and Johnson, Todd",
  booktitle = "Proceedings of the Seventh International Workshop on Health Text
               Mining and Information Analysis",
  year      =  2016
}

@INPROCEEDINGS{Schoenherr2014-lp,
  title     = "Interactions between health searchers and search engines",
  author    = "Schoenherr, Georg P and White, Ryen W",
  booktitle = "Proceedings of the 37th international ACM SIGIR conference on
               Research \& development in information retrieval",
  publisher = "ACM",
  pages     = "143–152",
  month     =  jul,
  year      =  2014
}

@INPROCEEDINGS{Soldaini2015-sm,
  title     = "Retrieving medical literature for clinical decision support",
  author    = "Soldaini, Luca and Cohan, Arman and Yates, Andrew and Goharian,
               Nazli and Frieder, Ophir",
  booktitle = "European Conference on Information Retrieval (ECIR)",
  publisher = "Springer",
  pages     = "538–549",
  year      =  2015
}

@INPROCEEDINGS{Soldaini2014-tr,
  title     = "Query reformulation for clinical decision support search",
  author    = "Soldaini, Luca and Cohan, Arman and Yates, Andrew and Goharian,
               Nazli and Frieder, Ophir",
  booktitle = "23rd Text REtrieval Conference (TREC)",
  year      =  2014
}

@ARTICLE{Frenay2014-fj,
  title    = "Classification in the presence of label noise: a survey",
  author   = "Frénay, Benoît and Verleysen, Michel",
  journal  = "IEEE transactions on neural networks and learning systems",
  volume   =  25,
  number   =  5,
  pages    = "845--869",
  abstract = "Label noise is an important issue in classification, with many
              potential negative consequences. For example, the accuracy of
              predictions may decrease, whereas the complexity of inferred
              models and the number of necessary training samples may increase.
              Many works in the literature have been devoted to the study of
              label noise and the development of techniques to deal with label
              noise. However, the field lacks a comprehensive survey on the
              different types of label noise, their consequences and the
              algorithms that consider label noise. This paper proposes to fill
              this gap. First, the definitions and sources of label noise are
              considered and a taxonomy of the types of label noise is proposed.
              Second, the potential consequences of label noise are discussed.
              Third, label noise-robust, label noise cleansing, and label
              noise-tolerant algorithms are reviewed. For each category of
              approaches, a short discussion is proposed to help the
              practitioner to choose the most suitable technique in its own
              particular field of application. Eventually, the design of
              experiments is also discussed, what may interest the researchers
              who would like to test their own algorithms. In this paper, label
              noise consists of mislabeled instances: no additional information
              is assumed to be available like e.g., confidences on labels.",
  month    =  may,
  year     =  2014,
  doi      = "10.1109/TNNLS.2013.2292894",
  pmid     =  24808033,
  issn     = "2162-2388,2162-237X",
  language = "en"
}

@INPROCEEDINGS{Druck2008-yv,
  title     = "Learning from labeled features using generalized expectation
               criteria",
  author    = "Druck, Gregory and Mann, Gideon and McCallum, Andrew",
  booktitle = "SIGIR",
  pages     = "595–602",
  year      =  2008
}

@INPROCEEDINGS{Cartright2011-ri,
  title     = "Intentions and attention in exploratory health search",
  author    = "Cartright, Marc-Allen and White, Ryen W and Horvitz, Eric",
  booktitle = "Proceedings of the 34th international ACM SIGIR conference on
               Research and development in Information Retrieval",
  publisher = "ACM",
  pages     = "65–74",
  month     =  jul,
  year      =  2011
}

@ARTICLE{Uzuner2011-ul,
  title    = "{2010} {i2b2}/{VA} challenge on concepts, assertions, and
              relations in clinical text",
  author   = "Uzuner, Özlem and South, Brett R and Shen, Shuying and DuVall,
              Scott L",
  journal  = "Journal of the American Medical Informatics Association: JAMIA",
  volume   =  18,
  number   =  5,
  pages    = "552–556",
  abstract = "The 2010 i2b2/VA Workshop on Natural Language Processing
              Challenges for Clinical Records presented three tasks: a concept
              extraction task focused on the extraction of medical concepts from
              patient reports; an assertion classification task focused on
              assigning assertion types for medical problem concepts; and a
              relation classification task focused on assigning relation types
              that hold between medical problems, tests, and treatments. i2b2
              and the VA provided an annotated reference standard corpus for the
              three tasks. Using this reference standard, 22 systems were
              developed for concept extraction, 21 for assertion classification,
              and 16 for relation classification. These systems showed that
              machine learning approaches could be augmented with rule-based
              systems to determine concepts, assertions, and relations.
              Depending on the task, the rule-based systems can either provide
              input for machine learning or post-process the output of machine
              learning. Ensembles of classifiers, information from unlabeled
              data, and external knowledge sources can help when the training
              data are inadequate.",
  month    =  sep,
  year     =  2011,
  issn     = "1067-5027",
  language = "en"
}

@ARTICLE{Roberts2016-xw,
  title   = "State-of-the-art in biomedical literature retrieval for clinical
             cases: a survey of the {TREC} {2014} {CDS} track",
  author  = "Roberts, Kirk and Simpson, Matthew and Demner-Fushman, Dina and
             Voorhees, Ellen and Hersh, William",
  journal = "Information Retrieval Journal",
  volume  =  19,
  number  = "1-2",
  pages   = "113–148",
  year    =  2016
}

@INPROCEEDINGS{Divita2014-sm,
  title     = "Sophia: a expedient {UMLS} concept extraction annotator",
  author    = "Divita, Guy and Zeng, Qing T and Gundlapalli, Adi V and Duvall,
               Scott and Nebeker, Jonathan and Samore, Matthew H",
  booktitle = "AMIA Annual Symposium Proceedings",
  volume    =  2014,
  year      =  2014
}

@INPROCEEDINGS{Severyn2015-ty,
  title     = "Learning to Rank Short Text Pairs with Convolutional Deep Neural
               Networks",
  author    = "Severyn, Aliaksei and Moschitti, Alessandro",
  booktitle = "Proceedings of the 38th International ACM SIGIR Conference on
               Research and Development in Information Retrieval",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "373–382",
  series    = "SIGIR '15",
  month     =  aug,
  year      =  2015
}

@INPROCEEDINGS{Okazaki2010-el,
  title     = "Simple and Efficient Algorithm for Approximate Dictionary
               Matching",
  author    = "Okazaki, Naoaki and Tsujii, Jun'ichi",
  booktitle = "Proceedings of the 23rd International Conference on Computational
               Linguistics",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "851--859",
  series    = "COLING '10",
  year      =  2010
}

@INPROCEEDINGS{Liu2007-wf,
  title     = "Letor: Benchmark dataset for research on learning to rank for
               information retrieval",
  author    = "Liu, Tie-Yan and Xu, Jun and Qin, Tao and Xiong, Wenying and Li,
               Hang",
  booktitle = "Proceedings of SIGIR 2007 workshop on learning to rank for
               information retrieval",
  pages     = "3–10",
  year      =  2007
}

@TECHREPORT{Choi2014-dd,
  title       = "{SNUMedinfo} at {TREC} {CDS} track {2014}: Medical case-based
                 retrieval task",
  author      = "Choi, Sungbin and Choi, Jinwook",
  institution = "DTIC Document",
  year        =  2014
}

@INPROCEEDINGS{Bendersky2008-jg,
  title     = "Discovering Key Concepts in Verbose Queries",
  author    = "Bendersky, Michael and Croft, W Bruce",
  booktitle = "Proceedings of the 31st Annual International ACM SIGIR Conference
               on Research and Development in Information Retrieval",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "491–498",
  series    = "SIGIR '08",
  year      =  2008
}

@INPROCEEDINGS{Bendersky2010-yj,
  title     = "Learning Concept Importance Using a Weighted Dependence Model",
  author    = "Bendersky, Michael and Metzler, Donald and Croft, W Bruce",
  booktitle = "Proceedings of the Third ACM International Conference on Web
               Search and Data Mining",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "31–40",
  series    = "WSDM '10",
  year      =  2010
}

@ARTICLE{Tax2015-qk,
  title   = "A cross-benchmark comparison of 87 learning to rank methods",
  author  = "Tax, Niek and Bockting, Sander and Hiemstra, Djoerd",
  journal = "Information processing \& management",
  year    =  2015,
  issn    = "0306-4573"
}

@INPROCEEDINGS{Stanton2014-qi,
  title     = "Circumlocution in diagnostic medical queries",
  author    = "Stanton, Isabelle and Ieong, Samuel and Mishra, Nina",
  booktitle = "Proceedings of the 37th international ACM SIGIR conference on
               Research \& development in information retrieval",
  publisher = "ACM",
  pages     = "133–142",
  month     =  jul,
  year      =  2014
}

@INPROCEEDINGS{Soldaini2016-ay,
  title     = "Team {GU}-{IRLAB} at {CLEF} {eHealth} {2016}: Task 3",
  author    = "Soldaini, Luca and Edman, Will and Goharian, Nazli",
  booktitle = "clef",
  year      =  2016
}

@INPROCEEDINGS{Bendersky2010-gv,
  title     = "Learning Concept Importance Using a Weighted Dependence Model",
  author    = "Bendersky, Michael and Metzler, Donald and Croft, W Bruce",
  booktitle = "WSDM",
  year      =  2010
}

@INPROCEEDINGS{Pennington2014-dp,
  title     = "Glove: Global vectors for word representation",
  author    = "Pennington, Jeffrey and Socher, Richard and Manning, Christopher",
  booktitle = "Proceedings of the 2014 conference on empirical methods in
               natural language processing (EMNLP)",
  pages     = "1532--1543",
  year      =  2014
}

@INPROCEEDINGS{Roberts2017-kf,
  title     = "Overview of the {TREC} {2016} Clinical Decision Support Track",
  author    = "Roberts, Kirk and Demner-Fushman, Dina and Voorhees, Ellen M and
               Hersh, William R",
  booktitle = "TREC",
  year      =  2017
}

@INPROCEEDINGS{Soldaini2015-lg,
  title     = "Retrieving medical literature for clinical decision support",
  author    = "Soldaini, Luca and Cohan, Arman and Yates, Andrew and Goharian,
               Nazli and Frieder, Ophir",
  booktitle = "ECIR",
  year      =  2015
}

@TECHREPORT{Soldaini2014-jz,
  title       = "Query reformulation for clinical decision support search",
  author      = "Soldaini, Luca and Cohan, Arman and Yates, Andrew and Goharian,
                 Nazli and Frieder, Ophir",
  institution = "DTIC Document",
  year        =  2014
}

@INPROCEEDINGS{Cohan2014-kb,
  title     = "On clinical decision support",
  author    = "Cohan, Arman and Soldaini, Luca and Yates, Andrew and Goharian,
               Nazli and Frieder, Ophir",
  booktitle = "Proceedings of the 5th ACM Conference on Bioinformatics,
               Computational Biology, and Health Informatics",
  publisher = "ACM",
  pages     = "651–652",
  year      =  2014
}

@INPROCEEDINGS{Cohan2014-jv,
  title     = "Towards citation-based summarization of biomedical literature",
  author    = "Cohan, Arman and Soldaini, Luca and Goharian, Nazli",
  booktitle = "Proceedings of the Text Analysis Conference (TAC'14)",
  year      =  2014
}

@ARTICLE{Cohan2015-ly,
  title   = "Matching Citation Text and Cited Spans in Biomedical Literature: a
             Search-Oriented Approach",
  author  = "Cohan, Arman and Soldaini, Luca and Goharian, Nazli",
  journal = "North American Chapter of the Association for Computational
             Linguistics–Human Language Technologies (NAACL HLT 2015)",
  year    =  2015
}

@INPROCEEDINGS{Bendersky2011-ov,
  title     = "Parameterized concept weighting in verbose queries",
  author    = "Bendersky, Michael and Metzler, Donald and Croft, W Bruce",
  booktitle = "Proceedings of the 34th international ACM SIGIR conference on
               Research and development in Information Retrieval",
  publisher = "ACM",
  pages     = "605–614",
  month     =  jul,
  year      =  2011
}

@INPROCEEDINGS{Soldaini2015-vj,
  title     = "Retrieving medical literature for clinical decision support",
  author    = "Soldaini, Luca and Cohan, Arman and Yates, Andrew and Goharian,
               Nazli and Frieder, Ophir",
  booktitle = "European Conference on Information Retrieval",
  publisher = "Springer, Cham",
  pages     = "538–549",
  year      =  2015
}

@INPROCEEDINGS{Cohan2015-zi,
  title     = "Identifying Significance of Discrepancies in Radiology Reports",
  author    = "Cohan, Arman and Soldaini, Luca and Goharian, Nazli and Fong,
               Allan and Filice, Ross and Ratwani, Raj",
  booktitle = "5th Workshop on Data Mining for Medicine and Healthcare",
  pages     =  41,
  year      =  2015
}

@ARTICLE{Soldaini2016-aa,
  title   = "Team {GU}-{IRLAB} at {CLEF} {eHealth} {2016}: Task 3",
  author  = "Soldaini, Luca and Edman, Will and Goharian, Nazli",
  journal = "Proceedings of CLEF eHealth 2016",
  year    =  2016
}

@INPROCEEDINGS{Soldaini2017-bm,
  title     = "Inferring individual attributes from search engine queries and
               auxiliary information",
  author    = "Soldaini, Luca and Yom-Tov, Elad",
  booktitle = "Proceedings of the 26th International Conference on World Wide
               Web",
  publisher = "International World Wide Web Conferences Steering Committee",
  pages     = "293–301",
  year      =  2017
}

@INPROCEEDINGS{Diaz2016-ja,
  title     = "Query Expansion with Locally-Trained Word Embeddings",
  author    = "Diaz, Fernando and Mitra, Bhaskar and Craswell, Nick",
  booktitle = "Proceedings of the 54th Annual Meeting of the Association for
               Computational Linguistics",
  pages     = "367–377",
  abstract  = "Continuous space word embeddings have received a great deal of
               attention in the natural language processing and machine learning
               communities for their ability to model term similarity and other
               relationships. We study the use of term relatedness in the
               context of query expansion for ad hoc information retrieval. We
               demonstrate that word embeddings such as word2vec and GloVe, when
               trained globally, underperform corpus and query specific
               embeddings for retrieval tasks. These results suggest that other
               tasks benefiting from global embeddings may also benefit from
               local embeddings.",
  month     =  may,
  year      =  2016
}

@ARTICLE{Rekabsaz2017-pf,
  title         = "Toward Incorporation of Relevant Documents in {word2vec}",
  author        = "Rekabsaz, Navid and Mitra, Bhaskar and Lupu, Mihai and
                   Hanbury, Allan",
  abstract      = "Recent advances in neural word embedding provide significant
                   benefit to various information retrieval tasks. However as
                   shown by recent studies, adapting the embedding models for
                   the needs of IR tasks can bring considerable further
                   improvements. The embedding models in general define the term
                   relatedness by exploiting the terms' co-occurrences in
                   short-window contexts. An alternative (and well-studied)
                   approach in IR for related terms to a query is using local
                   information i.e. a set of top-retrieved documents. In view of
                   these two methods of term relatedness, in this work, we
                   report our study on incorporating the local information of
                   the query in the word embeddings. One main challenge in this
                   direction is that the dense vectors of word embeddings and
                   their estimation of term-to-term relatedness remain difficult
                   to interpret and hard to analyze. As an alternative, explicit
                   word representations propose vectors whose dimensions are
                   easily interpretable, and recent methods show competitive
                   performance to the dense vectors. We introduce a neural-based
                   explicit representation, rooted in the conceptual ideas of
                   the word2vec Skip-Gram model. The method provides
                   interpretable explicit vectors while keeping the
                   effectiveness of the Skip-Gram model. The evaluation of
                   various explicit representations on word association
                   collections shows that the newly proposed method out-
                   performs the state-of-the-art explicit representations when
                   tasked with ranking highly similar terms. Based on the
                   introduced ex- plicit representation, we discuss our
                   approaches on integrating local documents in globally-trained
                   embedding models and discuss the preliminary results.",
  month         =  jul,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "1707.06598"
}

@INPROCEEDINGS{Nguyen2017-mm,
  title     = "Learning Concept-Driven Document Embeddings for Medical
               Information Search",
  author    = "Nguyen, Gia-Hung and Tamine, Lynda and Soulier, Laure and Souf,
               Nathalie",
  booktitle = "Artificial Intelligence in Medicine",
  publisher = "Springer, Cham",
  pages     = "160–170",
  abstract  = "Many medical tasks such as self-diagnosis, health-care
               assessment, and clinical trial patient recruitment involve the
               usage of information access tools. A key underlying step to
               achieve such tasks is the document-to-document matching which
               mostly fails to bridge the gap identified between raw level
               representations of information in documents and high-level human
               interpretation. In this paper, we study how to optimize the
               document representation by leveraging neural-based approaches to
               capture latent representations built upon both validated medical
               concepts specified in an external resource as well as the used
               words. We experimentally show the effectiveness of our proposed
               model used as a support of two different medical search tasks,
               namely health search and clinical search for cohorts.",
  series    = "Lecture Notes in Computer Science",
  month     =  jun,
  year      =  2017,
  language  = "en"
}

@ARTICLE{Ammar2016-xz,
  title         = "Massively Multilingual Word Embeddings",
  author        = "Ammar, Waleed and Mulcaire, George and Tsvetkov, Yulia and
                   Lample, Guillaume and Dyer, Chris and Smith, Noah A",
  abstract      = "We introduce new methods for estimating and evaluating
                   embeddings of words in more than fifty languages in a single
                   shared embedding space. Our estimation methods, multiCluster
                   and multiCCA, use dictionaries and monolingual data; they do
                   not require parallel data. Our new evaluation method,
                   multiQVEC-CCA, is shown to correlate better than previous
                   ones with two downstream tasks (text categorization and
                   parsing). We also describe a web portal for evaluation that
                   will facilitate further research in this area, along with
                   open-source releases of all our methods.",
  month         =  feb,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1602.01925"
}

@ARTICLE{Luong2015-ck,
  title    = "Bilingual Word Representations with Monolingual Quality in Mind",
  author   = "Luong, T and Pham, H and Manning, C D",
  journal  = "VS@ HLT-NAACL",
  abstract = "Abstract Recent work in learning bilingual representations tend to
              tailor towards achieving good performance on bilingual tasks, most
              often the crosslingual document classification (CLDC) evaluation,
              but to the detriment of preserving clustering structures of word
              representations monolingually. In this work, we propose a joint
              model to learn word representations from scratch that utilizes
              both the context coocurrence information through ...",
  year     =  2015
}

@INPROCEEDINGS{Nguyen2017-mw,
  title     = "{DSRIM}: A Deep Neural Information Retrieval Model Enhanced by a
               Knowledge Resource Driven Representation of Documents",
  author    = "Nguyen, Gia-Hung and Soulier, Laure and Tamine, Lynda and
               Bricon-Souf, Nathalie",
  booktitle = "Proceedings of the ACM SIGIR International Conference on Theory
               of Information Retrieval",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "19--26",
  abstract  = "The state-of-the-art solutions to the vocabulary mismatch in
               information retrieval (IR) mainly aim at leveraging either the
               relational semantics provided by external resources or the
               distributional semantics, recently investigated by deep neural
               approaches. Guided by the intuition that the relational semantics
               might improve the effectiveness of deep neural approaches, we
               propose the Deep Semantic Resource Inference Model (DSRIM) that
               relies on: 1) a representation of raw-data that models the
               relational semantics of text by jointly considering objects and
               relations expressed in a knowledge resource, and 2) an end-to-end
               neural architecture that learns the query-document relevance by
               leveraging the distributional and relational semantics of
               documents and queries. The experimental evaluation carried out on
               two TREC datasets from TREC Terabyte and TREC CDS tracks relying
               respectively on WordNet and MeSH resources, indicates that our
               model outperforms state-of-the-art semantic and deep neural IR
               models.",
  series    = "ICTIR '17",
  year      =  2017,
  keywords  = "ad-hoc ir, deep neural architecture, knowledge resource, semantic
               document representation",
  doi       = "10.1145/3121050.3121063",
  isbn      =  9781450344906
}

@INPROCEEDINGS{Riezler2007-fo,
  title     = "Statistical machine translation for query expansion in answer
               retrieval",
  author    = "Riezler, Stefan and Vasserman, Alexander and Tsochantaridis,
               Ioannis and Mittal, Vibhu and Liu, Yi",
  booktitle = "Annual Meeting-Association For Computational Linguistics",
  volume    =  45,
  pages     =  464,
  year      =  2007
}

@INPROCEEDINGS{Siencnik2015-hb,
  title     = "Adapting {word2vec} to named entity recognition",
  author    = "Sienčnik, Scharolta Katharina",
  booktitle = "Proceedings of the 20th Nordic Conference of Computational
               Linguistics, NODALIDA 2015, May 11-13, 2015, Vilnius, Lithuania",
  publisher = "Linköping University Electronic Press",
  pages     = "239–243",
  year      =  2015
}

@ARTICLE{Wick2016-us,
  title    = "Minimally-Constrained Multilingual Embeddings via Artificial
              Code-Switching",
  author   = "Wick, M and Kanani, P and Pocock, A C",
  journal  = "AAAI",
  abstract = "Abstract We present a method that consumes a large corpus of
              multilingual text and produces a single, unified word embedding in
              which the word vectors generalize across languages. In contrast to
              current approaches that require language identification, our
              method is agnostic about the languages with which the documents in
              the corpus are expressed, and does not rely on parallel corpora to
              constrain the spaces. Instead we ...",
  year     =  2016
}

@ARTICLE{Pedersen2007-gb,
  title    = "Measures of semantic similarity and relatedness in the biomedical
              domain",
  author   = "Pedersen, Ted and Pakhomov, Serguei V S and Patwardhan, Siddharth
              and Chute, Christopher G",
  journal  = "Journal of biomedical informatics",
  volume   =  40,
  number   =  3,
  pages    = "288–299",
  abstract = "Measures of semantic similarity between concepts are widely used
              in Natural Language Processing. In this article, we show how six
              existing domain-independent measures can be adapted to the
              biomedical domain. These measures were originally based on
              WordNet, an English lexical database of concepts and relations. In
              this research, we adapt these measures to the SNOMED-CT ontology
              of medical concepts. The measures include two path-based measures,
              and three measures that augment path-based measures with
              information content statistics from corpora. We also derive a
              context vector measure based on medical corpora that can be used
              as a measure of semantic relatedness. These six measures are
              evaluated against a newly created test bed of 30 medical concept
              pairs scored by three physicians and nine medical coders. We find
              that the medical coders and physicians differ in their ratings,
              and that the context vector measure correlates most closely with
              the physicians, while the path-based measures and one of the
              information content measures correlates most closely with the
              medical coders. We conclude that there is a role both for more
              flexible measures of relatedness based on information derived from
              corpora, as well as for measures that rely on existing ontological
              structures.",
  month    =  jun,
  year     =  2007,
  issn     = "1532-0464",
  language = "en"
}

@ARTICLE{Faruqui2014-gl,
  title         = "Retrofitting Word Vectors to Semantic Lexicons",
  author        = "Faruqui, Manaal and Dodge, Jesse and Jauhar, Sujay K and
                   Dyer, Chris and Hovy, Eduard and Smith, Noah A",
  abstract      = "Vector space word representations are learned from
                   distributional information of words in large corpora.
                   Although such statistics are semantically informative, they
                   disregard the valuable information that is contained in
                   semantic lexicons such as WordNet, FrameNet, and the
                   Paraphrase Database. This paper proposes a method for
                   refining vector space representations using relational
                   information from semantic lexicons by encouraging linked
                   words to have similar vector representations, and it makes no
                   assumptions about how the input vectors were constructed.
                   Evaluated on a battery of standard lexical semantic
                   evaluation tasks in several languages, we obtain substantial
                   improvements starting with a variety of word vector models.
                   Our refinement method outperforms prior techniques for
                   incorporating semantic lexicons into the word vector training
                   algorithms.",
  month         =  nov,
  year          =  2014,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1411.4166"
}

@INPROCEEDINGS{Kilicoglu2016-uq,
  title     = "Annotating Named Entities in Consumer Health Questions",
  author    = "Kilicoglu, Halil and Abacha, Asma Ben and Mrabet, Yassine and
               Roberts, Kirk and Rodriguez, Laritza and Shooshan, Sonya E and
               Demner-Fushman, Dina",
  booktitle = "LREC",
  year      =  2016
}

@ARTICLE{Sennrich2015-qp,
  title         = "Neural Machine Translation of Rare Words with Subword Units",
  author        = "Sennrich, Rico and Haddow, Barry and Birch, Alexandra",
  abstract      = "Neural machine translation (NMT) models typically operate
                   with a fixed vocabulary, but translation is an
                   open-vocabulary problem. Previous work addresses the
                   translation of out-of-vocabulary words by backing off to a
                   dictionary. In this paper, we introduce a simpler and more
                   effective approach, making the NMT model capable of
                   open-vocabulary translation by encoding rare and unknown
                   words as sequences of subword units. This is based on the
                   intuition that various word classes are translatable via
                   smaller units than words, for instance names (via character
                   copying or transliteration), compounds (via compositional
                   translation), and cognates and loanwords (via phonological
                   and morphological transformations). We discuss the
                   suitability of different word segmentation techniques,
                   including simple character n-gram models and a segmentation
                   based on the byte pair encoding compression algorithm, and
                   empirically show that subword models improve over a back-off
                   dictionary baseline for the WMT 15 translation tasks
                   English-German and English-Russian by 1.1 and 1.3 BLEU,
                   respectively.",
  month         =  aug,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1508.07909"
}

@INPROCEEDINGS{Ghosh2016-pa,
  title     = "Characterizing Diseases from Unstructured Text: A Vocabulary
               Driven {Word2Vec} Approach",
  author    = "Ghosh, Saurav and Chakraborty, Prithwish and Cohn, Emily and
               Brownstein, John S and Ramakrishnan, Naren",
  booktitle = "Proceedings of the 25th ACM International on Conference on
               Information and Knowledge Management",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "1129--1138",
  abstract  = "Traditional disease surveillance can be augmented with a wide
               variety of real-time sources such as, news and social media.
               However, these sources are in general unstructured and,
               construction of surveillance tools such as taxonomical
               correlations and trace mapping involves considerable human
               supervision. In this paper, we motivate a disease vocabulary
               driven word2vec model (Dis2Vec) to model diseases and constituent
               attributes as word embeddings from the HealthMap news corpus. We
               use these word embeddings to automatically create disease
               taxonomies and evaluate our model against corresponding human
               annotated taxonomies. We compare our model accuracies against
               several state-of-the art word2vec methods. Our results
               demonstrate that Dis2Vec outperforms traditional distributed
               vector representations in its ability to faithfully capture
               taxonomical attributes across different class of diseases such as
               endemic, emerging and rare.",
  series    = "CIKM '16",
  year      =  2016,
  keywords  = "application-specific word embeddings, disease characterization,
               emerging diseases, healthmap, rare diseases",
  doi       = "10.1145/2983323.2983362",
  isbn      =  9781450340731
}

@INPROCEEDINGS{Lin2017-yf,
  title     = "Neural Relation Extraction with Multi-lingual Attention",
  author    = "Lin, Yankai and Liu, Zhiyuan and Sun, Maosong",
  booktitle = "Proceedings of the 55th Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "34–43",
  year      =  2017
}

@ARTICLE{Pakhomov2010-wy,
  title    = "Semantic Similarity and Relatedness between Clinical Terms: An
              Experimental Study",
  author   = "Pakhomov, Serguei and McInnes, Bridget and Adam, Terrence and Liu,
              Ying and Pedersen, Ted and Melton, Genevieve B",
  journal  = "AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA
              Symposium",
  volume   =  2010,
  pages    = "572–576",
  abstract = "Automated approaches to measuring semantic similarity and
              relatedness can provide necessary semantic context information for
              information retrieval applications and a number of fundamental
              natural language processing tasks including word sense
              disambiguation. Challenges for the development of these approaches
              include the limited availability of validated reference standards
              and the need for better understanding of the notions of semantic
              relatedness and similarity in medical vocabulary. We present
              results of a study in which eight medical residents were asked to
              judge 724 pairs of medical terms for semantic similarity and
              relatedness. The results of the study confirm the existence of a
              measurable mental representation of semantic relatedness between
              medical terms that is distinct from similarity and independent of
              the context in which the terms occur. This study produced a
              validated publicly available dataset for developing automated
              approaches to measuring semantic relatedness and similarity.",
  month    =  nov,
  year     =  2010,
  language = "en"
}

@INPROCEEDINGS{Ammar2017-kg,
  title     = "The {AI2} system at {SemEval}-{2017} Task 10 ({ScienceIE}):
               semi-supervised end-to-end entity and relation extraction",
  author    = "Ammar, Waleed and Peters, Matthew E and Bhagavatula, Chandra and
               Power, Russell",
  booktitle = "Proceedings of SemEval 2017, Task 10, ScienceIE",
  year      =  2017
}

@INPROCEEDINGS{Mintz2009-ub,
  title     = "Distant Supervision for Relation Extraction Without Labeled Data",
  author    = "Mintz, Mike and Bills, Steven and Snow, Rion and Jurafsky, Dan",
  booktitle = "Proceedings of the Joint Conference of the 47th Annual Meeting of
               the ACL and the 4th International Joint Conference on Natural
               Language Processing of the AFNLP: Volume 2 - Volume 2",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "1003–1011",
  series    = "ACL '09",
  year      =  2009
}

@INPROCEEDINGS{Artetxe2017-zu,
  title     = "Learning bilingual word embeddings with (almost) no bilingual
               data",
  author    = "Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko",
  booktitle = "Proceedings of the 55th Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "451–462",
  year      =  2017
}

@ARTICLE{Nguyen2017-mi,
  title         = "Hierarchical Embeddings for Hypernymy Detection and
                   Directionality",
  author        = "Nguyen, Kim Anh and Köper, Maximilian and Walde, Sabine
                   Schulte im and Vu, Ngoc Thang",
  abstract      = "We present a novel neural model HyperVec to learn
                   hierarchical embeddings for hypernymy detection and
                   directionality. While previous embeddings have shown
                   limitations on prototypical hypernyms, HyperVec represents an
                   unsupervised measure where embeddings are learned in a
                   specific order and capture the hypernym$-$hyponym
                   distributional hierarchy. Moreover, our model is able to
                   generalize over unseen hypernymy pairs, when using only small
                   sets of training data, and by mapping to other languages.
                   Results on benchmark datasets show that HyperVec outperforms
                   both state$-$of$-$the$-$art unsupervised measures and
                   embedding models on hypernymy detection and directionality,
                   and on predicting graded lexical entailment.",
  month         =  jul,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1707.07273"
}

@INPROCEEDINGS{Yao2010-sb,
  title     = "Collective Cross-document Relation Extraction Without Labelled
               Data",
  author    = "Yao, Limin and Riedel, Sebastian and McCallum, Andrew",
  booktitle = "Proceedings of the 2010 Conference on Empirical Methods in
               Natural Language Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "1013–1023",
  series    = "EMNLP '10",
  year      =  2010
}

@ARTICLE{Johnson2016-jg,
  title    = "{MIMIC}-{III}, a freely accessible critical care database",
  author   = "Johnson, Alistair E W and Pollard, Tom J and Shen, Lu and Lehman,
              Li-Wei H and Feng, Mengling and Ghassemi, Mohammad and Moody,
              Benjamin and Szolovits, Peter and Celi, Leo Anthony and Mark,
              Roger G",
  journal  = "Sci Data",
  volume   =  3,
  pages    =  160035,
  abstract = "MIMIC-III ('Medical Information Mart for Intensive Care') is a
              large, single-center database comprising information relating to
              patients admitted to critical care units at a large tertiary care
              hospital. Data includes vital signs, medications, laboratory
              measurements, observations and notes charted by care providers,
              fluid balance, procedure codes, diagnostic codes, imaging reports,
              hospital length of stay, survival data, and more. The database
              supports applications including academic and industrial research,
              quality improvement initiatives, and higher education coursework.",
  month    =  may,
  year     =  2016,
  language = "en"
}

@ARTICLE{Tai2015-yt,
  title         = "Improved Semantic Representations From Tree-Structured Long
                   Short-Term Memory Networks",
  author        = "Tai, Kai Sheng and Socher, Richard and Manning, Christopher D",
  abstract      = "Because of their superior ability to preserve sequence
                   information over time, Long Short-Term Memory (LSTM)
                   networks, a type of recurrent neural network with a more
                   complex computational unit, have obtained strong results on a
                   variety of sequence modeling tasks. The only underlying LSTM
                   structure that has been explored so far is a linear chain.
                   However, natural language exhibits syntactic properties that
                   would naturally combine words to phrases. We introduce the
                   Tree-LSTM, a generalization of LSTMs to tree-structured
                   network topologies. Tree-LSTMs outperform all existing
                   systems and strong LSTM baselines on two tasks: predicting
                   the semantic relatedness of two sentences (SemEval 2014, Task
                   1) and sentiment classification (Stanford Sentiment
                   Treebank).",
  month         =  feb,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1503.00075"
}

@INPROCEEDINGS{Burl1994-ce,
  title     = "Automated analysis of radar imagery of Venus: handling lack of
               ground truth",
  author    = "Burl, M C and Fayyad, U M and Perona, P and Smyth, P",
  booktitle = "Proceedings of 1st International Conference on Image Processing",
  volume    =  3,
  pages     = "236–240 vol.3",
  abstract  = "Lack of verifiable ground truth is a common problem in remote
               sensing image analysis. For example, consider the synthetic
               aperture radar (SAR) image data of Venus obtained by the Magellan
               spacecraft. Planetary scientists are interested in automatically
               cataloging the locations of all the small volcanoes in this data
               set; however, the problem is very difficult and cannot be
               performed with perfect reliability even by human experts. Thus,
               training and evaluating the performance of an automatic algorithm
               on this data set must be handled carefully. We discuss the use of
               weighted free-response receiver-operating characteristics
               (wFROCs) for evaluating detection performance when the “ground
               truth” is subjective. In particular, we evaluate the relative
               detection performance of humans and automatic algorithms. Our
               experimental results indicate that proper assessment of the
               uncertainty in “ground truth” is essential in applications of
               this nature",
  month     =  nov,
  year      =  1994
}

@ARTICLE{Jin2018-on,
  title    = "Approximately optimizing {NDCG} using pair-wise loss",
  author   = "Jin, Xiao-Bo and Geng, Guang-Gang and Xie, Guo-Sen and Huang,
              Kaizhu",
  journal  = "Information sciences",
  volume   =  453,
  pages    = "50–65",
  abstract = "The Normalized Discounted Cumulative Gain (NDCG) is used to
              measure the performance of ranking algorithms. Much of the work on
              learning to rank by optimizing NDCG directly or indirectly is
              based on list-wise approaches. In our work, we approximately
              optimize a variant of NDCG called NDCGβ using pair-wise
              approaches. NDCGβ utilizes the linear discounting function. We
              first prove that the DCG error of NDCGβ is equal to the weighted
              pair-wise loss; then, on that basis, RankBoostndcg and RankSVMndcg
              are proposed to optimize the upper bound of the pair-wise 0–1 loss
              function. The experimental results from applying our approaches
              and ten other state-of-the-art methods to five public datasets
              show the superiority of the proposed methods, especially
              RankSVMndcg. In addition, RankBoostndcg are less influenced by the
              initial weight distribution.",
  month    =  jul,
  year     =  2018,
  issn     = "0020-0255"
}

@INPROCEEDINGS{Zeng2015-hw,
  title     = "Distant supervision for relation extraction via piecewise
               convolutional neural networks",
  author    = "Zeng, Daojian and Liu, Kang and Chen, Yubo and Zhao, Jun",
  booktitle = "Proceedings of the 2015 Conference on Empirical Methods in
               Natural Language Processing",
  pages     = "1753--1762",
  year      =  2015
}

@INPROCEEDINGS{Balaneshin-kordan2016-na,
  title     = "Optimization Method for Weighting Explicit and Latent Concepts in
               Clinical Decision Support Queries",
  author    = "Balaneshin-kordan, Saeid and Kotov, Alexander",
  booktitle = "Proceedings of the 2016 ACM International Conference on the
               Theory of Information Retrieval",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "241–250",
  abstract  = "Abstract Accurately answering verbose queries that describe a
               clinical case and aim at finding articles in a collection of
               medical literature requires capturing many explicit and latent
               aspects of complex information needs underlying such queries.
               Proper representation of",
  series    = "ICTIR '16",
  year      =  2016
}

@INPROCEEDINGS{Guo2017-zy,
  title     = "On Calibration of Modern Neural Networks",
  author    = "Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q",
  booktitle = "International Conference on Machine Learning",
  pages     = "1321--1330",
  abstract  = "Confidence calibration – the problem of predicting probability
               estimates representative of the true correctness likelihood – is
               important for classification models in many applications. We
               discover...",
  month     =  jul,
  year      =  2017,
  language  = "en"
}

@ARTICLE{Lipton2018-mi,
  title    = "Detecting and Correcting for Label Shift with Black Box Predictors",
  author   = "Lipton, Zachary C and Wang, Yu-Xiang and Smola, Alex",
  journal  = "arXiv [cs.LG]",
  abstract = "Faced with distribution shift between training and test set, we
              wish to detect and quantify the shift, and to correct our
              classifiers without test set labels. Motivated by medical
              diagnosis, where diseases (targets), cause symptoms
              (observations), we focus on label shift, where the label marginal
              $p(y)$ changes but the conditional $p(x|y)$ does not. We propose
              Black Box Shift Estimation (BBSE) to estimate the test
              distribution $p(y)$. BBSE exploits arbitrary black box predictors
              to reduce dimensionality prior to shift correction. While better
              predictors give tighter estimates, BBSE works even when predictors
              are biased, inaccurate, or uncalibrated, so long as their
              confusion matrices are invertible. We prove BBSE's consistency,
              bound its error, and introduce a statistical test that uses BBSE
              to detect shift. We also leverage BBSE to correct classifiers.
              Experiments demonstrate accurate estimates and improved
              prediction, even on high-dimensional datasets of natural images",
  month    =  feb,
  year     =  2018
}

@ARTICLE{Luong2015-np,
  title    = "Effective Approaches to Attention-based Neural Machine Translation",
  author   = "Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D",
  journal  = "arXiv:1508.04025 [cs]",
  abstract = "An attentional mechanism has lately been used to improve neural
              machine translation (NMT) by selectively focusing on parts of the
              source sentence during translation. However, there has been little
              work exploring useful architectures for attention-based NMT. This
              paper examines two simple and effective classes of attentional
              mechanism: a global approach which always attends to all source
              words and a local one that only looks at a subset of source words
              at a time. We demonstrate the effectiveness of both approaches
              over the WMT translation tasks between English and German in both
              directions. With local attention, we achieve a significant gain of
              5.0 BLEU points over non-attentional systems which already
              incorporate known techniques such as dropout. Our ensemble model
              using different attention architectures has established a new
              state-of-the-art result in the WMT'15 English to German
              translation task with 25.9 BLEU points, an improvement of 1.0 BLEU
              points over the existing best system backed by NMT and an n-gram
              reranker.",
  month    =  aug,
  year     =  2015,
  language = "en"
}

@ARTICLE{Duchi2011-on,
  title   = "Adaptive Subgradient Methods for Online Learning and Stochastic
             Optimization",
  author  = "Duchi, John and Hazan, Elad and Singer, Yoram",
  journal = "Journal of machine learning research: JMLR",
  volume  =  12,
  pages   = "2121–2159",
  month   =  jul,
  year    =  2011,
  issn    = "1532-4435"
}

@ARTICLE{Vaswani2017-ue,
  title    = "Attention Is All You Need",
  author   = "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit,
              Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and
              Polosukhin, Illia",
  journal  = "arXiv:1706.03762 [cs]",
  abstract = "The dominant sequence transduction models are based on complex
              recurrent or convolutional neural networks in an encoder-decoder
              configuration. The best performing models also connect the encoder
              and decoder through an attention mechanism. We propose a new
              simple network architecture, the Transformer, based solely on
              attention mechanisms, dispensing with recurrence and convolutions
              entirely. Experiments on two machine translation tasks show these
              models to be superior in quality while being more parallelizable
              and requiring significantly less time to train. Our model achieves
              28.4 BLEU on the WMT 2014 English-to-German translation task,
              improving over the existing best results, including ensembles by
              over 2 BLEU. On the WMT 2014 English-to-French translation task,
              our model establishes a new single-model state-of-the-art BLEU
              score of 41.8 after training for 3.5 days on eight GPUs, a small
              fraction of the training costs of the best models from the
              literature. We show that the Transformer generalizes well to other
              tasks by applying it successfully to English constituency parsing
              both with large and limited training data.",
  month    =  jun,
  year     =  2017
}

@INPROCEEDINGS{Ai2018-zh,
  title     = "Learning a Deep Listwise Context Model for Ranking Refinement",
  author    = "Ai, Qingyao and Bi, Keping and Guo, Jiafeng and Croft, W Bruce",
  publisher = "ACM Press",
  pages     = "135--144",
  abstract  = "Learning to rank has been intensively studied and widely applied
               in information retrieval. Typically, a global ranking function is
               learned from a set of labeled data, which can achieve good
               performance on average but may be suboptimal for individual
               queries by ignoring the fact that relevant documents for
               different queries may have different distributions in the feature
               space. Inspired by the idea of pseudo relevance feedback where
               top ranked documents, which we refer as the local ranking
               context, can provide important information about the query’s
               characteristics, we propose to use the inherent feature
               distributions of the top results to learn a Deep Listwise Context
               Model that helps us fine tune the initial ranked list.
               Specifically, we employ a recurrent neural network to
               sequentially encode the top results using their feature vectors,
               learn a local context model and use it to re-rank the top
               results. There are three merits with our model: (1) Our model can
               capture the local ranking context based on the complex
               interactions between top results using a deep neural network; (2)
               Our model can be built upon existing learning-to-rank methods by
               directly using their extracted feature vectors; (3) Our model is
               trained with an attention-based loss function, which is more
               effective and efficient than many existing listwise methods.
               Experimental results show that the proposed model can
               significantly improve the state-of-the-art learning to rank
               methods on benchmark retrieval corpora.",
  year      =  2018,
  doi       = "10.1145/3209978.3209985",
  isbn      =  9781450356572,
  language  = "en"
}

@INPROCEEDINGS{Kim2018-bu,
  title     = "Joint Learning of Domain Classification and Out-of-Domain
               Detection with Dynamic Class Weighting for Satisficing False
               Acceptance Rates",
  author    = "Kim, Joo-Kyung and Kim, Young-Bum",
  booktitle = "arXiv:1807.00072 [cs]",
  abstract  = "In domain classification for spoken dialog systems, correct
               detection of out-of-domain (OOD) utterances is crucial because it
               reduces confusion and unnecessary interaction costs between users
               and the systems. Previous work usually utilizes OOD detectors
               that are trained separately from in-domain (IND) classifiers, and
               confidence thresholding for OOD detection given target evaluation
               scores. In this paper, we introduce a neural joint learning model
               for domain classification and OOD detection, where dynamic class
               weighting is used during the model training to satisfice a given
               OOD false acceptance rate (FAR) while maximizing the domain
               classification accuracy. Evaluating on two domain classification
               tasks for the utterances from a large spoken dialogue system, we
               show that our approach significantly improves the domain
               classification performance with satisficing given target FARs.",
  month     =  jun,
  year      =  2018
}

@INPROCEEDINGS{Jagerman2017-ot,
  title     = "Modeling Label Ambiguity for Neural List-Wise Learning to Rank",
  author    = "Jagerman, Rolf and Kiseleva, Julia and de Rijke, Maarten",
  booktitle = "SIGIR 2017 Workshop on Neural Information Retrieval (Neu-IR’17),",
  abstract  = "List-wise learning to rank methods are considered to be the
               stateof-the-art. One of the major problems with these methods is
               that the ambiguous nature of relevance labels in learning to rank
               data is ignored. Ambiguity of relevance labels refers to the
               phenomenon that multiple documents may be assigned the same
               relevance label for a given query, so that no preference order
               should be learned for those documents. In this paper we propose a
               novel sampling technique for computing a list-wise loss that can
               take into account this ambiguity. We show the e ectiveness of the
               proposed method by training a 3-layer deep neural network. We
               compare our new loss function to two strong baselines: ListNet
               and ListMLE. We show that our method generalizes better and signi
               cantly outperforms other methods on the validation and test sets.",
  month     =  aug,
  year      =  2017,
  language  = "en"
}

@ARTICLE{Bahdanau2014-en,
  title    = "Neural Machine Translation by Jointly Learning to Align and
              Translate",
  author   = "Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua",
  journal  = "arXiv:1409.0473 [cs, stat]",
  abstract = "Neural machine translation is a recently proposed approach to
              machine translation. Unlike the traditional statistical machine
              translation, the neural machine translation aims at building a
              single neural network that can be jointly tuned to maximize the
              translation performance. The models proposed recently for neural
              machine translation often belong to a family of encoder–decoders
              and encode a source sentence into a ﬁxed-length vector from which
              a decoder generates a translation. In this paper, we conjecture
              that the use of a ﬁxed-length vector is a bottleneck in improving
              the performance of this basic encoder–decoder architecture, and
              propose to extend this by allowing a model to automatically
              (soft-)search for parts of a source sentence that are relevant to
              predicting a target word, without having to form these parts as a
              hard segment explicitly. With this new approach, we achieve a
              translation performance comparable to the existing
              state-of-the-art phrase-based system on the task of
              English-to-French translation. Furthermore, qualitative analysis
              reveals that the (soft-)alignments found by the model agree well
              with our intuition.",
  month    =  sep,
  year     =  2014,
  language = "en"
}

@INPROCEEDINGS{Jiang2018-fw,
  title     = "Learning Word Embeddings for Low-Resource Languages by {PU}
               Learning",
  author    = "Jiang, Chao and Yu, Hsiang-Fu and Hsieh, Cho-Jui and Chang,
               Kai-Wei",
  booktitle = "Proceedings of the 2018 Conference of the North American Chapter
               of the Association for Computational Linguistics: Human Language
               Technologies, Volume 1 (Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "New Orleans, Louisiana",
  pages     = "1024–1034",
  year      =  2018
}

@INPROCEEDINGS{Kim2018-xh,
  title     = "Efﬁcient Large-Scale Neural Domain Classiﬁcation with
               Personalized Attention",
  author    = "Kim, Young-Bum and Kim, Dongchan and Kumar, Anijishnu and
               Sarikaya, Ruhi",
  booktitle = "Proceedings of the 56th Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  pages     =  11,
  abstract  = "In this paper, we explore the task of mapping spoken language
               utterances to one of thousands of natural language understanding
               domains in intelligent personal digital assistants (IPDAs). This
               scenario is observed in mainstream IPDAs in industry that allow
               third parties to develop thousands of new domains to augment
               builtin ﬁrst party domains to rapidly increase domain coverage
               and overall IPDA capabilities. We propose a scalable neural model
               architecture with a shared encoder, a novel attention mechanism
               that incorporates personalization information and domain-speciﬁc
               classiﬁers that solves the problem efﬁciently. Our architecture
               is designed to efﬁciently accommodate incremental domain
               additions achieving two orders of magnitude speed up compared to
               full model retraining. We consider the practical constraints of
               real-time production systems, and design to minimize memory
               footprint and runtime latency. We demonstrate that incorporating
               personalization signiﬁcantly improves domain classiﬁcation
               accuracy in a setting with thousands of overlapping domains.",
  year      =  2018,
  language  = "en"
}

@INPROCEEDINGS{Kim2018-bm,
  title     = "A Scalable Neural Shortlisting-Reranking Approach for Large-Scale
               Domain Classification in Natural Language Understanding",
  author    = "Kim, Young-Bum and Kim, Dongchan and Kim, Joo-Kyung and Sarikaya,
               Ruhi",
  booktitle = "Proceedings of the 2018 Conference of the North American Chapter
               of the Association for Computational Linguistics: Human Language
               Technologies, Volume 3 (Industry Papers)",
  volume    =  3,
  pages     = "16--24",
  year      =  2018,
  doi       = "10.18653/v1/N18-3003",
  language  = "en"
}

@ARTICLE{Artetxe2017-ju,
  title    = "Unsupervised Neural Machine Translation",
  author   = "Artetxe, Mikel and Labaka, Gorka and Agirre, Eneko and Cho,
              Kyunghyun",
  journal  = "arXiv:1710.11041 [cs]",
  abstract = "In spite of the recent success of neural machine translation (NMT)
              in standard benchmarks, the lack of large parallel corpora poses a
              major practical problem for many language pairs. There have been
              several proposals to alleviate this issue with, for instance,
              triangulation and semi-supervised learning techniques, but they
              still require a strong cross-lingual signal. In this work, we
              completely remove the need of parallel data and propose a novel
              method to train an NMT system in a completely unsupervised manner,
              relying on nothing but monolingual corpora. Our model builds upon
              the recent work on unsupervised embedding mappings, and consists
              of a slightly modified attentional encoder-decoder model that can
              be trained on monolingual corpora alone using a combination of
              denoising and backtranslation. Despite the simplicity of the
              approach, our system obtains 15.56 and 10.21 BLEU points in WMT
              2014 French-to-English and German-to-English translation. The
              model can also profit from small parallel corpora, and attains
              21.81 and 15.24 points when combined with 100,000 parallel
              sentences, respectively. Our implementation is released as an open
              source project.",
  month    =  oct,
  year     =  2017
}

@ARTICLE{Niculae2017-zy,
  title    = "A Regularized Framework for Sparse and Structured Neural Attention",
  author   = "Niculae, Vlad and Blondel, Mathieu",
  journal  = "arXiv:1705.07704 [cs, stat]",
  abstract = "Modern neural networks are often augmented with an attention
              mechanism, which tells the network where to focus within the
              input. We propose in this paper a new framework for sparse and
              structured attention, building upon a smoothed max operator. We
              show that the gradient of this operator defines a mapping from
              real values to probabilities, suitable as an attention mechanism.
              Our framework includes softmax and a slight generalization of the
              recently-proposed sparsemax as special cases. However, we also
              show how our framework can incorporate modern structured
              penalties, resulting in more interpretable attention mechanisms,
              that focus on entire segments or groups of an input. We derive
              efficient algorithms to compute the forward and backward passes of
              our attention mechanisms, enabling their use in a neural network
              trained with backpropagation. To showcase their potential as a
              drop-in replacement for existing ones, we evaluate our attention
              mechanisms on three large-scale tasks: textual entailment, machine
              translation, and sentence summarization. Our attention mechanisms
              improve interpretability without sacrificing performance; notably,
              on textual entailment and summarization, we outperform the
              standard attention mechanisms based on softmax and sparsemax.",
  month    =  may,
  year     =  2017
}

@ARTICLE{Wu2018-fb,
  title    = "Group Normalization",
  author   = "Wu, Yuxin and He, Kaiming",
  journal  = "arXiv:1803.08494 [cs]",
  abstract = "Batch Normalization (BN) is a milestone technique in the
              development of deep learning, enabling various networks to train.
              However, normalizing along the batch dimension introduces problems
              --- BN's error increases rapidly when the batch size becomes
              smaller, caused by inaccurate batch statistics estimation. This
              limits BN's usage for training larger models and transferring
              features to computer vision tasks including detection,
              segmentation, and video, which require small batches constrained
              by memory consumption. In this paper, we present Group
              Normalization (GN) as a simple alternative to BN. GN divides the
              channels into groups and computes within each group the mean and
              variance for normalization. GN's computation is independent of
              batch sizes, and its accuracy is stable in a wide range of batch
              sizes. On ResNet-50 trained in ImageNet, GN has 10.6\% lower error
              than its BN counterpart when using a batch size of 2; when using
              typical batch sizes, GN is comparably good with BN and outperforms
              other normalization variants. Moreover, GN can be naturally
              transferred from pre-training to fine-tuning. GN can outperform
              its BN-based counterparts for object detection and segmentation in
              COCO, and for video classification in Kinetics, showing that GN
              can effectively replace the powerful BN in a variety of tasks. GN
              can be easily implemented by a few lines of code in modern
              libraries.",
  month    =  mar,
  year     =  2018
}

@INPROCEEDINGS{Resnik2015-mo,
  title     = "Beyond {LDA}: Exploring Supervised Topic Modeling for
               Depression-Related Language in Twitter",
  author    = "Resnik, Philip and Armstrong, William and Claudino, Leonardo and
               Nguyen, Thang and Nguyen, Viet-An and Boyd-Graber, Jordan",
  publisher = "Association for Computational Linguistics",
  pages     = "99--107",
  abstract  = "Topic models can yield insight into how depressed and
               non-depressed individuals use language differently. In this
               paper, we explore the use of supervised topic models in the
               analysis of linguistic signal for detecting depression, providing
               promising results using several models.",
  year      =  2015,
  doi       = "10.3115/v1/W15-1212",
  language  = "en"
}

@TECHREPORT{Pennebaker2015-be,
  title    = "The Development and Psychometric Properties of {LIWC2015}",
  author   = "Pennebaker, James W and Boyd, Ryan L and Jordan, Kayla and
              Blackburn, Kate",
  pages    =  26,
  year     =  2015,
  language = "en"
}

@INPROCEEDINGS{Perez-Rosas2017-kq,
  title     = "Understanding and Predicting Empathic Behavior in Counseling
               Therapy",
  author    = "Pérez-Rosas, Verónica and Mihalcea, Rada and Resnicow, Kenneth
               and Singh, Satinder and An, Lawrence",
  publisher = "Association for Computational Linguistics",
  pages     = "1426--1435",
  abstract  = "Counselor empathy is associated with better outcomes in
               psychology and behavioral counseling. In this paper, we explore
               several aspects pertaining to counseling interaction dynamics and
               their relation to counselor empathy during motivational
               interviewing encounters. Particularly, we analyze aspects such as
               participants’ engagement, participants’ verbal and nonverbal
               accommodation, as well as topics being discussed during the
               conversation, with the ﬁnal goal of identifying linguistic and
               acoustic markers of counselor empathy. We also show how we can
               use these ﬁndings alongside other raw linguistic and acoustic
               features to build accurate counselor empathy classiﬁers with
               accuracies of up to 80\%.",
  year      =  2017,
  doi       = "10.18653/v1/P17-1131",
  language  = "en"
}

@ARTICLE{Conneau2018-hl,
  title    = "What you can cram into a single vector: Probing sentence
              embeddings for linguistic properties",
  author   = "Conneau, Alexis and Kruszewski, German and Lample, Guillaume and
              Barrault, Loïc and Baroni, Marco",
  journal  = "arXiv:1805.01070 [cs]",
  abstract = "Although much effort has recently been devoted to training
              high-quality sentence embeddings, we still have a poor
              understanding of what they are capturing. ``Downstream'' tasks,
              often based on sentence classification, are commonly used to
              evaluate the quality of sentence representations. The complexity
              of the tasks makes it however difficult to infer what kind of
              information is present in the representations. We introduce here
              10 probing tasks designed to capture simple linguistic features of
              sentences, and we use them to study embeddings generated by three
              different encoders trained in eight distinct ways, uncovering
              intriguing properties of both encoders and training methods.",
  month    =  may,
  year     =  2018
}

@INPROCEEDINGS{Kumar2015-kp,
  title     = "Detecting Changes in Suicide Content Manifested in Social Media
               Following Celebrity Suicides",
  author    = "Kumar, Mrinal and Dredze, Mark and Coppersmith, Glen and De
               Choudhury, Munmun",
  publisher = "ACM Press",
  pages     = "85--94",
  abstract  = "The Werther effect describes the increased rate of completed or
               attempted suicides following the depiction of an individual’s
               suicide in the media, typically a celebrity. We present ﬁndings
               on the prevalence of this effect in an online platform:
               r/SuicideWatch on Reddit. We examine both the posting activity
               and post content after the death of ten high-proﬁle suicides.
               Posting activity increases following reports of celebrity
               suicides, and post content exhibits considerable changes that
               indicate increased suicidal ideation. Specifically, we observe
               that post-celebrity suicide content is more likely to be inward
               focused, manifest decreased social concerns, and laden with
               greater anxiety, anger, and negative emotion. Topic model
               analysis further reveals content in this period to switch to a
               more derogatory tone that bears evidence of self-harm and
               suicidal tendencies. We discuss the implications of our ﬁndings
               in enabling better community support to psychologically
               vulnerable populations, and the potential of building suicide
               prevention interventions following high-proﬁle suicides.",
  year      =  2015,
  doi       = "10.1145/2700171.2791026",
  isbn      =  9781450333955,
  language  = "en"
}

@INPROCEEDINGS{Milne2016-ss,
  title     = "{CLPsych} {2016} Shared Task: Triaging content in online
               peer-support forums",
  author    = "Milne, David N and Pink, Glen and Hachey, Ben and Calvo, Rafael A",
  booktitle = "Proceedings of the Third Workshop on Computational Linguistics
               and Clinical Psychology",
  publisher = "Association for Computational Linguistics",
  address   = "San Diego, CA, USA",
  pages     = "118–127",
  month     =  jun,
  year      =  2016
}

@INPROCEEDINGS{Khanpour2017-ms,
  title     = "Identifying Empathetic Messages in Online Health Communities",
  author    = "Khanpour, Hamed and Caragea, Cornelia and Biyani, Prakhar",
  booktitle = "Proceedings of the Eighth International Joint Conference on
               Natural Language Processing (Volume 2: Short Papers)",
  publisher = "Asian Federation of Natural Language Processing",
  address   = "Taipei, Taiwan",
  pages     = "246–251",
  abstract  = "Empathy captures one’s ability to correlate with and understand
               others’ emotional states and experiences. Messages with
               empathetic content are considered as one of the main advantages
               for joining online health communities due to their potential to
               improve people’s moods. Unfortunately, to this date, no
               computational studies exist that automatically identify
               empathetic messages in online health communities. We propose a
               combination of Convolutional Neural Networks (CNN) and Long Short
               Term Memory (LSTM) networks, and show that the proposed model
               outperforms each individual model (CNN and LSTM) as well as
               several baselines.",
  month     =  nov,
  year      =  2017
}

@INPROCEEDINGS{De_Choudhury2016-ag,
  title     = "Discovering Shifts to Suicidal Ideation from Mental Health
               Content in Social Media",
  author    = "De Choudhury, Munmun and Kiciman, Emre and Dredze, Mark and
               Coppersmith, Glen and Kumar, Mrinal",
  publisher = "ACM Press",
  pages     = "2098--2110",
  abstract  = "History of mental illness is a major factor behind suicide risk
               and ideation. However research efforts toward characterizing and
               forecasting this risk is limited due to the paucity of
               information regarding suicide ideation, exacerbated by the stigma
               of mental illness. This paper ﬁlls gaps in the literature by
               developing a statistical methodology to infer which individuals
               could undergo transitions from mental health discourse to
               suicidal ideation. We utilize semi-anonymous support communities
               on Reddit as unobtrusive data sources to infer the likelihood of
               these shifts. We develop language and interactional measures for
               this purpose, as well as a propensity score matching based
               statistical approach. Our approach allows us to derive distinct
               markers of shifts to suicidal ideation. These markers can be
               modeled in a prediction framework to identify individuals likely
               to engage in suicidal ideation in the future. We discuss societal
               and ethical implications of this research.",
  year      =  2016,
  doi       = "10.1145/2858036.2858207",
  isbn      =  9781450333627,
  language  = "en"
}

@ARTICLE{Choudhury2017-vb,
  title    = "The Language of Social Support in Social Media and its Effect on
              Suicidal Ideation Risk",
  author   = "Choudhury, Munmun De and Kiciman, Emre",
  journal  = "Microsoft Research",
  abstract = "Online social support is known to play a signiﬁcant role in mental
              well-being. However, current research is limited in its ability to
              quantify this link. Challenges exist due to the paucity of
              longitudinal, pre- and post-mental illness risk data, and reliable
              methods that can examine causality between past availability of
              support and future risk. In …",
  month    =  mar,
  year     =  2017,
  language = "en-US"
}

@ARTICLE{Choudhury2013-rp,
  title    = "Predicting Depression via Social Media",
  author   = "Choudhury, Munmun De and Gamon, Michael and Counts, Scott and
              Horvitz, Eric",
  journal  = "Microsoft Research",
  abstract = "Major depression constitutes a serious challenge in personal and
              public health. Tens of millions of people each year suffer from
              depression and only a fraction receives adequate treatment. We
              explore the potential to use social media to detect and diagnose
              major depressive disorder in individuals. We first employ
              crowdsourcing to compile a set of Twitter …",
  month    =  jul,
  year     =  2013,
  language = "en-US"
}

@INPROCEEDINGS{Coppersmith2016-lf,
  title     = "Exploratory Analysis of Social Media Prior to a Suicide Attempt",
  author    = "Coppersmith, Glen and Ngo, Kim and Leary, Ryan and Wood, Anthony",
  booktitle = "Proceedings of the Third Workshop on Computational Linguistics
               and Clinical Psychology",
  publisher = "Association for Computational Linguistics",
  address   = "San Diego, CA, USA",
  pages     = "106–117",
  month     =  jun,
  year      =  2016
}

@INPROCEEDINGS{Coppersmith2015-qg,
  title     = "Quantifying suicidal ideation via language usage on social media",
  author    = "Coppersmith, Glen and Leary, Ryan and Whyne, Eric and Wood, Tony",
  booktitle = "Joint Statistics Meetings Proceedings, Statistical Computing
               Section, JSM",
  year      =  2015
}

@INPROCEEDINGS{Yates2017-ku,
  title     = "Depression and Self-Harm Risk Assessment in Online Forums",
  author    = "Yates, Andrew and Cohan, Arman and Goharian, Nazli",
  booktitle = "Proceedings of the 2017 Conference on Empirical Methods in
               Natural Language Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Copenhagen, Denmark",
  pages     = "2968–2978",
  abstract  = "Users suffering from mental health conditions often turn to
               online resources for support, including specialized online
               support communities or general communities such as Twitter and
               Reddit. In this work, we present a framework for supporting and
               studying users in both types of communities. We propose methods
               for identifying posts in support communities that may indicate a
               risk of self-harm, and demonstrate that our approach outperforms
               strong previously proposed methods for identifying such posts.
               Self-harm is closely related to depression, which makes
               identifying depressed users on general forums a crucial related
               task. We introduce a large-scale general forum dataset consisting
               of users with self-reported depression diagnoses matched with
               control users. We show how our method can be applied to
               effectively identify depressed users from their use of language
               alone. We demonstrate that our method outperforms strong
               baselines on this general forum dataset.",
  month     =  sep,
  year      =  2017
}

@INPROCEEDINGS{Coppersmith2017-wf,
  title     = "Scalable mental health analysis in the clinical whitespace via
               natural language processing",
  author    = "Coppersmith, G and Hilland, C and Frieder, O and Leary, R",
  booktitle = "2017 IEEE EMBS International Conference on Biomedical Health
               Informatics (BHI)",
  pages     = "393--396",
  abstract  = "Our increasingly digital life provides a wealth of data about our
               behavior, beliefs, mood, and well-being. This data provides some
               insight into the lives of patients outside the healthcare
               setting, and in aggregate can be insightful for the person's
               mental health and emotional crisis. Here, we introduce this
               community to some of the recent advancement in using natural
               language processing and machine learning to provide insight into
               mental health of both individuals and populations. We advocate
               using these linguistic signals as a supplement to those that are
               collected in the health care system, filling in some of the
               so-called “whitespace” between visits.",
  month     =  feb,
  year      =  2017,
  doi       = "10.1109/BHI.2017.7897288"
}

@ARTICLE{Cohan2017-xc,
  title    = "Triaging content severity in online mental health forums",
  author   = "Cohan, Arman and Young, Sydney and Yates, Andrew and Goharian,
              Nazli",
  journal  = "Journal of the Association for Information Science and Technology",
  volume   =  68,
  number   =  11,
  pages    = "2675--2689",
  year     =  2017,
  doi      = "10.1002/asi.23865",
  issn     = "2330-1635",
  language = "en"
}

@INPROCEEDINGS{Soldaini2015-fi,
  title     = "Query Reformulation for Clinical Decision Support Search",
  author    = "Soldaini, Luca and Cohan, Arman and Yates, Andrew and Goharian,
               Nazli and Frieder, Ophir",
  booktitle = "The Twenty-Third Text REtrieval Conference Proceedings (TREC
               2014)",
  year      =  2015
}

@INPROCEEDINGS{Cohan2014-gp,
  title     = "On clinical decision support",
  author    = "Cohan, Arman and Soldaini, Luca and Yates, Andrew and Goharian,
               Nazli and Frieder, Ophir",
  publisher = "ACM Press",
  pages     = "651--652",
  abstract  = "Recent interest in search tools for Clinical Decision Support
               (CDS) has dramatically increased. These tools help clinicians
               assess a medical situation by providing actionable information in
               the form of a select few highly relevant recent medical papers.
               Unlike traditional search, which is designed to deal with short
               queries, queries in CDS are long and narrative. We investigate
               the utility of applying pseudo-relevance feedback (PRF), a query
               expansion method that performs well in keyword-based medical
               literature search to CDS search. Using the optimum combination of
               PRF parameters we obtained statistically significant retrieval
               efficiency improvement in terms of nDCG, over the baseline.",
  year      =  2014,
  doi       = "10.1145/2649387.2660820",
  isbn      =  9781450328944,
  language  = "en"
}

@INPROCEEDINGS{Cohan2014-kb,
  title     = "Towards Citation-Based Summarization of Biomedical Literature",
  author    = "Cohan, Arman and Soldaini, Luca and Mengle, Saket S R and
               Goharian, Nazli",
  booktitle = "Text Analysis Conference Proceedings (TAC 2014)",
  pages     =  8,
  abstract  = "Citation-based summarization is a form of technical summarization
               that uses citations to an article to form its summary. In
               biomedical literature, citations by themselves are not reliable
               to be used for summary as they fail to consider the context of
               the ﬁndings in the referenced article. One way to remedy such
               problem is to link citations to the related text spans in the
               reference article. The ultimate goal in TAC1 biomedical
               summarization track is to generate a citation-based summary,
               using both the citations and the context information. This paper
               describes our approach for ﬁnding the context information related
               to each citation and determining their discourse facet (Task 1 of
               the track). We approach this task as a search task, applying
               different query reformulation techniques for retrieving the
               relevant text spans. After ﬁnding the relevant spans, we classify
               each citation to a set of discourse facets to capture the
               structure of the referenced paper. While our results show 20\%
               improvement over the baseline, the efﬁciency of the system still
               leaves much room for improvement.",
  year      =  2014,
  language  = "en"
}

@INPROCEEDINGS{Soldaini2015-va,
  title     = "Retrieving Medical Literature for Clinical Decision Support",
  author    = "Soldaini, Luca and Cohan, Arman and Yates, Andrew and Goharian,
               Nazli and Frieder, Ophir",
  booktitle = "Advances in Information Retrieval",
  publisher = "Springer, Cham",
  pages     = "538--549",
  abstract  = "Keeping current given the vast volume of medical literature
               published yearly poses a serious challenge for medical
               professionals. Thus, interest in systems that aid physicians in
               making clinical decisions is intensifying. A task of Clinical
               Decision Support (CDS) systems is retrieving highly relevant
               medical literature that could help healthcare professionals in
               formulating diagnoses or determining treatments. This search task
               is atypical as the queries are medical case reports, which
               differs in terms of size and structure from queries in other,
               more common search tasks. We apply query reformulation techniques
               to address literature search based on case reports. The proposed
               system achieves a statistically significant improvement over the
               baseline (29\% – 32\%) and the state-of-the-art (12\% – 59\%).",
  series    = "Lecture Notes in Computer Science",
  month     =  mar,
  year      =  2015,
  doi       = "10.1007/978-3-319-16354-3\_59",
  isbn      = "9783319163536,9783319163543",
  language  = "en"
}

@INPROCEEDINGS{Cohan2015-cb,
  title     = "Matching Citation Text and Cited Spans in Biomedical Literature:
               a Search-Oriented Approach",
  author    = "Cohan, Arman and Soldaini, Luca and Goharian, Nazli",
  booktitle = "Proceedings of the 2015 Conference of the North American Chapter
               of the Association for Computational Linguistics: Human Language
               Technologies",
  publisher = "Association for Computational Linguistics",
  address   = "Denver, Colorado",
  pages     = "1042–1048",
  year      =  2015
}

@ARTICLE{Soldaini2016-dl,
  title    = "Enhancing web search in the medical domain via query clarification",
  author   = "Soldaini, Luca and Yates, Andrew and Yom-Tov, Elad and Frieder,
              Ophir and Goharian, Nazli",
  journal  = "Information Retrieval Journal",
  volume   =  19,
  number   = "1-2",
  pages    = "149--173",
  abstract = "The majority of Internet users search for medical information
              online; however, many do not have an adequate medical vocabulary.
              Users might have diﬃculties ﬁnding the most authoritative and
              useful information because they are unfamiliar with the
              appropriate medical expressions describing their condition;
              consequently, they are unable to adequately satisfy their
              information need. We investigate the utility of bridging the gap
              between layperson and expert vocabularies; our approach adds the
              most appropriate expert expression to queries submitted by users,
              a task we call query clariﬁcation. We evaluated the impact of
              query clariﬁcation. Using three diﬀerent synonym mappings and
              conducting two task-based retrieval studies, users were asked to
              answer medically-related questions using interleaved results from
              a major search engine. Our results show that the proposed system
              was preferred by users and helped them answer medical concerns
              correctly more often, with up to a 7\% increase in correct answers
              over an unmodiﬁed query. Finally, we introduce a supervised
              classiﬁer to select the most appropriate synonym mapping for each
              query, which further increased the fraction of correct answers
              (12\%).",
  year     =  2016,
  doi      = "10.1007/s10791-015-9258-y",
  issn     = "1386-4564,1573-7659",
  language = "en"
}

@INPROCEEDINGS{Cohan2016-gm,
  title     = "Identifying Signiﬁcance of Discrepancies in Radiology Reports",
  author    = "Cohan, Arman and Soldaini, Luca and Fong, Allan and Filice, Ross
               and Goharian, Nazli and Ratwani, Raj",
  booktitle = "Workshop on Data Mining for Medicine and Healthcare",
  pages     =  8,
  abstract  = "At many teaching hospitals, it is common practice for on-call
               radiology residents to interpret radiology examinations; such
               reports are later reviewed and revised by an attending physician
               before being used for any decision making. In case there are
               substantial problems in the resident’s initial report, the
               resident is called and the problems are reviewed to prevent
               similar future reporting errors. However, due to the large volume
               of reports produced, attending physicians rarely discuss the
               problems side by side with residents, thus missing an educational
               opportunity. In this work, we introduce a pipeline to
               discriminate between reports with signiﬁcant discrepancies and
               those with non-signiﬁcant discrepancies. The former contain
               severe errors or mis-interpretations, thus representing a great
               learning opportunity for the resident; the latter presents only
               minor diﬀerences (often stylistic) and have a minor role in the
               education of a resident. By discriminating between the two, the
               proposed system could ﬂag those reports that an attending
               radiology should deﬁnitely review with residents under their
               supervision. We evaluated our approach on 350 manually annotated
               radiology reports sampled from a collection of tens of thousands.
               The proposed classiﬁer achieves an Area Under the Curve (AUC) of
               0.837, which represent a 14\% improvement over the baselines.
               Furthermore, the classiﬁer reduces the False Negative Rate (FNR)
               by 52\%, a desirable performance metric for any recall-oriented
               task such as the one studied in this work.",
  year      =  2016,
  language  = "en"
}

@INPROCEEDINGS{Soldaini2017-xx,
  title     = "Inferring Individual Attributes from Search Engine Queries and
               Auxiliary Information",
  author    = "Soldaini, Luca and Yom-Tov, Elad",
  publisher = "ACM Press",
  pages     = "293--301",
  abstract  = "Internet data has surfaced as a primary source for investigation
               of diﬀerent aspects of human behavior. A crucial step in such
               studies is ﬁnding a suitable cohort (i.e., a set of users) that
               shares a common trait of interest to researchers. However, direct
               identiﬁcation of users sharing this trait is often impossible, as
               the data available to researchers is usually anonymized to
               preserve user privacy. To facilitate research on speciﬁc topics
               of interest, especially in medicine, we introduce an algorithm
               for identifying a trait of interest in anonymous users. We
               illustrate how a small set of labeled examples, together with
               statistical information about the entire population, can be
               aggregated to obtain labels on unseen examples. We validate our
               approach using labeled data from the political domain.",
  year      =  2017,
  doi       = "10.1145/3038912.3052629",
  isbn      =  9781450349130,
  language  = "en"
}

@INPROCEEDINGS{Soldaini2016-bf,
  title     = "{QuickUMLS}: a fast, unsupervised approach for medical concept
               extraction",
  author    = "Soldaini, Luca and Goharian, Nazli",
  booktitle = "Proceedings of the MedIR Workshop",
  pages     =  4,
  abstract  = "Entity extraction is a fundamental step in many health
               informatics systems. In recent years, tools such as MetaMap and
               cTAKES have been widely used for medical concept extraction on
               medical literature and clinical notes; however, relatively little
               interest has been placed on their scalability to large datasets.
               In this work, we present QuickUMLS: a fast, unsupervised,
               approximate dictionary matching algorithm for medical concept
               extraction. The proposed method achieves similar precision and
               recall of state-of-the-art systems on two clinical notes corpora,
               and outperforms MetaMap and cTAKES on a dataset of consumer drug
               reviews. More importantly, it is up to 135 times faster than both
               systems.",
  year      =  2016,
  language  = "en"
}

@ARTICLE{Soldaini2017-rl,
  title    = "Learning to reformulate long queries for clinical decision support",
  author   = "Soldaini, Luca and Yates, Andrew and Goharian, Nazli",
  journal  = "Journal of the Association for Information Science and Technology",
  volume   =  68,
  number   =  11,
  pages    = "2602--2619",
  abstract = "The large volume of biomedical literature poses a serious problem
              for medical professionals, who are often struggling to keep
              current with it. At the same time, many health providers consider
              knowledge of the latest literature in their ﬁeld a key component
              for successful clinical practice. In this work, we introduce two
              systems designed to help retrieving medical literature. Both
              receive a long, discursive clinical note as input query, and
              return highly relevant literature that could be used in support of
              clinical practice. The ﬁrst system is an improved version of a
              method previously proposed by the authors; it combines pseudo
              relevance feedback and a domain speciﬁc term ﬁlter to reformulate
              the query. The second is an approach that uses a deep neural
              network to reformulate a clinical note. Both approaches were
              evaluated on the 2014 and 2015 TREC CDS datasets; in our tests,
              they outperform the previously proposed method by up to 28\% in
              inferred NDCG; furthermore, they are competitive with the state of
              the art, achieving up to 8\% improvement in inferred NDCG.",
  year     =  2017,
  doi      = "10.1002/asi.23924",
  issn     = "2330-1635",
  language = "en"
}

@INPROCEEDINGS{Soldaini2017-ty,
  title     = "Denoising Clinical Notes for Medical Literature Retrieval with
               Convolutional Neural Model",
  author    = "Soldaini, Luca and Yates, Andrew and Goharian, Nazli",
  publisher = "ACM Press",
  pages     = "2307--2310",
  year      =  2017,
  doi       = "10.1145/3132847.3133149",
  isbn      =  9781450349185,
  language  = "en"
}

@ARTICLE{Su2018-sc,
  title         = "A Re-ranker Scheme for Integrating Large Scale {NLU} models",
  author        = "Su, Chengwei and Gupta, Rahul and Ananthakrishnan, Shankar
                   and Matsoukas, Spyros",
  abstract      = "Large scale Natural Language Understanding (NLU) systems are
                   typically trained on large quantities of data, requiring a
                   fast and scalable training strategy. A typical design for NLU
                   systems consists of domain-level NLU modules (domain
                   classifier, intent classifier and named entity recognizer).
                   Hypotheses (NLU interpretations consisting of various
                   intent+slot combinations) from these domain specific modules
                   are typically aggregated with another downstream component.
                   The re-ranker integrates outputs from domain-level
                   recognizers, returning a scored list of cross domain
                   hypotheses. An ideal re-ranker will exhibit the following two
                   properties: (a) it should prefer the most relevant hypothesis
                   for the given input as the top hypothesis and, (b) the
                   interpretation scores corresponding to each hypothesis
                   produced by the re-ranker should be calibrated. Calibration
                   allows the final NLU interpretation score to be comparable
                   across domains. We propose a novel re-ranker strategy that
                   addresses these aspects, while also maintaining domain
                   specific modularity. We design optimization loss functions
                   for such a modularized re-ranker and present results on
                   decreasing the top hypothesis error rate as well as
                   maintaining the model calibration. We also experiment with an
                   extension involving training the domain specific re-rankers
                   on datasets curated independently by each domain to allow
                   further asynchronization. \%The proposed re-ranker design
                   showcases the following: (i) improved NLU performance over an
                   unweighted aggregation strategy, (ii) cross-domain calibrated
                   performance and, (iii) support for use cases involving
                   training each re-ranker on datasets curated by each domain
                   independently.",
  month         =  sep,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1809.09605",
  language      = "en"
}

@INPROCEEDINGS{Wang2018-fw,
  title     = "Key Terms Guided Expansion for Verbose Queries in Medical Domain",
  author    = "Wang, Yue and Fang, Hui",
  booktitle = "Information Retrieval Technology",
  publisher = "Springer International Publishing",
  pages     = "143--156",
  abstract  = "Due to the complex nature of medical concepts and information
               need, the queries tend to be verbose in medical domain. Verbose
               queries lead to sub-optimal performance since the current search
               engine promotes the results covering every query term, but not
               the truly important ones. Key term extraction has been studied to
               solve this problem, but another problem, i.e., vocabulary gap
               between query and documents, need to be discussed. Although
               various query expansion techniques have been well studied for the
               vocabulary gap problem, existing methods suffer different
               drawbacks such as inefficiency and expansion term mismatch. In
               this work, we propose to solve this problem by following the
               intuition that the surrounding contexts of the important terms in
               the original query should also be essential for retrieval.
               Specifically, we first identify the key terms from the verbose
               query and then locate the contexts of these key terms in the
               original document collection. The terms in the contexts are
               weighted and aggregated to select the expansion terms. We conduct
               experiments with five TREC data collections using the proposed
               methods. The results show that the improvement of the retrieval
               performance of proposed method is statistically significant
               comparing with the baseline methods.",
  year      =  2018,
  doi       = "10.1007/978-3-030-03520-4\_14"
}

@ARTICLE{Mikolov2017-uj,
  title         = "Advances in Pre-Training Distributed Word Representations",
  author        = "Mikolov, Tomas and Grave, Edouard and Bojanowski, Piotr and
                   Puhrsch, Christian and Joulin, Armand",
  abstract      = "Many Natural Language Processing applications nowadays rely
                   on pre-trained word representations estimated from large text
                   corpora such as news collections, Wikipedia and Web Crawl. In
                   this paper, we show how to train high-quality word vector
                   representations by using a combination of known tricks that
                   are however rarely used together. The main result of our work
                   is the new set of publicly available pre-trained models that
                   outperform the current state of the art by a large margin on
                   a number of tasks.",
  month         =  dec,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1712.09405"
}

@ARTICLE{Johnson2016-bw,
  title         = "Google's Multilingual Neural Machine Translation System:
                   Enabling Zero-Shot Translation",
  author        = "Johnson, Melvin and Schuster, Mike and Le, Quoc V and Krikun,
                   Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat, Nikhil
                   and Viégas, Fernanda and Wattenberg, Martin and Corrado, Greg
                   and Hughes, Macduff and Dean, Jeffrey",
  abstract      = "We propose a simple solution to use a single Neural Machine
                   Translation (NMT) model to translate between multiple
                   languages. Our solution requires no change in the model
                   architecture from our base system but instead introduces an
                   artificial token at the beginning of the input sentence to
                   specify the required target language. The rest of the model,
                   which includes encoder, decoder and attention, remains
                   unchanged and is shared across all languages. Using a shared
                   wordpiece vocabulary, our approach enables Multilingual NMT
                   using a single model without any increase in parameters,
                   which is significantly simpler than previous proposals for
                   Multilingual NMT. Our method often improves the translation
                   quality of all involved language pairs, even while keeping
                   the total number of model parameters constant. On the WMT'14
                   benchmarks, a single multilingual model achieves comparable
                   performance for English$\rightarrow$French and surpasses
                   state-of-the-art results for English$\rightarrow$German.
                   Similarly, a single multilingual model surpasses
                   state-of-the-art results for French$\rightarrow$English and
                   German$\rightarrow$English on WMT'14 and WMT'15 benchmarks
                   respectively. On production corpora, multilingual models of
                   up to twelve language pairs allow for better translation of
                   many individual pairs. In addition to improving the
                   translation quality of language pairs that the model was
                   trained with, our models can also learn to perform implicit
                   bridging between language pairs never seen explicitly during
                   training, showing that transfer learning and zero-shot
                   translation is possible for neural translation. Finally, we
                   show analyses that hints at a universal interlingua
                   representation in our models and show some interesting
                   examples when mixing languages.",
  month         =  nov,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1611.04558"
}

@ARTICLE{Devlin2018-hk,
  title         = "{BERT}: Pre-training of Deep Bidirectional Transformers for
                   Language Understanding",
  author        = "Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and
                   Toutanova, Kristina",
  abstract      = "We introduce a new language representation model called BERT,
                   which stands for Bidirectional Encoder Representations from
                   Transformers. Unlike recent language representation models,
                   BERT is designed to pre-train deep bidirectional
                   representations by jointly conditioning on both left and
                   right context in all layers. As a result, the pre-trained
                   BERT representations can be fine-tuned with just one
                   additional output layer to create state-of-the-art models for
                   a wide range of tasks, such as question answering and
                   language inference, without substantial task-specific
                   architecture modifications. BERT is conceptually simple and
                   empirically powerful. It obtains new state-of-the-art results
                   on eleven natural language processing tasks, including
                   pushing the GLUE benchmark to 80.4\% (7.6\% absolute
                   improvement), MultiNLI accuracy to 86.7 (5.6\% absolute
                   improvement) and the SQuAD v1.1 question answering Test F1 to
                   93.2 (1.5\% absolute improvement), outperforming human
                   performance by 2.0\%.",
  month         =  oct,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1810.04805"
}

@ARTICLE{Radford2018-hz,
  title   = "Improving language understanding by generative pre-training",
  author  = "Radford, Alec and Narasimhan, Karthik and Salimans, Tim and
             Sutskever, Ilya",
  journal = "URL https://s3-us-west-2. amazonaws.
             com/openai-assets/research-covers/language-unsupervised/language\_
             understanding\_paper. pdf",
  year    =  2018
}

@ARTICLE{Hui2017-fu,
  title         = "{PACRR}: A Position-Aware Neural {IR} Model for Relevance
                   Matching",
  author        = "Hui, Kai and Yates, Andrew and Berberich, Klaus and de Melo,
                   Gerard",
  abstract      = "In order to adopt deep learning for information retrieval,
                   models are needed that can capture all relevant information
                   required to assess the relevance of a document to a given
                   user query. While previous works have successfully captured
                   unigram term matches, how to fully employ position-dependent
                   information such as proximity and term dependencies has been
                   insufficiently explored. In this work, we propose a novel
                   neural IR model named PACRR aiming at better modeling
                   position-dependent interactions between a query and a
                   document. Extensive experiments on six years' TREC Web Track
                   data confirm that the proposed model yields better results
                   under multiple benchmarks.",
  month         =  apr,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "1704.03940"
}

@ARTICLE{Hui2017-de,
  title         = "Co-{PACRR}: A Context-Aware Neural {IR} Model for Ad-hoc
                   Retrieval",
  author        = "Hui, Kai and Yates, Andrew and Berberich, Klaus and de Melo,
                   Gerard",
  abstract      = "Neural IR models, such as DRMM and PACRR, have achieved
                   strong results by successfully capturing relevance matching
                   signals. We argue that the context of these matching signals
                   is also important. Intuitively, when extracting, modeling,
                   and combining matching signals, one would like to consider
                   the surrounding text (local context) as well as other signals
                   from the same document that can contribute to the overall
                   relevance score. In this work, we highlight three potential
                   shortcomings caused by not considering context information
                   and propose three neural ingredients to address them: a
                   disambiguation component, cascade k-max pooling, and a
                   shuffling combination layer. Incorporating these components
                   into the PACRR model yields Co-PACRR, a novel context-aware
                   neural IR model. Extensive comparisons with established
                   models on Trec Web Track data confirm that the proposed model
                   can achieve superior search results. In addition, an ablation
                   analysis is conducted to gain insights into the impact of and
                   interactions between different components. We release our
                   code to enable future comparisons.",
  month         =  jun,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "1706.10192",
  doi           = "10.1145/nnnnnnn.nnnnnnn"
}

@INPROCEEDINGS{Fan2018-lp,
  title     = "Relation Extraction for Protein-protein Interactions Affected by
               Mutations",
  author    = "Fan, Ziling and Soldaini, Luca and Cohan, Arman and Goharian,
               Nazli",
  booktitle = "Proceedings of the 2018 ACM International Conference on
               Bioinformatics, Computational Biology, and Health Informatics",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "506--507",
  series    = "BCB '18",
  year      =  2018,
  keywords  = "information extraction, mutation detection, precision medicine,
               protein-protein interaction",
  doi       = "10.1145/3233547.3233617",
  isbn      =  9781450357944
}

@ARTICLE{Cohan2018-tv,
  title   = "{SMHD}: a Large-Scale Resource for Exploring Online Language Usage
             for Multiple Mental Health Conditions",
  author  = "Cohan, Arman and Desmet, Bart and Yates, Andrew and Soldaini, Luca
             and MacAvaney, Sean and Goharian, Nazli",
  journal = "Proceedings of the 27th International Conference on Computational
             Linguistics",
  pages   = "1485--1497",
  year    =  2018
}

@ARTICLE{MacAvaney2018-dn,
  title         = "{RSDD}-Time: Temporal Annotation of Self-Reported Mental
                   Health Diagnoses",
  author        = "MacAvaney, Sean and Desmet, Bart and Cohan, Arman and
                   Soldaini, Luca and Yates, Andrew and Zirikly, Ayah and
                   Goharian, Nazli",
  abstract      = "Self-reported diagnosis statements have been widely employed
                   in studying language related to mental health in social
                   media. However, existing research has largely ignored the
                   temporality of mental health diagnoses. In this work, we
                   introduce RSDD-Time: a new dataset of 598 manually annotated
                   self-reported depression diagnosis posts from Reddit that
                   include temporal information about the diagnosis. Annotations
                   include whether a mental health condition is present and how
                   recently the diagnosis happened. Furthermore, we include
                   exact temporal spans that relate to the date of diagnosis.
                   This information is valuable for various computational
                   methods to examine mental health through social media because
                   one's mental health state is not static. We also test several
                   baseline classification and extraction approaches, which
                   suggest that extracting temporal information from
                   self-reported diagnosis statements is challenging.",
  month         =  jun,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1806.07916"
}

@MISC{Kurita_Keita2018-yk,
  title        = "An Overview of Normalization Methods in Deep Learning",
  author       = "{Kurita, Keita}",
  booktitle    = "Machine Learning Explained",
  abstract     = "Normalization in deep learning has always been a hot topic.
                  Getting normalization right can be a crucial factor in getting
                  your model to train effectively, but this isn’t as easy as it
                  sounds…",
  month        =  nov,
  year         =  2018,
  howpublished = "\url{http://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/}",
  note         = "Accessed: 2018-12-5"
}

@ARTICLE{Thompson2018-da,
  title         = "Relevant Word Order Vectorization for Improved Natural
                   Language Processing in Electronic Healthcare Records",
  author        = "Thompson, Jeffrey and Hu, Jinxiang and Mudaranthakam, Dinesh
                   Pal and Streeter, David and Neums, Lisa and Park, Michele and
                   Koestler, Devin C and Gajewski, Byron and Mayo, Matthew S",
  abstract      = "Objective: Electronic health records (EHR) represent a rich
                   resource for conducting observational studies, supporting
                   clinical trials, and more. However, much of the relevant
                   information is stored in an unstructured format that makes it
                   difficult to use. Natural language processing approaches that
                   attempt to automatically classify the data depend on
                   vectorization algorithms that impose structure on the text,
                   but these algorithms were not designed for the unique
                   characteristics of EHR. Here, we propose a new algorithm for
                   structuring so-called free-text that may help researchers
                   make better use of EHR. We call this method Relevant Word
                   Order Vectorization (RWOV). Materials and Methods: As a
                   proof-of-concept, we attempted to classify the hormone
                   receptor status of breast cancer patients treated at the
                   University of Kansas Medical Center during a recent year,
                   from the unstructured text of pathology reports. Our approach
                   attempts to account for the semi-structured way that
                   healthcare providers often enter information. We compared
                   this approach to the ngrams and word2vec methods. Results:
                   Our approach resulted in the most consistently high accuracy,
                   as measured by F1 score and area under the receiver operating
                   characteristic curve (AUC). Discussion: Our results suggest
                   that methods of structuring free text that take into account
                   its context may show better performance, and that our
                   approach is promising. Conclusion: By using a method that
                   accounts for the fact that healthcare providers tend to use
                   certain key words repetitively and that the order of these
                   key words is important, we showed improved performance over
                   methods that do not.",
  month         =  dec,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1812.02627"
}

@ARTICLE{Shang2017-gf,
  title         = "Automated Phrase Mining from Massive Text Corpora",
  author        = "Shang, Jingbo and Liu, Jialu and Jiang, Meng and Ren, Xiang
                   and Voss, Clare R and Han, Jiawei",
  abstract      = "As one of the fundamental tasks in text analysis, phrase
                   mining aims at extracting quality phrases from a text corpus.
                   Phrase mining is important in various tasks such as
                   information extraction/retrieval, taxonomy construction, and
                   topic modeling. Most existing methods rely on complex,
                   trained linguistic analyzers, and thus likely have
                   unsatisfactory performance on text corpora of new domains and
                   genres without extra but expensive adaption. Recently, a few
                   data-driven methods have been developed successfully for
                   extraction of phrases from massive domain-specific text.
                   However, none of the state-of-the-art models is fully
                   automated because they require human experts for designing
                   rules or labeling phrases. Since one can easily obtain many
                   quality phrases from public knowledge bases to a scale that
                   is much larger than that produced by human experts, in this
                   paper, we propose a novel framework for automated phrase
                   mining, AutoPhrase, which leverages this large amount of
                   high-quality phrases in an effective way and achieves better
                   performance compared to limited human labeled phrases. In
                   addition, we develop a POS-guided phrasal segmentation model,
                   which incorporates the shallow syntactic information in
                   part-of-speech (POS) tags to further enhance the performance,
                   when a POS tagger is available. Note that, AutoPhrase can
                   support any language as long as a general knowledge base
                   (e.g., Wikipedia) in that language is available, while
                   benefiting from, but not requiring, a POS tagger. Compared to
                   the state-of-the-art methods, the new method has shown
                   significant improvements in effectiveness on five real-world
                   datasets across different domains and languages.",
  month         =  feb,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1702.04457"
}

@ARTICLE{Kumar2018-hs,
  title         = "Von Mises-Fisher Loss for Training Sequence to Sequence
                   Models with Continuous Outputs",
  author        = "Kumar, Sachin and Tsvetkov, Yulia",
  abstract      = "The Softmax function is used in the final layer of nearly all
                   existing sequence-to-sequence models for language generation.
                   However, it is usually the slowest layer to compute which
                   limits the vocabulary size to a subset of most frequent
                   types; and it has a large memory footprint. We propose a
                   general technique for replacing the softmax layer with a
                   continuous embedding layer. Our primary innovations are a
                   novel probabilistic loss, and a training and inference
                   procedure in which we generate a probability distribution
                   over pre-trained word embeddings, instead of a multinomial
                   distribution over the vocabulary obtained via softmax. We
                   evaluate this new class of sequence-to-sequence models with
                   continuous outputs on the task of neural machine translation.
                   We show that our models obtain upto 2.5x speed-up in training
                   time while performing on par with the state-of-the-art models
                   in terms of translation quality. These models are capable of
                   handling very large vocabularies without compromising on
                   translation quality. They also produce more meaningful errors
                   than in the softmax-based models, as these errors typically
                   lie in a subspace of the vector space of the reference
                   translations.",
  month         =  dec,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1812.04616"
}

@ARTICLE{Zhang2014-fq,
  title    = "A Review on Multi-Label Learning Algorithms",
  author   = "Zhang, M and Zhou, Z",
  journal  = "IEEE transactions on knowledge and data engineering",
  volume   =  26,
  number   =  8,
  pages    = "1819--1837",
  abstract = "Multi-label learning studies the problem where each example is
              represented by a single instance while associated with a set of
              labels simultaneously. During the past decade, significant amount
              of progresses have been made toward this emerging machine learning
              paradigm. This paper aims to provide a timely review on this area
              with emphasis on state-of-the-art multi-label learning algorithms.
              Firstly, fundamentals on multi-label learning including formal
              definition and evaluation metrics are given. Secondly and
              primarily, eight representative multi-label learning algorithms
              are scrutinized under common notations with relevant analyses and
              discussions. Thirdly, several related learning settings are
              briefly summarized. As a conclusion, online resources and open
              research problems on multi-label learning are outlined for
              reference purposes.",
  month    =  aug,
  year     =  2014,
  keywords = "learning (artificial intelligence);multilabel learning
              algorithms;instance learning;machine learning paradigm;formal
              definition;evaluation metrics;learning
              settings;Training;Correlation;Supervised
              learning;Semantics;Machine learning algorithms;Algorithm design
              and analysis;Vectors;Computing Methodologies;Artificial
              Intelligence;Learning;Information Technology and Systems;Database
              Management;Database Applications;Data mining;Multi-label
              learning;label correlations;problem transformation;algorithm
              adaptation",
  doi      = "10.1109/TKDE.2013.39",
  issn     = "1041-4347"
}

@ARTICLE{Lee2018-qe,
  title         = "Coupled Representation Learning for Domains, Intents and
                   Slots in Spoken Language Understanding",
  author        = "Lee, Jihwan and Kim, Dongchan and Sarikaya, Ruhi and Kim,
                   Young-Bum",
  abstract      = "Representation learning is an essential problem in a wide
                   range of applications and it is important for performing
                   downstream tasks successfully. In this paper, we propose a
                   new model that learns coupled representations of domains,
                   intents, and slots by taking advantage of their hierarchical
                   dependency in a Spoken Language Understanding system. Our
                   proposed model learns the vector representation of intents
                   based on the slots tied to these intents by aggregating the
                   representations of the slots. Similarly, the vector
                   representation of a domain is learned by aggregating the
                   representations of the intents tied to a specific domain. To
                   the best of our knowledge, it is the first approach to
                   jointly learning the representations of domains, intents, and
                   slots using their hierarchical relationships. The
                   experimental results demonstrate the effectiveness of the
                   representations learned by our model, as evidenced by
                   improved performance on the contextual cross-domain reranking
                   task.",
  month         =  dec,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1812.06083"
}

@INPROCEEDINGS{Cao2007-dd,
  title     = "Learning to Rank: From Pairwise Approach to Listwise Approach",
  author    = "Cao, Zhe and Qin, Tao and Liu, Tie-Yan and Tsai, Ming-Feng and
               Li, Hang",
  booktitle = "Proceedings of the 24th International Conference on Machine
               Learning",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "129--136",
  abstract  = "The paper is concerned with learning to rank, which is to
               construct a model or a function for ranking objects. Learning to
               rank is useful for document retrieval, collaborative filtering,
               and many other applications. Several methods for learning to rank
               have been proposed, which …",
  series    = "ICML '07",
  year      =  2007,
  doi       = "10.1145/1273496.1273513",
  isbn      =  9781595937933
}

@INPROCEEDINGS{Lee2017-ax,
  title     = "Adverse Drug Event Detection in Tweets with Semi-Supervised
               Convolutional Neural Networks",
  author    = "Lee, Kathy and Qadir, Ashequl and Hasan, Sadid A and Datla, Vivek
               and Prakash, Aaditya and Liu, Joey and Farri, Oladimeji",
  booktitle = "Proceedings of the 26th International Conference on World Wide
               Web",
  publisher = "International World Wide Web Conferences Steering Committee",
  pages     = "705--714",
  month     =  apr,
  year      =  2017,
  keywords  = "adverse drug events; healthcare; pharmacovigilance;
               semi-supervised convolutional neural networks; social media; text
               classification",
  doi       = "10.1145/3038912.3052671",
  isbn      =  9781450349130
}

@ARTICLE{Wu2015-pl,
  title    = "A Study of Neural Word Embeddings for Named Entity Recognition in
              Clinical Text",
  author   = "Wu, Yonghui and Xu, Jun and Jiang, Min and Zhang, Yaoyun and Xu,
              Hua",
  journal  = "AMIA ... Annual Symposium proceedings / AMIA Symposium. AMIA
              Symposium",
  volume   =  2015,
  pages    = "1326--1333",
  abstract = "Clinical Named Entity Recognition (NER) is a critical task for
              extracting important patient information from clinical text to
              support clinical and translational research. This study explored
              the neural word embeddings derived from a large unlabeled clinical
              corpus for clinical NER. We systematically compared two neural
              word embedding algorithms and three different strategies for
              deriving distributed word representations. Two neural word
              embeddings were derived from the unlabeled Multiparameter
              Intelligent Monitoring in Intensive Care (MIMIC) II corpus
              (403,871 notes). The results from both 2010 i2b2 and 2014 Semantic
              Evaluation (SemEval) data showed that the binarized word embedding
              features outperformed other strategies for deriving distributed
              word representations. The binarized embedding features improved
              the F1-score of the Conditional Random Fields based clinical NER
              system by 2.3\% on i2b2 data and 2.4\% on SemEval data. The
              combined feature from the binarized embeddings and the Brown
              clusters improved the F1-score of the clinical NER system by 2.9\%
              on i2b2 data and 2.7\% on SemEval data. Our study also showed that
              the distributed word embedding features derived from a large
              unlabeled corpus can be better than the widely used Brown
              clusters. Further analysis found that the neural word embeddings
              captured a wide range of semantic relations, which could be
              discretized into distributed word representations to benefit the
              clinical NER system. The low-cost distributed feature
              representation can be adapted to any other clinical natural
              language processing research.",
  month    =  nov,
  year     =  2015,
  pmc      = "PMC4765694",
  pmid     =  26958273,
  issn     = "1942-597X,1559-4076",
  language = "en"
}

@ARTICLE{Yang2019-fx,
  title         = "{XLNet}: Generalized Autoregressive Pretraining for Language
                   Understanding",
  author        = "Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell,
                   Jaime and Salakhutdinov, Ruslan and Le, Quoc V",
  abstract      = "With the capability of modeling bidirectional contexts,
                   denoising autoencoding based pretraining like BERT achieves
                   better performance than pretraining approaches based on
                   autoregressive language modeling. However, relying on
                   corrupting the input with masks, BERT neglects dependency
                   between the masked positions and suffers from a
                   pretrain-finetune discrepancy. In light of these pros and
                   cons, we propose XLNet, a generalized autoregressive
                   pretraining method that (1) enables learning bidirectional
                   contexts by maximizing the expected likelihood over all
                   permutations of the factorization order and (2) overcomes the
                   limitations of BERT thanks to its autoregressive formulation.
                   Furthermore, XLNet integrates ideas from Transformer-XL, the
                   state-of-the-art autoregressive model, into pretraining.
                   Empirically, XLNet outperforms BERT on 20 tasks, often by a
                   large margin, and achieves state-of-the-art results on 18
                   tasks including question answering, natural language
                   inference, sentiment analysis, and document ranking.",
  month         =  jun,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1906.08237"
}

@ARTICLE{Sil2017-lj,
  title         = "Neural Cross-Lingual Entity Linking",
  author        = "Sil, Avirup and Kundu, Gourab and Florian, Radu and Hamza,
                   Wael",
  abstract      = "A major challenge in Entity Linking (EL) is making effective
                   use of contextual information to disambiguate mentions to
                   Wikipedia that might refer to different entities in different
                   contexts. The problem exacerbates with cross-lingual EL which
                   involves linking mentions written in non-English documents to
                   entries in the English Wikipedia: to compare textual clues
                   across languages we need to compute similarity between
                   textual fragments across languages. In this paper, we propose
                   a neural EL model that trains fine-grained similarities and
                   dissimilarities between the query and candidate document from
                   multiple perspectives, combined with convolution and tensor
                   networks. Further, we show that this English-trained system
                   can be applied, in zero-shot learning, to other languages by
                   making surprisingly effective use of multi-lingual
                   embeddings. The proposed system has strong empirical evidence
                   yielding state-of-the-art results in English as well as
                   cross-lingual: Spanish and Chinese TAC 2015 datasets.",
  month         =  dec,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1712.01813"
}

@ARTICLE{Papagiannopoulou2019-zw,
  title         = "A Review of Keyphrase Extraction",
  author        = "Papagiannopoulou, Eirini and Tsoumakas, Grigorios",
  abstract      = "Automated keyphrase extraction is a crucial textual
                   information processing task regarding the most types of
                   digital content management systems. It concerns the selection
                   of representative and characteristic phrases from a document
                   that express all aspects related to its content. This article
                   introduces the task of keyphrase extraction and provides a
                   view of existing work that is well organized and
                   comprehensive. Moreover, it discusses the different
                   evaluation approaches giving meaningful insights and
                   highlighting open issues. Finally, a comparative experimental
                   study for popular unsupervised techniques on five datasets is
                   presented.",
  month         =  may,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1905.05044"
}

@ARTICLE{Neelakantan2015-er,
  title         = "Learning Dictionaries for Named Entity Recognition using
                   Minimal Supervision",
  author        = "Neelakantan, Arvind and Collins, Michael",
  abstract      = "This paper describes an approach for automatic construction
                   of dictionaries for Named Entity Recognition (NER) using
                   large amounts of unlabeled data and a few seed examples. We
                   use Canonical Correlation Analysis (CCA) to obtain lower
                   dimensional embeddings (representations) for candidate
                   phrases and classify these phrases using a small number of
                   labeled examples. Our method achieves 16.5\% and 11.3\% F-1
                   score improvement over co-training on disease and virus NER
                   respectively. We also show that by adding candidate phrase
                   embeddings as features in a sequence tagger gives better
                   performance compared to using word embeddings.",
  month         =  apr,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1504.06650"
}

@ARTICLE{P2014-lc,
  title         = "An Autoencoder Approach to Learning Bilingual Word
                   Representations",
  author        = "P, Sarath Chandar A and Lauly, Stanislas and Larochelle, Hugo
                   and Khapra, Mitesh M and Ravindran, Balaraman and Raykar,
                   Vikas and Saha, Amrita",
  abstract      = "Cross-language learning allows us to use training data from
                   one language to build models for a different language. Many
                   approaches to bilingual learning require that we have
                   word-level alignment of sentences from parallel corpora. In
                   this work we explore the use of autoencoder-based methods for
                   cross-language learning of vectorial word representations
                   that are aligned between two languages, while not relying on
                   word-level alignments. We show that by simply learning to
                   reconstruct the bag-of-words representations of aligned
                   sentences, within and between languages, we can in fact learn
                   high-quality representations and do without word alignments.
                   Since training autoencoders on word observations presents
                   certain computational issues, we propose and compare
                   different variations adapted to this setting. We also propose
                   an explicit correlation maximizing regularizer that leads to
                   significant improvement in the performance. We empirically
                   investigate the success of our approach on the problem of
                   cross-language test classification, where a classifier
                   trained on a given language (e.g., English) must learn to
                   generalize to a different language (e.g., German). These
                   experiments demonstrate that our approaches are competitive
                   with the state-of-the-art, achieving up to 10-14 percentage
                   point improvements over the best reported results on this
                   task.",
  month         =  feb,
  year          =  2014,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1402.1454"
}

@ARTICLE{Conneau2017-fb,
  title         = "Word Translation Without Parallel Data",
  author        = "Conneau, Alexis and Lample, Guillaume and Ranzato,
                   Marc'aurelio and Denoyer, Ludovic and Jégou, Hervé",
  abstract      = "State-of-the-art methods for learning cross-lingual word
                   embeddings have relied on bilingual dictionaries or parallel
                   corpora. Recent studies showed that the need for parallel
                   data supervision can be alleviated with character-level
                   information. While these methods showed encouraging results,
                   they are not on par with their supervised counterparts and
                   are limited to pairs of languages sharing a common alphabet.
                   In this work, we show that we can build a bilingual
                   dictionary between two languages without using any parallel
                   corpora, by aligning monolingual word embedding spaces in an
                   unsupervised way. Without using any character information,
                   our model even outperforms existing supervised methods on
                   cross-lingual tasks for some language pairs. Our experiments
                   demonstrate that our method works very well also for distant
                   language pairs, like English-Russian or English-Chinese. We
                   finally describe experiments on the English-Esperanto
                   low-resource language pair, on which there only exists a
                   limited amount of parallel data, to show the potential impact
                   of our method in fully unsupervised machine translation. Our
                   code, embeddings and dictionaries are publicly available.",
  month         =  oct,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1710.04087"
}

@ARTICLE{Gomaa2013-lr,
  title     = "A survey of text similarity approaches",
  author    = "Gomaa, W H and Fahmy, A A",
  journal   = "International Journal of Computer Applications in Technology",
  publisher = "Citeseer",
  abstract  = "Measuring the similarity between words, sentences, paragraphs and
               documents is an important component in various tasks such as
               information retrieval, document clustering, word-sense
               disambiguation, automatic essay scoring, short answer grading,
               machine …",
  year      =  2013,
  issn      = "0952-8091"
}

@ARTICLE{Lee2017-vq,
  title         = "Transfer Learning for Named-Entity Recognition with Neural
                   Networks",
  author        = "Lee, Ji Young and Dernoncourt, Franck and Szolovits, Peter",
  abstract      = "Recent approaches based on artificial neural networks (ANNs)
                   have shown promising results for named-entity recognition
                   (NER). In order to achieve high performances, ANNs need to be
                   trained on a large labeled dataset. However, labels might be
                   difficult to obtain for the dataset on which the user wants
                   to perform NER: label scarcity is particularly pronounced for
                   patient note de-identification, which is an instance of NER.
                   In this work, we analyze to what extent transfer learning may
                   address this issue. In particular, we demonstrate that
                   transferring an ANN model trained on a large labeled dataset
                   to another dataset with a limited number of labels improves
                   upon the state-of-the-art results on two different datasets
                   for patient note de-identification.",
  month         =  may,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1705.06273"
}

@ARTICLE{Ferragina2010-be,
  title         = "Fast and accurate annotation of short texts with Wikipedia
                   pages",
  author        = "Ferragina, Paolo and Scaiella, Ugo",
  abstract      = "We address the problem of cross-referencing text fragments
                   with Wikipedia pages, in a way that synonymy and polysemy
                   issues are resolved accurately and efficiently. We take
                   inspiration from a recent flow of work [Cucerzan 2007,
                   Mihalcea and Csomai 2007, Milne and Witten 2008, Chakrabarti
                   et al 2009], and extend their scenario from the annotation of
                   long documents to the annotation of short texts, such as
                   snippets of search-engine results, tweets, news, blogs, etc..
                   These short and poorly composed texts pose new challenges in
                   terms of efficiency and effectiveness of the annotation
                   process, that we address by designing and engineering TAGME,
                   the first system that performs an accurate and on-the-fly
                   annotation of these short textual fragments. A large set of
                   experiments shows that TAGME outperforms state-of-the-art
                   algorithms when they are adapted to work on short texts and
                   it results fast and competitive on long texts.",
  month         =  jun,
  year          =  2010,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "1006.3498"
}

@ARTICLE{Lample2019-ah,
  title         = "Cross-lingual Language Model Pretraining",
  author        = "Lample, Guillaume and Conneau, Alexis",
  abstract      = "Recent studies have demonstrated the efficiency of generative
                   pretraining for English natural language understanding. In
                   this work, we extend this approach to multiple languages and
                   show the effectiveness of cross-lingual pretraining. We
                   propose two methods to learn cross-lingual language models
                   (XLMs): one unsupervised that only relies on monolingual
                   data, and one supervised that leverages parallel data with a
                   new cross-lingual language model objective. We obtain
                   state-of-the-art results on cross-lingual classification,
                   unsupervised and supervised machine translation. On XNLI, our
                   approach pushes the state of the art by an absolute gain of
                   4.9\% accuracy. On unsupervised machine translation, we
                   obtain 34.3 BLEU on WMT'16 German-English, improving the
                   previous state of the art by more than 9 BLEU. On supervised
                   machine translation, we obtain a new state of the art of 38.5
                   BLEU on WMT'16 Romanian-English, outperforming the previous
                   best approach by more than 4 BLEU. Our code and pretrained
                   models will be made publicly available.",
  month         =  jan,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1901.07291"
}

@ARTICLE{Peters2019-rw,
  title         = "To Tune or Not to Tune? Adapting Pretrained Representations
                   to Diverse Tasks",
  author        = "Peters, Matthew and Ruder, Sebastian and Smith, Noah A",
  abstract      = "While most previous work has focused on different pretraining
                   objectives and architectures for transfer learning, we ask
                   how to best adapt the pretrained model to a given target
                   task. We focus on the two most common forms of adaptation,
                   feature extraction (where the pretrained weights are frozen),
                   and directly fine-tuning the pretrained model. Our empirical
                   results across diverse NLP tasks with two state-of-the-art
                   models show that the relative performance of fine-tuning vs.
                   feature extraction depends on the similarity of the
                   pretraining and target tasks. We explore possible
                   explanations for this finding and provide a set of adaptation
                   guidelines for the NLP practitioner.",
  month         =  mar,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1903.05987"
}

@INCOLLECTION{Vaswani2017-fw,
  title     = "Attention is All you Need",
  author    = "Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit,
               Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Ł Ukasz and
               Polosukhin, Illia",
  editor    = "Guyon, I and Luxburg, U V and Bengio, S and Wallach, H and
               Fergus, R and Vishwanathan, S and Garnett, R",
  booktitle = "Advances in Neural Information Processing Systems 30",
  publisher = "Curran Associates, Inc.",
  pages     = "5998--6008",
  year      =  2017
}

@ARTICLE{Shazeer2017-qa,
  title         = "Outrageously Large Neural Networks: The Sparsely-Gated
                   Mixture-of-Experts Layer",
  author        = "Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof
                   and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean,
                   Jeff",
  abstract      = "The capacity of a neural network to absorb information is
                   limited by its number of parameters. Conditional computation,
                   where parts of the network are active on a per-example basis,
                   has been proposed in theory as a way of dramatically
                   increasing model capacity without a proportional increase in
                   computation. In practice, however, there are significant
                   algorithmic and performance challenges. In this work, we
                   address these challenges and finally realize the promise of
                   conditional computation, achieving greater than 1000x
                   improvements in model capacity with only minor losses in
                   computational efficiency on modern GPU clusters. We introduce
                   a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting
                   of up to thousands of feed-forward sub-networks. A trainable
                   gating network determines a sparse combination of these
                   experts to use for each example. We apply the MoE to the
                   tasks of language modeling and machine translation, where
                   model capacity is critical for absorbing the vast quantities
                   of knowledge available in the training corpora. We present
                   model architectures in which a MoE with up to 137 billion
                   parameters is applied convolutionally between stacked LSTM
                   layers. On large language modeling and machine translation
                   benchmarks, these models achieve significantly better results
                   than state-of-the-art at lower computational cost.",
  month         =  jan,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1701.06538"
}

@ARTICLE{Shen2018-ut,
  title         = "Ordered Neurons: Integrating Tree Structures into Recurrent
                   Neural Networks",
  author        = "Shen, Yikang and Tan, Shawn and Sordoni, Alessandro and
                   Courville, Aaron",
  abstract      = "Natural language is hierarchically structured: smaller units
                   (e.g., phrases) are nested within larger units (e.g.,
                   clauses). When a larger constituent ends, all of the smaller
                   constituents that are nested within it must also be closed.
                   While the standard LSTM architecture allows different neurons
                   to track information at different time scales, it does not
                   have an explicit bias towards modeling a hierarchy of
                   constituents. This paper proposes to add such an inductive
                   bias by ordering the neurons; a vector of master input and
                   forget gates ensures that when a given neuron is updated, all
                   the neurons that follow it in the ordering are also updated.
                   Our novel recurrent architecture, ordered neurons LSTM
                   (ON-LSTM), achieves good performance on four different tasks:
                   language modeling, unsupervised parsing, targeted syntactic
                   evaluation, and logical inference.",
  month         =  oct,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1810.09536"
}

@INPROCEEDINGS{Ettinger2016-gg,
  title     = "Retrofitting sense-specific word vectors using parallel text",
  author    = "Ettinger, Allyson and Resnik, Philip and Carpuat, Marine",
  booktitle = "Proceedings of the 2016 Conference of the North American Chapter
               of the Association for Computational Linguistics: Human Language
               Technologies",
  publisher = "aclweb.org",
  pages     = "1378--1383",
  abstract  = "… to retrofit existing word vector representations and create a
               sense-based vec … Retrofitting sense-specific embed- dings using
               only 300k sentence pairs, which repre- sent about 5\% of the
               total training … Improving statisti- cal machine translation
               using word sense disambigua- tion …",
  year      =  2016
}

@ARTICLE{Song2019-ek,
  title         = "{MASS}: Masked Sequence to Sequence Pre-training for Language
                   Generation",
  author        = "Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and
                   Liu, Tie-Yan",
  abstract      = "Pre-training and fine-tuning, e.g., BERT, have achieved great
                   success in language understanding by transferring knowledge
                   from rich-resource pre-training task to the low/zero-resource
                   downstream tasks. Inspired by the success of BERT, we propose
                   MAsked Sequence to Sequence pre-training (MASS) for the
                   encoder-decoder based language generation tasks. MASS adopts
                   the encoder-decoder framework to reconstruct a sentence
                   fragment given the remaining part of the sentence: its
                   encoder takes a sentence with randomly masked fragment
                   (several consecutive tokens) as input, and its decoder tries
                   to predict this masked fragment. In this way, MASS can
                   jointly train the encoder and decoder to develop the
                   capability of representation extraction and language
                   modeling. By further fine-tuning on a variety of
                   zero/low-resource language generation tasks, including neural
                   machine translation, text summarization and conversational
                   response generation (3 tasks and totally 8 datasets), MASS
                   achieves significant improvements over the baselines without
                   pre-training or with other pre-training methods. Specially,
                   we achieve the state-of-the-art accuracy (37.5 in terms of
                   BLEU score) on the unsupervised English-French translation,
                   even beating the early attention-based supervised model.",
  month         =  may,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1905.02450"
}

@INPROCEEDINGS{Joachims2017-fd,
  title     = "Unbiased Learning-to-Rank with Biased Feedback",
  author    = "Joachims, Thorsten and Swaminathan, Adith and Schnabel, Tobias",
  booktitle = "Proceedings of the Tenth ACM International Conference on Web
               Search and Data Mining",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "781--789",
  abstract  = "Implicit feedback (eg, clicks, dwell times, etc.) is an abundant
               source of data in human- interactive systems. While implicit
               feedback has many advantages (eg, it is inexpensive to collect,
               user centric, and timely), its inherent biases are a key obstacle
               to its effective use. For …",
  series    = "WSDM '17",
  year      =  2017,
  keywords  = "click models, implicit feedback, learning to rank, propensity
               weighting, ranking svm",
  doi       = "10.1145/3018661.3018699",
  isbn      =  9781450346757
}

@ARTICLE{Peters2019-bq,
  title         = "Sparse Sequence-to-Sequence Models",
  author        = "Peters, Ben and Niculae, Vlad and Martins, André F T",
  abstract      = "Sequence-to-sequence models are a powerful workhorse of NLP.
                   Most variants employ a softmax transformation in both their
                   attention mechanism and output layer, leading to dense
                   alignments and strictly positive output probabilities. This
                   density is wasteful, making models less interpretable and
                   assigning probability mass to many implausible outputs. In
                   this paper, we propose sparse sequence-to-sequence models,
                   rooted in a new family of $\alpha$-entmax transformations,
                   which includes softmax and sparsemax as particular cases, and
                   is sparse for any $\alpha > 1$. We provide fast algorithms to
                   evaluate these transformations and their gradients, which
                   scale well for large vocabulary sizes. Our models are able to
                   produce sparse alignments and to assign nonzero probability
                   to a short list of plausible outputs, sometimes rendering
                   beam search exact. Experiments on morphological inflection
                   and machine translation reveal consistent gains over dense
                   models.",
  month         =  may,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1905.05702"
}

@ARTICLE{Mielke2019-tk,
  title         = "What Kind of Language Is Hard to Language-Model?",
  author        = "Mielke, Sebastian J and Cotterell, Ryan and Gorman, Kyle and
                   Roark, Brian and Eisner, Jason",
  abstract      = "How language-agnostic are current state-of-the-art NLP tools?
                   Are there some types of language that are easier to model
                   with current methods? In prior work (Cotterell et al., 2018)
                   we attempted to address this question for language modeling,
                   and observed that recurrent neural network language models do
                   not perform equally well over all the high-resource European
                   languages found in the Europarl corpus. We speculated that
                   inflectional morphology may be the primary culprit for the
                   discrepancy. In this paper, we extend these earlier
                   experiments to cover 69 languages from 13 language families
                   using a multilingual Bible corpus. Methodologically, we
                   introduce a new paired-sample multiplicative mixed-effects
                   model to obtain language difficulty coefficients from
                   at-least-pairwise parallel corpora. In other words, the model
                   is aware of inter-sentence variation and can handle missing
                   data. Exploiting this model, we show that ``translationese''
                   is not any easier to model than natively written language in
                   a fair comparison. Trying to answer the question of what
                   features difficult languages have in common, we try and fail
                   to reproduce our earlier (Cotterell et al., 2018) observation
                   about morphological complexity and instead reveal far simpler
                   statistics of the data that seem to drive complexity in a
                   much larger sample.",
  month         =  jun,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1906.04726"
}

@ARTICLE{Sun2019-sn,
  title         = "{ERNIE}: Enhanced Representation through Knowledge
                   Integration",
  author        = "Sun, Yu and Wang, Shuohuan and Li, Yukun and Feng, Shikun and
                   Chen, Xuyi and Zhang, Han and Tian, Xin and Zhu, Danxiang and
                   Tian, Hao and Wu, Hua",
  abstract      = "We present a novel language representation model enhanced by
                   knowledge called ERNIE (Enhanced Representation through
                   kNowledge IntEgration). Inspired by the masking strategy of
                   BERT, ERNIE is designed to learn language representation
                   enhanced by knowledge masking strategies, which includes
                   entity-level masking and phrase-level masking. Entity-level
                   strategy masks entities which are usually composed of
                   multiple words.Phrase-level strategy masks the whole phrase
                   which is composed of several words standing together as a
                   conceptual unit.Experimental results show that ERNIE
                   outperforms other baseline methods, achieving new
                   state-of-the-art results on five Chinese natural language
                   processing tasks including natural language inference,
                   semantic similarity, named entity recognition, sentiment
                   analysis and question answering. We also demonstrate that
                   ERNIE has more powerful knowledge inference capacity on a
                   cloze test.",
  month         =  apr,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1904.09223"
}

@ARTICLE{Gashteovski2019-fm,
  title         = "{OPIEC}: An Open Information Extraction Corpus",
  author        = "Gashteovski, Kiril and Wanner, Sebastian and Hertling, Sven
                   and Broscheit, Samuel and Gemulla, Rainer",
  abstract      = "Open information extraction (OIE) systems extract relations
                   and their arguments from natural language text in an
                   unsupervised manner. The resulting extractions are a valuable
                   resource for downstream tasks such as knowledge base
                   construction, open question answering, or event schema
                   induction. In this paper, we release, describe, and analyze
                   an OIE corpus called OPIEC, which was extracted from the text
                   of English Wikipedia. OPIEC complements the available OIE
                   resources: It is the largest OIE corpus publicly available to
                   date (over 340M triples) and contains valuable metadata such
                   as provenance information, confidence scores, linguistic
                   annotations, and semantic annotations including spatial and
                   temporal information. We analyze the OPIEC corpus by
                   comparing its content with knowledge bases such as DBpedia or
                   YAGO, which are also based on Wikipedia. We found that most
                   of the facts between entities present in OPIEC cannot be
                   found in DBpedia and/or YAGO, that OIE facts often differ in
                   the level of specificity compared to knowledge base facts,
                   and that OIE open relations are generally highly polysemous.
                   We believe that the OPIEC corpus is a valuable resource for
                   future research on automated knowledge base construction.",
  month         =  apr,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1904.12324"
}

@ARTICLE{Le2019-km,
  title         = "Distant Learning for Entity Linking with Automatic Noise
                   Detection",
  author        = "Le, Phong and Titov, Ivan",
  abstract      = "Accurate entity linkers have been produced for domains and
                   languages where annotated data (i.e., texts linked to a
                   knowledge base) is available. However, little progress has
                   been made for the settings where no or very limited amounts
                   of labeled data are present (e.g., legal or most scientific
                   domains). In this work, we show how we can learn to link
                   mentions without having any labeled examples, only a
                   knowledge base and a collection of unannotated texts from the
                   corresponding domain. In order to achieve this, we frame the
                   task as a multi-instance learning problem and rely on surface
                   matching to create initial noisy labels. As the learning
                   signal is weak and our surrogate labels are noisy, we
                   introduce a noise detection component in our model: it lets
                   the model detect and disregard examples which are likely to
                   be noisy. Our method, jointly learning to detect noise and
                   link entities, greatly outperforms the surface matching
                   baseline and for a subset of entity categories even
                   approaches the performance of supervised learning.",
  month         =  may,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1905.07189"
}

@ARTICLE{Liu2019-uq,
  title         = "Linguistic Knowledge and Transferability of Contextual
                   Representations",
  author        = "Liu, Nelson F and Gardner, Matt and Belinkov, Yonatan and
                   Peters, Matthew E and Smith, Noah A",
  abstract      = "Contextual word representations derived from large-scale
                   neural language models are successful across a diverse set of
                   NLP tasks, suggesting that they encode useful and
                   transferable features of language. To shed light on the
                   linguistic knowledge they capture, we study the
                   representations produced by several recent pretrained
                   contextualizers (variants of ELMo, the OpenAI transformer
                   language model, and BERT) with a suite of seventeen diverse
                   probing tasks. We find that linear models trained on top of
                   frozen contextual representations are competitive with
                   state-of-the-art task-specific models in many cases, but fail
                   on tasks requiring fine-grained linguistic knowledge (e.g.,
                   conjunct identification). To investigate the transferability
                   of contextual word representations, we quantify differences
                   in the transferability of individual layers within
                   contextualizers, especially between recurrent neural networks
                   (RNNs) and transformers. For instance, higher layers of RNNs
                   are more task-specific, while transformer layers do not
                   exhibit the same monotonic trend. In addition, to better
                   understand what makes contextual word representations
                   transferable, we compare language model pretraining with
                   eleven supervised pretraining tasks. For any given task,
                   pretraining on a closely related task yields better
                   performance than language model pretraining (which is better
                   on average) when the pretraining dataset is fixed. However,
                   language model pretraining on more data gives the best
                   results.",
  month         =  mar,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1903.08855"
}

@ARTICLE{Gururangan2019-fd,
  title         = "Variational Pretraining for Semi-supervised Text
                   Classification",
  author        = "Gururangan, Suchin and Dang, Tam and Card, Dallas and Smith,
                   Noah A",
  abstract      = "We introduce VAMPIRE, a lightweight pretraining framework for
                   effective text classification when data and computing
                   resources are limited. We pretrain a unigram document model
                   as a variational autoencoder on in-domain, unlabeled data and
                   use its internal states as features in a downstream
                   classifier. Empirically, we show the relative strength of
                   VAMPIRE against computationally expensive contextual
                   embeddings and other popular semi-supervised baselines under
                   low resource settings. We also find that fine-tuning to
                   in-domain data is crucial to achieving decent performance
                   from contextual embeddings when working with limited
                   supervision. We accompany this paper with code to pretrain
                   and use VAMPIRE embeddings in downstream tasks.",
  month         =  jun,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1906.02242"
}

@ARTICLE{Lin2019-op,
  title         = "Open Sesame: Getting Inside {BERT}'s Linguistic Knowledge",
  author        = "Lin, Yongjie and Tan, Yi Chern and Frank, Robert",
  abstract      = "How and to what extent does BERT encode
                   syntactically-sensitive hierarchical information or
                   positionally-sensitive linear information? Recent work has
                   shown that contextual representations like BERT perform well
                   on tasks that require sensitivity to linguistic structure. We
                   present here two studies which aim to provide a better
                   understanding of the nature of BERT's representations. The
                   first of these focuses on the identification of
                   structurally-defined elements using diagnostic classifiers,
                   while the second explores BERT's representation of
                   subject-verb agreement and anaphor-antecedent dependencies
                   through a quantitative assessment of self-attention vectors.
                   In both cases, we find that BERT encodes positional
                   information about word tokens well on its lower layers, but
                   switches to a hierarchically-oriented encoding on higher
                   layers. We conclude then that BERT's representations do
                   indeed model linguistically relevant aspects of hierarchical
                   structure, though they do not appear to show the sharp
                   sensitivity to hierarchical structure that is found in human
                   processing of reflexive anaphora.",
  month         =  jun,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1906.01698"
}

@ARTICLE{Coenen2019-sd,
  title         = "Visualizing and Measuring the Geometry of {BERT}",
  author        = "Coenen, Andy and Reif, Emily and Yuan, Ann and Kim, Been and
                   Pearce, Adam and Viégas, Fernanda and Wattenberg, Martin",
  abstract      = "Transformer architectures show significant promise for
                   natural language processing. Given that a single pretrained
                   model can be fine-tuned to perform well on many different
                   tasks, these networks appear to extract generally useful
                   linguistic features. A natural question is how such networks
                   represent this information internally. This paper describes
                   qualitative and quantitative investigations of one
                   particularly effective model, BERT. At a high level,
                   linguistic features seem to be represented in separate
                   semantic and syntactic subspaces. We find evidence of a
                   fine-grained geometric representation of word senses. We also
                   present empirical descriptions of syntactic representations
                   in both attention matrices and individual word embeddings, as
                   well as a mathematical argument to explain the geometry of
                   these representations.",
  month         =  jun,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1906.02715"
}

@ARTICLE{Logeswaran2019-wo,
  title         = "Zero-Shot Entity Linking by Reading Entity Descriptions",
  author        = "Logeswaran, Lajanugen and Chang, Ming-Wei and Lee, Kenton and
                   Toutanova, Kristina and Devlin, Jacob and Lee, Honglak",
  abstract      = "We present the zero-shot entity linking task, where mentions
                   must be linked to unseen entities without in-domain labeled
                   data. The goal is to enable robust transfer to highly
                   specialized domains, and so no metadata or alias tables are
                   assumed. In this setting, entities are only identified by
                   text descriptions, and models must rely strictly on language
                   understanding to resolve the new entities. First, we show
                   that strong reading comprehension models pre-trained on large
                   unlabeled data can be used to generalize to unseen entities.
                   Second, we propose a simple and effective adaptive
                   pre-training strategy, which we term domain-adaptive
                   pre-training (DAP), to address the domain shift problem
                   associated with linking unseen entities in a new domain. We
                   present experiments on a new dataset that we construct for
                   this task and show that DAP improves over strong pre-training
                   baselines, including BERT. The data and code are available at
                   https://github.com/lajanugen/zeshel.",
  month         =  jun,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1906.07348"
}

@ARTICLE{Alt2019-rc,
  title         = "Fine-tuning Pre-Trained Transformer Language Models to
                   Distantly Supervised Relation Extraction",
  author        = "Alt, Christoph and Hübner, Marc and Hennig, Leonhard",
  abstract      = "Distantly supervised relation extraction is widely used to
                   extract relational facts from text, but suffers from noisy
                   labels. Current relation extraction methods try to alleviate
                   the noise by multi-instance learning and by providing
                   supporting linguistic and contextual information to more
                   efficiently guide the relation classification. While
                   achieving state-of-the-art results, we observed these models
                   to be biased towards recognizing a limited set of relations
                   with high precision, while ignoring those in the long tail.
                   To address this gap, we utilize a pre-trained language model,
                   the OpenAI Generative Pre-trained Transformer (GPT) [Radford
                   et al., 2018]. The GPT and similar models have been shown to
                   capture semantic and syntactic features, and also a notable
                   amount of ``common-sense'' knowledge, which we hypothesize
                   are important features for recognizing a more diverse set of
                   relations. By extending the GPT to the distantly supervised
                   setting, and fine-tuning it on the NYT10 dataset, we show
                   that it predicts a larger set of distinct relation types with
                   high confidence. Manual and automated evaluation of our model
                   shows that it achieves a state-of-the-art AUC score of 0.422
                   on the NYT10 dataset, and performs especially well at higher
                   recall levels.",
  month         =  jun,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1906.08646"
}

@ARTICLE{Liu2019-ra,
  title         = "Incorporating Priors with Feature Attribution on Text
                   Classification",
  author        = "Liu, Frederick and Avci, Besim",
  abstract      = "Feature attribution methods, proposed recently, help users
                   interpret the predictions of complex models. Our approach
                   integrates feature attributions into the objective function
                   to allow machine learning practitioners to incorporate priors
                   in model building. To demonstrate the effectiveness our
                   technique, we apply it to two tasks: (1) mitigating
                   unintended bias in text classifiers by neutralizing identity
                   terms; (2) improving classifier performance in a scarce data
                   setting by forcing the model to focus on toxic terms. Our
                   approach adds an L2 distance loss between feature
                   attributions and task-specific prior values to the objective.
                   Our experiments show that i) a classifier trained with our
                   technique reduces undesired model biases without a trade off
                   on the original task; ii) incorporating priors helps model
                   performance in scarce data settings.",
  month         =  jun,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1906.08286"
}

@ARTICLE{Lample2019-og,
  title         = "Large Memory Layers with Product Keys",
  author        = "Lample, Guillaume and Sablayrolles, Alexandre and Ranzato,
                   Marc'aurelio and Denoyer, Ludovic and Jégou, Hervé",
  abstract      = "This paper introduces a structured memory which can be easily
                   integrated into a neural network. The memory is very large by
                   design and therefore significantly increases the capacity of
                   the architecture, by up to a billion parameters with a
                   negligible computational overhead. Its design and access
                   pattern is based on product keys, which enable fast and exact
                   nearest neighbor search. The ability to increase the number
                   of parameters while keeping the same computational budget
                   lets the overall system strike a better trade-off between
                   prediction accuracy and computation efficiency both at
                   training and test time. This memory layer allows us to tackle
                   very large scale language modeling tasks. In our experiments
                   we consider a dataset with up to 30 billion words, and we
                   plug our memory layer in a state-of-the-art transformer-based
                   architecture. In particular, we found that a memory augmented
                   model with only 12 layers outperforms a baseline transformer
                   model with 24 layers, while being twice faster at inference
                   time. We release our code for reproducibility purposes.",
  month         =  jul,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1907.05242"
}

@ARTICLE{Feng2019-db,
  title         = "Misleading Failures of Partial-input Baselines",
  author        = "Feng, Shi and Wallace, Eric and Boyd-Graber, Jordan",
  abstract      = "Recent work establishes dataset difficulty and removes
                   annotation artifacts via partial-input baselines (e.g.,
                   hypothesis-only models for SNLI or question-only models for
                   VQA). When a partial-input baseline gets high accuracy, a
                   dataset is cheatable. However, the converse is not
                   necessarily true: the failure of a partial-input baseline
                   does not mean a dataset is free of artifacts. To illustrate
                   this, we first design artificial datasets which contain
                   trivial patterns in the full input that are undetectable by
                   any partial-input model. Next, we identify such artifacts in
                   the SNLI dataset - a hypothesis-only model augmented with
                   trivial patterns in the premise can solve 15\% of the
                   examples that are previously considered ``hard''. Our work
                   provides a caveat for the use of partial-input baselines for
                   dataset verification and creation.",
  month         =  may,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1905.05778"
}

@INPROCEEDINGS{Balasubramanian2010-gq,
  title     = "Exploring Reductions for Long Web Queries",
  author    = "Balasubramanian, Niranjan and Kumaran, Giridhar and Carvalho,
               Vitor R",
  booktitle = "Proceedings of the 33rd International ACM SIGIR Conference on
               Research and Development in Information Retrieval",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "571--578",
  series    = "SIGIR '10",
  year      =  2010,
  keywords  = "combining searches, learning to rank, query reformulation",
  doi       = "10.1145/1835449.1835545",
  isbn      =  9781450301534
}

@INPROCEEDINGS{Zhou2006-lx,
  title     = "{MaxMatcher}: Biological Concept Extraction Using Approximate
               Dictionary Lookup",
  author    = "Zhou, Xiaohua and Zhang, Xiaodan and Hu, Xiaohua",
  booktitle = "PRICAI 2006: Trends in Artificial Intelligence",
  publisher = "Springer Berlin Heidelberg",
  pages     = "1145--1149",
  abstract  = "Dictionary-based biological concept extraction is still the
               state-of-the-art approach to large-scale biomedical literature
               annotation and indexing. The exact dictionary lookup is a very
               simple approach, but always achieves low extraction recall
               because a biological term often has many variants while a
               dictionary is impossible to collect all of them. We propose a
               generic extraction approach, referred to as approximate
               dictionary lookup, to cope with term variations and implement it
               as an extraction system called MaxMatcher. The basic idea of this
               approach is to capture the significant words instead of all words
               to a particular concept. The new approach dramatically improves
               the extraction recall while maintaining the precision. In a
               comparative study on GENIA corpus, the recall of the new approach
               reaches a 57\% recall while the exact dictionary lookup only
               achieves a 26\% recall.",
  year      =  2006,
  doi       = "10.1007/978-3-540-36668-3\_150"
}

@INPROCEEDINGS{Cui2005-xp,
  title     = "Question Answering Passage Retrieval Using Dependency Relations",
  author    = "Cui, Hang and Sun, Renxu and Li, Keya and Kan, Min-Yen and Chua,
               Tat-Seng",
  booktitle = "Proceedings of the 28th Annual International ACM SIGIR Conference
               on Research and Development in Information Retrieval",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "400--407",
  series    = "SIGIR '05",
  year      =  2005,
  keywords  = "dependency parsing, passage retrieval, question answering",
  doi       = "10.1145/1076034.1076103",
  isbn      =  9781595930347
}

@INPROCEEDINGS{Soldaini2017-sx,
  title     = "Learning to Rank for Consumer Health Search: A Semantic Approach",
  author    = "Soldaini, Luca and Goharian, Nazli",
  booktitle = "Advances in Information Retrieval",
  publisher = "Springer International Publishing",
  pages     = "640--646",
  abstract  = "For many internet users, searching for health advice online is
               the first step in seeking treatment. We present a Learning to
               Rank system that uses a novel set of syntactic and semantic
               features to improve consumer health search. Our approach was
               evaluated on the 2016 CLEF eHealth dataset, outperforming the
               best method by 26.6\% in NDCG@10.",
  year      =  2017,
  doi       = "10.1007/978-3-319-56608-5\_60"
}

@INPROCEEDINGS{Sakai2019-cv,
  title     = "Which Diversity Evaluation Measures Are ``Good''?",
  author    = "Sakai, Tetsuya and Zeng, Zhaohao",
  booktitle = "Proceedings of the 42Nd International ACM SIGIR Conference on
               Research and Development in Information Retrieval",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "595--604",
  series    = "SIGIR'19",
  year      =  2019,
  keywords  = "evaluation measures, search result diversification, user
               preferences",
  doi       = "10.1145/3331184.3331215",
  isbn      =  9781450361729
}

@ARTICLE{Fan2019-gc,
  title         = "Reducing Transformer Depth on Demand with Structured Dropout",
  author        = "Fan, Angela and Grave, Edouard and Joulin, Armand",
  abstract      = "Overparameterized transformer networks have obtained state of
                   the art results in various natural language processing tasks,
                   such as machine translation, language modeling, and question
                   answering. These models contain hundreds of millions of
                   parameters, necessitating a large amount of computation and
                   making them prone to overfitting. In this work, we explore
                   LayerDrop, a form of structured dropout, which has a
                   regularization effect during training and allows for
                   efficient pruning at inference time. In particular, we show
                   that it is possible to select sub-networks of any depth from
                   one large network without having to finetune them and with
                   limited impact on performance. We demonstrate the
                   effectiveness of our approach by improving the state of the
                   art on machine translation, language modeling, summarization,
                   question answering, and language understanding benchmarks.
                   Moreover, we show that our approach leads to small BERT-like
                   models of higher quality compared to training from scratch or
                   using distillation.",
  month         =  sep,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1909.11556"
}

@BOOK{Murphy2012-xi,
  title     = "Machine Learning: A Probabilistic Perspective",
  author    = "Murphy, Kevin P",
  publisher = "MIT Press",
  abstract  = "A comprehensive introduction to machine learning that uses
               probabilistic models and inference as a unifying approach.Today's
               Web-enabled deluge of electronic data calls for automated methods
               of data analysis. Machine learning provides these, developing
               methods that can automatically detect patterns in data and then
               use the uncovered patterns to predict future data. This textbook
               offers a comprehensive and self-contained introduction to the
               field of machine learning, based on a unified, probabilistic
               approach.The coverage combines breadth and depth, offering
               necessary background material on such topics as probability,
               optimization, and linear algebra as well as discussion of recent
               developments in the field, including conditional random fields,
               L1 regularization, and deep learning. The book is written in an
               informal, accessible style, complete with pseudo-code for the
               most important algorithms. All topics are copiously illustrated
               with color images and worked examples drawn from such application
               domains as biology, text processing, computer vision, and
               robotics. Rather than providing a cookbook of different heuristic
               methods, the book stresses a principled model-based approach,
               often using the language of graphical models to specify models in
               a concise and intuitive way. Almost all the models described have
               been implemented in a MATLAB software package—PMTK (probabilistic
               modeling toolkit)—that is freely available online. The book is
               suitable for upper-level undergraduates with an
               introductory-level college math background and beginning graduate
               students.",
  month     =  sep,
  year      =  2012,
  isbn      =  9780262304320,
  language  = "en"
}

@ARTICLE{Karpukhin2020-cn,
  title         = "Dense Passage Retrieval for Open-Domain Question Answering",
  author        = "Karpukhin, Vladimir and Oğuz, Barlas and Min, Sewon and
                   Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen,
                   Danqi and Yih, Wen-Tau",
  abstract      = "Open-domain question answering relies on efficient passage
                   retrieval to select candidate contexts, where traditional
                   sparse vector space models, such as TF-IDF or BM25, are the
                   de facto method. In this work, we show that retrieval can be
                   practically implemented using dense representations alone,
                   where embeddings are learned from a small number of questions
                   and passages by a simple dual-encoder framework. When
                   evaluated on a wide range of open-domain QA datasets, our
                   dense retriever outperforms a strong Lucene-BM25 system
                   largely by 9\%-19\% absolute in terms of top-20 passage
                   retrieval accuracy, and helps our end-to-end QA system
                   establish new state-of-the-art on multiple open-domain QA
                   benchmarks.",
  month         =  apr,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2004.04906"
}

@ARTICLE{Eisenschlos2019-et,
  title         = "{MultiFiT}: Efficient Multi-lingual Language Model
                   Fine-tuning",
  author        = "Eisenschlos, Julian and Ruder, Sebastian and Czapla, Piotr
                   and Kardas, Marcin and Gugger, Sylvain and Howard, Jeremy",
  abstract      = "Pretrained language models are promising particularly for
                   low-resource languages as they only require unlabelled data.
                   However, training existing models requires huge amounts of
                   compute, while pretrained cross-lingual models often
                   underperform on low-resource languages. We propose
                   Multi-lingual language model Fine-Tuning (MultiFiT) to enable
                   practitioners to train and fine-tune language models
                   efficiently in their own language. In addition, we propose a
                   zero-shot method using an existing pretrained cross-lingual
                   model. We evaluate our methods on two widely used
                   cross-lingual classification datasets where they outperform
                   models pretrained on orders of magnitude more data and
                   compute. We release all models and code.",
  month         =  sep,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1909.04761"
}

@INPROCEEDINGS{Bruch2019-ig,
  title     = "Revisiting Approximate Metric Optimization in the Age of Deep
               Neural Networks",
  author    = "Bruch, Sebastian and Zoghi, Masrour and Bendersky, Michael and
               Najork, Marc",
  booktitle = "Proceedings of the 42nd International ACM SIGIR Conference on
               Research and Development in Information Retrieval",
  publisher = "ACM",
  pages     = "1241--1244",
  month     =  jul,
  year      =  2019,
  keywords  = "deep neural networks for IR; direct ranking metric optimization;
               learning to rank",
  doi       = "10.1145/3331184.3331347",
  isbn      =  9781450361729
}

@ARTICLE{Wiegreffe2019-kg,
  title         = "Attention is not not Explanation",
  author        = "Wiegreffe, Sarah and Pinter, Yuval",
  abstract      = "Attention mechanisms play a central role in NLP systems,
                   especially within recurrent neural network (RNN) models.
                   Recently, there has been increasing interest in whether or
                   not the intermediate representations offered by these modules
                   may be used to explain the reasoning for a model's
                   prediction, and consequently reach insights regarding the
                   model's decision-making process. A recent paper claims that
                   `Attention is not Explanation' (Jain and Wallace, 2019). We
                   challenge many of the assumptions underlying this work,
                   arguing that such a claim depends on one's definition of
                   explanation, and that testing it needs to take into account
                   all elements of the model, using a rigorous experimental
                   design. We propose four alternative tests to determine
                   when/whether attention can be used as explanation: a simple
                   uniform-weights baseline; a variance calibration based on
                   multiple random seed runs; a diagnostic framework using
                   frozen weights from pretrained models; and an end-to-end
                   adversarial attention training protocol. Each allows for
                   meaningful interpretation of attention mechanisms in RNN
                   models. We show that even when reliable adversarial
                   distributions can be found, they don't perform well on the
                   simple diagnostic, indicating that prior work does not
                   disprove the usefulness of attention mechanisms for
                   explainability.",
  month         =  aug,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1908.04626"
}

@BOOK{Bishop2006-gm,
  title     = "Pattern recognition and machine learning",
  author    = "Bishop, Christopher M",
  publisher = "Springer Science+ Business Media",
  abstract  = "Pattern recognition has its origins in engineering, whereas
               machine learning grew out of computer science. However, these
               activities can be viewed as two facets of the same field, and
               together they have undergone substantial development over the
               past ten years. In …",
  year      =  2006
}

@ARTICLE{Raffel2019-af,
  title         = "Exploring the Limits of Transfer Learning with a Unified
                   Text-to-Text Transformer",
  author        = "Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee,
                   Katherine and Narang, Sharan and Matena, Michael and Zhou,
                   Yanqi and Li, Wei and Liu, Peter J",
  abstract      = "Transfer learning, where a model is first pre-trained on a
                   data-rich task before being fine-tuned on a downstream task,
                   has emerged as a powerful technique in natural language
                   processing (NLP). The effectiveness of transfer learning has
                   given rise to a diversity of approaches, methodology, and
                   practice. In this paper, we explore the landscape of transfer
                   learning techniques for NLP by introducing a unified
                   framework that converts every language problem into a
                   text-to-text format. Our systematic study compares
                   pre-training objectives, architectures, unlabeled datasets,
                   transfer approaches, and other factors on dozens of language
                   understanding tasks. By combining the insights from our
                   exploration with scale and our new ``Colossal Clean Crawled
                   Corpus'', we achieve state-of-the-art results on many
                   benchmarks covering summarization, question answering, text
                   classification, and more. To facilitate future work on
                   transfer learning for NLP, we release our dataset,
                   pre-trained models, and code.",
  month         =  oct,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1910.10683"
}

@MISC{Zadrozny2002-lc,
  title   = "Transforming classifier scores into accurate multiclass probability
             estimates",
  author  = "Zadrozny, Bianca and Elkan, Charles",
  journal = "Proceedings of the eighth ACM SIGKDD international conference on
             Knowledge discovery and data mining - KDD '02",
  year    =  2002,
  doi     = "10.1145/775047.775151"
}

@ARTICLE{Peskov2019-bp,
  title         = "Mitigating Noisy Inputs for Question Answering",
  author        = "Peskov, Denis and Barrow, Joe and Rodriguez, Pedro and
                   Neubig, Graham and Boyd-Graber, Jordan",
  abstract      = "Natural language processing systems are often downstream of
                   unreliable inputs: machine translation, optical character
                   recognition, or speech recognition. For instance, virtual
                   assistants can only answer your questions after understanding
                   your speech. We investigate and mitigate the effects of noise
                   from Automatic Speech Recognition systems on two factoid
                   Question Answering (QA) tasks. Integrating confidences into
                   the model and forced decoding of unknown words are
                   empirically shown to improve the accuracy of downstream
                   neural QA systems. We create and train models on a synthetic
                   corpus of over 500,000 noisy sentences and evaluate on two
                   human corpora from Quizbowl and Jeopardy! competitions.",
  month         =  aug,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1908.02914"
}

@ARTICLE{Garg2019-by,
  title         = "{TANDA}: Transfer and Adapt Pre-Trained Transformer Models
                   for Answer Sentence Selection",
  author        = "Garg, Siddhant and Vu, Thuy and Moschitti, Alessandro",
  abstract      = "We propose TANDA, an effective technique for fine-tuning
                   pre-trained Transformer models for natural language tasks.
                   Specifically, we first transfer a pre-trained model into a
                   model for a general task by fine-tuning it with a large and
                   high-quality dataset. We then perform a second fine-tuning
                   step to adapt the transferred model to the target domain. We
                   demonstrate the benefits of our approach for answer sentence
                   selection, which is a well-known inference task in Question
                   Answering. We built a large scale dataset to enable the
                   transfer step, exploiting the Natural Questions dataset. Our
                   approach establishes the state of the art on two well-known
                   benchmarks, WikiQA and TREC-QA, achieving MAP scores of 92\%
                   and 94.3\%, respectively, which largely outperform the
                   previous highest scores of 83.4\% and 87.5\%, obtained in
                   very recent work. We empirically show that TANDA generates
                   more stable and robust models reducing the effort required
                   for selecting optimal hyper-parameters. Additionally, we show
                   that the transfer step of TANDA makes the adaptation step
                   more robust to noise. This enables a more effective use of
                   noisy datasets for fine-tuning. Finally, we also confirm the
                   positive impact of TANDA in an industrial setting, using
                   domain specific datasets subject to different types of noise.",
  month         =  nov,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1911.04118"
}

@ARTICLE{Zhang2018-nm,
  title         = "Accelerating Neural Transformer via an Average Attention
                   Network",
  author        = "Zhang, Biao and Xiong, Deyi and Su, Jinsong",
  abstract      = "With parallelizable attention networks, the neural
                   Transformer is very fast to train. However, due to the
                   auto-regressive architecture and self-attention in the
                   decoder, the decoding procedure becomes slow. To alleviate
                   this issue, we propose an average attention network as an
                   alternative to the self-attention network in the decoder of
                   the neural Transformer. The average attention network
                   consists of two layers, with an average layer that models
                   dependencies on previous positions and a gating layer that is
                   stacked over the average layer to enhance the expressiveness
                   of the proposed attention network. We apply this network on
                   the decoder part of the neural Transformer to replace the
                   original target-side self-attention model. With masking
                   tricks and dynamic programming, our model enables the neural
                   Transformer to decode sentences over four times faster than
                   its original version with almost no loss in training time and
                   translation performance. We conduct a series of experiments
                   on WMT17 translation tasks, where on 6 different language
                   pairs, we obtain robust and consistent speed-ups in decoding.",
  month         =  may,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1805.00631"
}

@MISC{Xiao2019-sl,
  title   = "Sharing Attention Weights for Fast Transformer",
  author  = "Xiao, Tong and Li, Yinqiao and Zhu, Jingbo and Yu, Zhengtao and
             Liu, Tongran",
  journal = "Proceedings of the Twenty-Eighth International Joint Conference on
             Artificial Intelligence",
  year    =  2019,
  doi     = "10.24963/ijcai.2019/735"
}

@ARTICLE{Child2019-zy,
  title         = "Generating Long Sequences with Sparse Transformers",
  author        = "Child, Rewon and Gray, Scott and Radford, Alec and Sutskever,
                   Ilya",
  abstract      = "Transformers are powerful sequence models, but require time
                   and memory that grows quadratically with the sequence length.
                   In this paper we introduce sparse factorizations of the
                   attention matrix which reduce this to $O(n \sqrt{n})$. We
                   also introduce a) a variation on architecture and
                   initialization to train deeper networks, b) the recomputation
                   of attention matrices to save memory, and c) fast attention
                   kernels for training. We call networks with these changes
                   Sparse Transformers, and show they can model sequences tens
                   of thousands of timesteps long using hundreds of layers. We
                   use the same architecture to model images, audio, and text
                   from raw bytes, setting a new state of the art for density
                   modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate
                   unconditional samples that demonstrate global coherence and
                   great diversity, and show it is possible in principle to use
                   self-attention to model sequences of length one million or
                   more.",
  month         =  apr,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1904.10509"
}

@ARTICLE{Li2019-ap,
  title         = "Specializing Word Embeddings (for Parsing) by Information
                   Bottleneck",
  author        = "Li, Xiang Lisa and Eisner, Jason",
  abstract      = "Pre-trained word embeddings like ELMo and BERT contain rich
                   syntactic and semantic information, resulting in
                   state-of-the-art performance on various tasks. We propose a
                   very fast variational information bottleneck (VIB) method to
                   nonlinearly compress these embeddings, keeping only the
                   information that helps a discriminative parser. We compress
                   each word embedding to either a discrete tag or a continuous
                   vector. In the discrete version, our automatically compressed
                   tags form an alternative tag set: we show experimentally that
                   our tags capture most of the information in traditional POS
                   tag annotations, but our tag sequences can be parsed more
                   accurately at the same level of tag granularity. In the
                   continuous version, we show experimentally that moderately
                   compressing the word embeddings by our method yields a more
                   accurate parser in 8 of 9 languages, unlike simple
                   dimensionality reduction.",
  month         =  oct,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1910.00163"
}

@ARTICLE{Wang2016-js,
  title         = "A Compare-Aggregate Model for Matching Text Sequences",
  author        = "Wang, Shuohang and Jiang, Jing",
  abstract      = "Many NLP tasks including machine comprehension, answer
                   selection and text entailment require the comparison between
                   sequences. Matching the important units between sequences is
                   a key to solve these problems. In this paper, we present a
                   general ``compare-aggregate'' framework that performs
                   word-level matching followed by aggregation using
                   Convolutional Neural Networks. We particularly focus on the
                   different comparison functions we can use to match two
                   vectors. We use four different datasets to evaluate the
                   model. We find that some simple comparison functions based on
                   element-wise operations can work better than standard neural
                   network and neural tensor network.",
  month         =  nov,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1611.01747"
}

@ARTICLE{Yoon2019-gh,
  title         = "A Compare-Aggregate Model with Latent Clustering for Answer
                   Selection",
  author        = "Yoon, Seunghyun and Dernoncourt, Franck and Kim, Doo Soon and
                   Bui, Trung and Jung, Kyomin",
  abstract      = "In this paper, we propose a novel method for a sentence-level
                   answer-selection task that is a fundamental problem in
                   natural language processing. First, we explore the effect of
                   additional information by adopting a pretrained language
                   model to compute the vector representation of the input text
                   and by applying transfer learning from a large-scale corpus.
                   Second, we enhance the compare-aggregate model by proposing a
                   novel latent clustering method to compute additional
                   information within the target corpus and by changing the
                   objective function from listwise to pointwise. To evaluate
                   the performance of the proposed approaches, experiments are
                   performed with the WikiQA and TREC-QA datasets. The empirical
                   results demonstrate the superiority of our proposed approach,
                   which achieve state-of-the-art performance for both datasets.",
  month         =  may,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1905.12897"
}

@ARTICLE{Provilkov2019-sa,
  title         = "{BPE}-Dropout: Simple and Effective Subword Regularization",
  author        = "Provilkov, Ivan and Emelianenko, Dmitrii and Voita, Elena",
  abstract      = "Subword segmentation is widely used to address the open
                   vocabulary problem in machine translation. The dominant
                   approach to subword segmentation is Byte Pair Encoding (BPE),
                   which keeps the most frequent words intact while splitting
                   the rare ones into multiple tokens. While multiple
                   segmentations are possible even with the same vocabulary, BPE
                   splits words into unique sequences; this may prevent a model
                   from better learning the compositionality of words and being
                   robust to segmentation errors. So far, the only way to
                   overcome this BPE imperfection, its deterministic nature, was
                   to create another subword segmentation algorithm (Kudo,
                   2018). In contrast, we show that BPE itself incorporates the
                   ability to produce multiple segmentations of the same word.
                   We introduce BPE-dropout - simple and effective subword
                   regularization method based on and compatible with
                   conventional BPE. It stochastically corrupts the segmentation
                   procedure of BPE, which leads to producing multiple
                   segmentations within the same fixed BPE framework. Using
                   BPE-dropout during training and the standard BPE during
                   inference improves translation quality up to 3 BLEU compared
                   to BPE and up to 0.9 BLEU compared to the previous subword
                   regularization.",
  month         =  oct,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1910.13267"
}

@ARTICLE{Liu2019-cc,
  title         = "{RoBERTa}: A Robustly Optimized {BERT} Pretraining Approach",
  author        = "Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei
                   and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis,
                   Mike and Zettlemoyer, Luke and Stoyanov, Veselin",
  abstract      = "Language model pretraining has led to significant performance
                   gains but careful comparison between different approaches is
                   challenging. Training is computationally expensive, often
                   done on private datasets of different sizes, and, as we will
                   show, hyperparameter choices have significant impact on the
                   final results. We present a replication study of BERT
                   pretraining (Devlin et al., 2019) that carefully measures the
                   impact of many key hyperparameters and training data size. We
                   find that BERT was significantly undertrained, and can match
                   or exceed the performance of every model published after it.
                   Our best model achieves state-of-the-art results on GLUE,
                   RACE and SQuAD. These results highlight the importance of
                   previously overlooked design choices, and raise questions
                   about the source of recently reported improvements. We
                   release our models and code.",
  month         =  jul,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1907.11692"
}

@ARTICLE{Tyagi2019-wx,
  title         = "Fast Intent Classification for Spoken Language Understanding",
  author        = "Tyagi, Akshit and Sharma, Varun and Gupta, Rahul and Samson,
                   Lynn and Zhuang, Nan and Wang, Zihang and Campbell, Bill",
  abstract      = "Spoken Language Understanding (SLU) systems consist of
                   several machine learning components operating together (e.g.
                   intent classification, named entity recognition and
                   resolution). Deep learning models have obtained state of the
                   art results on several of these tasks, largely attributed to
                   their better modeling capacity. However, an increase in
                   modeling capacity comes with added costs of higher latency
                   and energy usage, particularly when operating on low
                   complexity devices. To address the latency and computational
                   complexity issues, we explore a BranchyNet scheme on an
                   intent classification scheme within SLU systems. The
                   BranchyNet scheme when applied to a high complexity model,
                   adds exit points at various stages in the model allowing
                   early decision making for a set of queries to the SLU model.
                   We conduct experiments on the Facebook Semantic Parsing
                   dataset with two candidate model architectures for intent
                   classification. Our experiments show that the BranchyNet
                   scheme provides gains in terms of computational complexity
                   without compromising model accuracy. We also conduct
                   analytical studies regarding the improvements in the
                   computational cost, distribution of utterances that egress
                   from various exit points and the impact of adding more
                   complexity to models with the BranchyNet scheme.",
  month         =  dec,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1912.01728"
}

@ARTICLE{Teerapittayanon2017-nl,
  title         = "{BranchyNet}: Fast Inference via Early Exiting from Deep
                   Neural Networks",
  author        = "Teerapittayanon, Surat and McDanel, Bradley and Kung, H T",
  abstract      = "Deep neural networks are state of the art methods for many
                   learning tasks due to their ability to extract increasingly
                   better features at each network layer. However, the improved
                   performance of additional layers in a deep network comes at
                   the cost of added latency and energy usage in feedforward
                   inference. As networks continue to get deeper and larger,
                   these costs become more prohibitive for real-time and
                   energy-sensitive applications. To address this issue, we
                   present BranchyNet, a novel deep network architecture that is
                   augmented with additional side branch classifiers. The
                   architecture allows prediction results for a large portion of
                   test samples to exit the network early via these branches
                   when samples can already be inferred with high confidence.
                   BranchyNet exploits the observation that features learned at
                   an early layer of a network may often be sufficient for the
                   classification of many data points. For more difficult
                   samples, which are expected less frequently, BranchyNet will
                   use further or all network layers to provide the best
                   likelihood of correct prediction. We study the BranchyNet
                   architecture using several well-known networks (LeNet,
                   AlexNet, ResNet) and datasets (MNIST, CIFAR10) and show that
                   it can both improve accuracy and significantly reduce the
                   inference time of the network.",
  month         =  sep,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.NE",
  eprint        = "1709.01686"
}

@INPROCEEDINGS{Gallagher2019-zz,
  title       = "Joint Optimization of Cascade Ranking Models",
  author      = "Gallagher, Luke and Chen, Ruey-Cheng and Blanco, Roi and
                 Culpepper, J Shane",
  booktitle   = "Proceedings of the Twelfth ACM International Conference on Web
                 Search and Data Mining",
  publisher   = "dl.acm.org",
  institution = "ACM",
  pages       = "15--23",
  abstract    = "Reducing excessive costs in feature acquisition and model
                 evaluation has been a long- standing challenge in
                 learning-to-rank systems. A cascaded ranking architecture turns
                 ranking into a pipeline of multiple stages, and has been shown
                 to be a powerful approach to balancing efficiency and
                 effectiveness trade-offs in large-scale search systems.
                 However, learning a cascade model is often complex, and usually
                 performed stagewise independently across the entire ranking
                 pipeline. In this work we show that learning a cascade ranking
                 …",
  year        =  2019
}

@ARTICLE{Kitaev2020-wc,
  title         = "Reformer: The Efficient Transformer",
  author        = "Kitaev, Nikita and Kaiser, Łukasz and Levskaya, Anselm",
  abstract      = "Large Transformer models routinely achieve state-of-the-art
                   results on a number of tasks but training these models can be
                   prohibitively costly, especially on long sequences. We
                   introduce two techniques to improve the efficiency of
                   Transformers. For one, we replace dot-product attention by
                   one that uses locality-sensitive hashing, changing its
                   complexity from O($L^2$) to O($L\log L$), where $L$ is the
                   length of the sequence. Furthermore, we use reversible
                   residual layers instead of the standard residuals, which
                   allows storing activations only once in the training process
                   instead of $N$ times, where $N$ is the number of layers. The
                   resulting model, the Reformer, performs on par with
                   Transformer models while being much more memory-efficient and
                   much faster on long sequences.",
  month         =  jan,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2001.04451"
}

@ARTICLE{Fisas2015-mw,
  title   = "On the Discoursive Structure of Computer Graphics Research Papers",
  author  = "Fisas, Beatriz and Saggion, Horacio and Ronzano, Francesco",
  journal = "LAW@ NAACL-HLT",
  volume  =  2015,
  pages   = "42--51",
  year    =  2015
}

@ARTICLE{Fiesler2018-le,
  title     = "“participant” perceptions of Twitter research ethics",
  author    = "Fiesler, Casey and Proferes, Nicholas",
  journal   = "Social media + society",
  publisher = "SAGE Publications",
  volume    =  4,
  number    =  1,
  pages     =  205630511876336,
  abstract  = "Social computing systems such as Twitter present new research
               sites that have provided billions of data points to researchers.
               However, the availability of public social media data has also
               presented ethical challenges. As the research community works to
               create ethical norms, we should be considering users’ concerns as
               well. With this in mind, we report on an exploratory survey of
               Twitter users’ perceptions of the use of tweets in research.
               Within our survey sample, few users were previously aware that
               their public tweets could be used by researchers, and the
               majority felt that researchers should not be able to use tweets
               without consent. However, we find that these attitudes are highly
               contextual, depending on factors such as how the research is
               conducted or disseminated, who is conducting it, and what the
               study is about. The findings of this study point to potential
               best practices for researchers conducting observation and
               analysis of public data.",
  month     =  jan,
  year      =  2018,
  doi       = "10.1177/2056305118763366",
  issn      = "2056-3051",
  language  = "en"
}

@ARTICLE{Ayers2018-rr,
  title     = "Don't quote me: reverse identification of research participants
               in social media studies",
  author    = "Ayers, John W and Caputi, Theodore L and Nebeker, Camille and
               Dredze, Mark",
  journal   = "npj digital medicine",
  publisher = "Springer Science and Business Media LLC",
  volume    =  1,
  number    =  1,
  pages     =  30,
  abstract  = "We investigated if participants in social media surveillance
               studies could be reverse identified by reviewing all articles
               published on PubMed in 2015 or 2016 with the words ``Twitter''
               and either ``read,'' ``coded,'' or ``content'' in the title or
               abstract. Seventy-two percent (95\% CI: 63-80) of articles quoted
               at least one participant's tweet and searching for the quoted
               content led to the participant 84\% (95\% CI: 74-91) of the time.
               Twenty-one percent (95\% CI: 13-29) of articles disclosed a
               participant's Twitter username thereby making the participant
               immediately identifiable. Only one article reported obtaining
               consent to disclose identifying information and institutional
               review board (IRB) involvement was mentioned in only 40\% (95\%
               CI: 31-50) of articles, of which 17\% (95\% CI: 10-25) received
               IRB-approval and 23\% (95\% CI:16-32) were deemed exempt.
               Biomedical publications are routinely including identifiable
               information by quoting tweets or revealing usernames which, in
               turn, violates ICMJE ethical standards governing scientific
               ethics, even though said content is scientifically unnecessary.
               We propose that authors convey aggregate findings without
               revealing participants' identities, editors refuse to publish
               reports that reveal a participant's identity, and IRBs attend to
               these privacy issues when reviewing studies involving social
               media data. These strategies together will ensure participants
               are protected going forward.",
  month     =  aug,
  year      =  2018,
  keywords  = "Epidemiology; Translational research",
  doi       = "10.1038/s41746-018-0036-2",
  pmc       = "PMC6550214",
  pmid      =  31304312,
  issn      = "2398-6352",
  language  = "en"
}

@ARTICLE{Jain2019-fg,
  title         = "Attention is not Explanation",
  author        = "Jain, Sarthak and Wallace, Byron C",
  abstract      = "Attention mechanisms have seen wide adoption in neural NLP
                   models. In addition to improving predictive performance,
                   these are often touted as affording transparency: models
                   equipped with attention provide a distribution over
                   attended-to input units, and this is often presented (at
                   least implicitly) as communicating the relative importance of
                   inputs. However, it is unclear what relationship exists
                   between attention weights and model outputs. In this work, we
                   perform extensive experiments across a variety of NLP tasks
                   that aim to assess the degree to which attention weights
                   provide meaningful `explanations' for predictions. We find
                   that they largely do not. For example, learned attention
                   weights are frequently uncorrelated with gradient-based
                   measures of feature importance, and one can identify very
                   different attention distributions that nonetheless yield
                   equivalent predictions. Our findings show that standard
                   attention modules do not provide meaningful explanations and
                   should not be treated as though they do. Code for all
                   experiments is available at
                   https://github.com/successar/AttentionExplanation.",
  month         =  feb,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1902.10186"
}

@ARTICLE{Schwartz2020-ho,
  title         = "The Right Tool for the Job: Matching Model and Instance
                   Complexities",
  author        = "Schwartz, Roy and Stanovsky, Gabi and Swayamdipta, Swabha and
                   Dodge, Jesse and Smith, Noah A",
  abstract      = "As NLP models become larger, executing a trained model
                   requires significant computational resources incurring
                   monetary and environmental costs. To better respect a given
                   inference budget, we propose a modification to contextual
                   representation fine-tuning which, during inference, allows
                   for an early (and fast) ``exit'' from neural network
                   calculations for simple instances, and late (and accurate)
                   exit for hard instances. To achieve this, we add classifiers
                   to different layers of BERT and use their calibrated
                   confidence scores to make early exit decisions. We test our
                   proposed modification on five different datasets in two
                   tasks: three text classification datasets and two natural
                   language inference benchmarks. Our method presents a
                   favorable speed/accuracy tradeoff in almost all cases,
                   producing models which are up to five times faster than the
                   state of the art, while preserving their accuracy. Our method
                   also requires almost no additional training resources (in
                   either time or parameters) compared to the baseline BERT
                   model. Finally, our method alleviates the need for costly
                   retraining of multiple models at different levels of
                   efficiency; we allow users to control the inference
                   speed/accuracy tradeoff using a single trained model, by
                   setting a single variable at inference time. We publicly
                   release our code.",
  month         =  apr,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2004.07453"
}

@INPROCEEDINGS{Metzler2005-ql,
  title     = "A Markov random field model for term dependencies",
  author    = "Metzler, Donald and Croft, W Bruce",
  booktitle = "Proceedings of the 28th annual international ACM SIGIR conference
               on Research and development in information retrieval",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "472--479",
  series    = "SIGIR '05",
  month     =  aug,
  year      =  2005,
  keywords  = "term dependence, phrases, information retrieval, Markov random
               fields",
  doi       = "10.1145/1076034.1076115",
  isbn      =  9781595930347
}

@INPROCEEDINGS{Smucker2007-aa,
  title     = "A comparison of statistical significance tests for information
               retrieval evaluation",
  author    = "Smucker, Mark D and Allan, James and Carterette, Ben",
  booktitle = "Proceedings of the sixteenth ACM conference on Conference on
               information and knowledge management",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "623--632",
  series    = "CIKM '07",
  month     =  nov,
  year      =  2007,
  keywords  = "statistical significance, hypothesis test, student's t-test,
               sign, permutation, wilcoxon, randomization, bootstrap",
  doi       = "10.1145/1321440.1321528",
  isbn      =  9781595938039
}

@ARTICLE{MacAvaney2020-rn,
  title         = "Efficient Document Re-Ranking for Transformers by
                   Precomputing Term Representations",
  author        = "MacAvaney, Sean and Nardini, Franco Maria and Perego,
                   Raffaele and Tonellotto, Nicola and Goharian, Nazli and
                   Frieder, Ophir",
  abstract      = "Deep pretrained transformer networks are effective at various
                   ranking tasks, such as question answering and ad-hoc document
                   ranking. However, their computational expenses deem them
                   cost-prohibitive in practice. Our proposed approach, called
                   PreTTR (Precomputing Transformer Term Representations),
                   considerably reduces the query-time latency of deep
                   transformer networks (up to a 42x speedup on web document
                   ranking) making these networks more practical to use in a
                   real-time ranking scenario. Specifically, we precompute part
                   of the document term representations at indexing time without
                   a query, and merge them with the query representation at
                   query time to compute the final ranking score. Due to the
                   large size of the token representations, we also propose an
                   effective approach to reduce the storage requirement by
                   training a compression layer to match attention scores. Our
                   compression technique reduces the storage required up to 95\%
                   and it can be applied without a substantial degradation in
                   ranking performance.",
  month         =  apr,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "2004.14255"
}

@ARTICLE{MacAvaney2020-en,
  title         = "Training Curricula for Open Domain Answer Re-Ranking",
  author        = "MacAvaney, Sean and Nardini, Franco Maria and Perego,
                   Raffaele and Tonellotto, Nicola and Goharian, Nazli and
                   Frieder, Ophir",
  abstract      = "In precision-oriented tasks like answer ranking, it is more
                   important to rank many relevant answers highly than to
                   retrieve all relevant answers. It follows that a good ranking
                   strategy would be to learn how to identify the easiest
                   correct answers first (i.e., assign a high ranking score to
                   answers that have characteristics that usually indicate
                   relevance, and a low ranking score to those with
                   characteristics that do not), before incorporating more
                   complex logic to handle difficult cases (e.g., semantic
                   matching or reasoning). In this work, we apply this idea to
                   the training of neural answer rankers using curriculum
                   learning. We propose several heuristics to estimate the
                   difficulty of a given training sample. We show that the
                   proposed heuristics can be used to build a training
                   curriculum that down-weights difficult samples early in the
                   training process. As the training process progresses, our
                   approach gradually shifts to weighting all samples equally,
                   regardless of difficulty. We present a comprehensive
                   evaluation of our proposed idea on three answer ranking
                   datasets. Results show that our approach leads to superior
                   performance of two leading neural ranking architectures,
                   namely BERT and ConvKNRM, using both pointwise and pairwise
                   losses. When applied to a BERT-based ranker, our method
                   yields up to a 4\% improvement in MRR and a 9\% improvement
                   in P@1 (compared to the model trained without a curriculum).
                   This results in models that can achieve comparable
                   performance to more expensive state-of-the-art techniques.",
  month         =  apr,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "2004.14269"
}

@ARTICLE{Kikkawa2022-fm,
  title    = "Dataset of first appearances of the scholarly bibliographic
              references on Wikipedia articles",
  author   = "Kikkawa, Jiro and Takaku, Masao and Yoshikane, Fuyuki",
  journal  = "Scientific data",
  volume   =  9,
  number   =  1,
  pages    =  85,
  abstract = "Referencing scholarly documents as information sources on
              Wikipedia is important because it supports or improves the quality
              of Wikipedia content. Several studies have been conducted
              regarding scholarly references on Wikipedia; however, little is
              known of the editors and their edits contributing to add the
              scholarly references on Wikipedia. In this study, we develop a
              methodology to detect the oldest scholarly reference added to
              Wikipedia articles by which a certain paper is uniquely
              identifiable as the ``first appearance of the scholarly
              reference.'' We identified the first appearances of 923,894
              scholarly references (611,119 unique DOIs) in 180,795 unique pages
              on English Wikipedia as of March 1, 2017 and stored them in the
              dataset. Moreover, we assessed the precision of the dataset, which
              was highly precise regardless of the research field. Finally, we
              demonstrate the potential of our dataset. This dataset is unique
              and attracts those who are interested in how the scholarly
              references on Wikipedia grew and which editors added them.",
  month    =  mar,
  year     =  2022,
  doi      = "10.1038/s41597-022-01190-z",
  pmc      = "PMC8921307",
  pmid     =  35288593,
  issn     = "2052-4463",
  language = "en"
}

@ARTICLE{Singh2020-nb,
  title         = "Wikipedia Citations: A comprehensive dataset of citations
                   with identifiers extracted from English Wikipedia",
  author        = "Singh, Harshdeep and West, Robert and Colavizza, Giovanni",
  abstract      = "Wikipedia's contents are based on reliable and published
                   sources. To this date, relatively little is known about what
                   sources Wikipedia relies on, in part because extracting
                   citations and identifying cited sources is challenging. To
                   close this gap, we release Wikipedia Citations, a
                   comprehensive dataset of citations extracted from Wikipedia.
                   A total of 29.3M citations were extracted from 6.1M English
                   Wikipedia articles as of May 2020, and classified as being to
                   books, journal articles or Web contents. We were thus able to
                   extract 4.0M citations to scholarly publications with known
                   identifiers -- including DOI, PMC, PMID, and ISBN -- and
                   further equip an extra 261K citations with DOIs from
                   Crossref. As a result, we find that 6.7\% of Wikipedia
                   articles cite at least one journal article with an associated
                   DOI, and that Wikipedia cites just 2\% of all articles with a
                   DOI currently indexed in the Web of Science. We release our
                   code to allow the community to extend upon our work and
                   update the dataset in the future.",
  month         =  jul,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.DL",
  eprint        = "2007.07022"
}

@ARTICLE{McCoy2023-nh,
  title         = "Embers of autoregression: Understanding large language models
                   through the problem they are trained to solve",
  author        = "McCoy, R Thomas and Yao, Shunyu and Friedman, Dan and Hardy,
                   Matthew and Griffiths, Thomas L",
  abstract      = "The widespread adoption of large language models (LLMs) makes
                   it important to recognize their strengths and limitations. We
                   argue that in order to develop a holistic understanding of
                   these systems we need to consider the problem that they were
                   trained to solve: next-word prediction over Internet text. By
                   recognizing the pressures that this task exerts we can make
                   predictions about the strategies that LLMs will adopt,
                   allowing us to reason about when they will succeed or fail.
                   This approach - which we call the teleological approach -
                   leads us to identify three factors that we hypothesize will
                   influence LLM accuracy: the probability of the task to be
                   performed, the probability of the target output, and the
                   probability of the provided input. We predict that LLMs will
                   achieve higher accuracy when these probabilities are high
                   than when they are low - even in deterministic settings where
                   probability should not matter. To test our predictions, we
                   evaluate two LLMs (GPT-3.5 and GPT-4) on eleven tasks, and we
                   find robust evidence that LLMs are influenced by probability
                   in the ways that we have hypothesized. In many cases, the
                   experiments reveal surprising failure modes. For instance,
                   GPT-4's accuracy at decoding a simple cipher is 51\% when the
                   output is a high-probability word sequence but only 13\% when
                   it is low-probability. These results show that AI
                   practitioners should be careful about using LLMs in
                   low-probability situations. More broadly, we conclude that we
                   should not evaluate LLMs as if they are humans but should
                   instead treat them as a distinct type of system - one that
                   has been shaped by its own particular set of pressures.",
  month         =  sep,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2309.13638"
}

@ARTICLE{Gardner2020-kx,
  title         = "Determining Question-Answer Plausibility in Crowdsourced
                   Datasets Using Multi-Task Learning",
  author        = "Gardner, Rachel and Varma, Maya and Zhu, Clare and Krishna,
                   Ranjay",
  abstract      = "Datasets extracted from social networks and online forums are
                   often prone to the pitfalls of natural language, namely the
                   presence of unstructured and noisy data. In this work, we
                   seek to enable the collection of high-quality question-answer
                   datasets from social media by proposing a novel task for
                   automated quality analysis and data cleaning: question-answer
                   (QA) plausibility. Given a machine or user-generated question
                   and a crowd-sourced response from a social media user, we
                   determine if the question and response are valid; if so, we
                   identify the answer within the free-form response. We design
                   BERT-based models to perform the QA plausibility task, and we
                   evaluate the ability of our models to generate a clean,
                   usable question-answer dataset. Our highest-performing
                   approach consists of a single-task model which determines the
                   plausibility of the question, followed by a multi-task model
                   which evaluates the plausibility of the response as well as
                   extracts answers (Question Plausibility AUROC=0.75, Response
                   Plausibility AUROC=0.78, Answer Extraction F1=0.665).",
  month         =  nov,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2011.04883"
}

@INPROCEEDINGS{Duh2008-lm,
  title     = "Learning to Rank with Partially-labeled Data",
  author    = "Duh, Kevin and Kirchhoff, Katrin",
  booktitle = "Proceedings of the 31st Annual International ACM SIGIR Conference
               on Research and Development in Information Retrieval",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "251--258",
  series    = "SIGIR '08",
  year      =  2008,
  keywords  = "boosting, information retrieval, kernel principal components
               analysis, learning to rank, transductive learning",
  doi       = "10.1145/1390334.1390379",
  isbn      =  9781605581644
}

@ARTICLE{Peters2018-be,
  title         = "Deep contextualized word representations",
  author        = "Peters, Matthew E and Neumann, Mark and Iyyer, Mohit and
                   Gardner, Matt and Clark, Christopher and Lee, Kenton and
                   Zettlemoyer, Luke",
  abstract      = "We introduce a new type of deep contextualized word
                   representation that models both (1) complex characteristics
                   of word use (e.g., syntax and semantics), and (2) how these
                   uses vary across linguistic contexts (i.e., to model
                   polysemy). Our word vectors are learned functions of the
                   internal states of a deep bidirectional language model
                   (biLM), which is pre-trained on a large text corpus. We show
                   that these representations can be easily added to existing
                   models and significantly improve the state of the art across
                   six challenging NLP problems, including question answering,
                   textual entailment and sentiment analysis. We also present an
                   analysis showing that exposing the deep internals of the
                   pre-trained network is crucial, allowing downstream models to
                   mix different types of semi-supervision signals.",
  month         =  feb,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1802.05365"
}

@INPROCEEDINGS{Kumaran2009-wo,
  title     = "Reducing Long Queries Using Query Quality Predictors",
  author    = "Kumaran, Giridhar and Carvalho, Vitor R",
  booktitle = "Proceedings of the 32Nd International ACM SIGIR Conference on
               Research and Development in Information Retrieval",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "564--571",
  series    = "SIGIR '09",
  year      =  2009,
  keywords  = "long queries, query quality, query reduction, verbose queries",
  doi       = "10.1145/1571941.1572038",
  isbn      =  9781605584836
}

@INPROCEEDINGS{Sennrich2019-pn,
  title     = "Revisiting Low-Resource Neural Machine Translation: A Case Study",
  author    = "Sennrich, Rico and Zhang, Biao",
  booktitle = "Proceedings of the 57th Annual Meeting of the Association for
               Computational Linguistics",
  publisher = "Association for Computational Linguistics",
  address   = "Florence, Italy",
  pages     = "211--221",
  abstract  = "It has been shown that the performance of neural machine
               translation (NMT) drops starkly in low-resource conditions,
               underperforming phrase-based statistical machine translation
               (PBSMT) and requiring large amounts of auxiliary data to achieve
               competitive results. In this paper, we re-assess the validity of
               these results, arguing that they are the result of lack of system
               adaptation to low-resource settings. We discuss some pitfalls to
               be aware of when training low-resource NMT systems, and recent
               techniques that have shown to be especially helpful in
               low-resource settings, resulting in a set of best practices for
               low-resource NMT. In our experiments on German--English with
               different amounts of IWSLT14 training data, we show that, without
               the use of any auxiliary monolingual or multilingual data, an
               optimized NMT system can outperform PBSMT with far less data than
               previously claimed. We also apply these techniques to a
               low-resource Korean--English dataset, surpassing previously
               reported results by 4 BLEU.",
  month     =  jul,
  year      =  2019,
  doi       = "10.18653/v1/P19-1021"
}

@INPROCEEDINGS{Severyn2015-pm,
  title     = "Learning to Rank Short Text Pairs with Convolutional Deep Neural
               Networks",
  author    = "Severyn, Aliaksei and Moschitti, Alessandro",
  booktitle = "SIGIR",
  year      =  2015
}

@INPROCEEDINGS{Kovaleva2019-lj,
  title     = "Revealing the Dark Secrets of {BERT}",
  author    = "Kovaleva, Olga and Romanov, Alexey and Rogers, Anna and
               Rumshisky, Anna",
  booktitle = "Proceedings of the 2019 Conference on Empirical Methods in
               Natural Language Processing and the 9th International Joint
               Conference on Natural Language Processing (EMNLP-IJCNLP)",
  publisher = "Association for Computational Linguistics",
  address   = "Hong Kong, China",
  pages     = "4365--4374",
  abstract  = "BERT-based architectures currently give state-of-the-art
               performance on many NLP tasks, but little is known about the
               exact mechanisms that contribute to its success. In the current
               work, we focus on the interpretation of self-attention, which is
               one of the fundamental underlying components of BERT. Using a
               subset of GLUE tasks and a set of handcrafted
               features-of-interest, we propose the methodology and carry out a
               qualitative and quantitative analysis of the information encoded
               by the individual BERT's heads. Our findings suggest that there
               is a limited set of attention patterns that are repeated across
               different heads, indicating the overall model
               overparametrization. While different heads consistently use the
               same attention patterns, they have varying impact on performance
               across different tasks. We show that manually disabling attention
               in certain heads leads to a performance improvement over the
               regular fine-tuned BERT models.",
  month     =  nov,
  year      =  2019,
  doi       = "10.18653/v1/D19-1445"
}

@INPROCEEDINGS{Gururangan2020-fm,
  title     = "Don{'}t Stop Pretraining: Adapt Language Models to Domains and
               Tasks",
  author    = "Gururangan, Suchin and Marasović, Ana and Swayamdipta, Swabha and
               Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A",
  booktitle = "Proceedings of the 58th Annual Meeting of the Association for
               Computational Linguistics",
  publisher = "Association for Computational Linguistics",
  address   = "Online",
  pages     = "8342--8360",
  abstract  = "Language models pretrained on text from a wide variety of sources
               form the foundation of today's NLP. In light of the success of
               these broad-coverage models, we investigate whether it is still
               helpful to tailor a pretrained model to the domain of a target
               task. We present a study across four domains (biomedical and
               computer science publications, news, and reviews) and eight
               classification tasks, showing that a second phase of pretraining
               in-domain (domain-adaptive pretraining) leads to performance
               gains, under both high- and low-resource settings. Moreover,
               adapting to the task's unlabeled data (task-adaptive pretraining)
               improves performance even after domain-adaptive pretraining.
               Finally, we show that adapting to a task corpus augmented using
               simple data selection strategies is an effective alternative,
               especially when resources for domain-adaptive pretraining might
               be unavailable. Overall, we consistently find that multi-phase
               adaptive pretraining offers large gains in task performance.",
  month     =  jul,
  year      =  2020,
  doi       = "10.18653/v1/2020.acl-main.740"
}

@ARTICLE{Lewis2020-wi,
  title         = "Pre-training via Paraphrasing",
  author        = "Lewis, Mike and Ghazvininejad, Marjan and Ghosh, Gargi and
                   Aghajanyan, Armen and Wang, Sida and Zettlemoyer, Luke",
  abstract      = "We introduce MARGE, a pre-trained sequence-to-sequence model
                   learned with an unsupervised multi-lingual multi-document
                   paraphrasing objective. MARGE provides an alternative to the
                   dominant masked language modeling paradigm, where we
                   self-supervise the reconstruction of target text by
                   retrieving a set of related texts (in many languages) and
                   conditioning on them to maximize the likelihood of generating
                   the original. We show it is possible to jointly learn to do
                   retrieval and reconstruction, given only a random
                   initialization. The objective noisily captures aspects of
                   paraphrase, translation, multi-document summarization, and
                   information retrieval, allowing for strong zero-shot
                   performance on several tasks. For example, with no additional
                   task-specific training we achieve BLEU scores of up to 35.8
                   for document translation. We further show that fine-tuning
                   gives strong performance on a range of discriminative and
                   generative tasks in many languages, making MARGE the most
                   generally applicable pre-training method to date.",
  month         =  jun,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2006.15020"
}

@INPROCEEDINGS{Lee2019-dm,
  title     = "Latent Retrieval for Weakly Supervised Open Domain Question
               Answering",
  author    = "Lee, Kenton and Chang, Ming-Wei and Toutanova, Kristina",
  booktitle = "Proceedings of the 57th Annual Meeting of the Association for
               Computational Linguistics",
  publisher = "Association for Computational Linguistics",
  address   = "Florence, Italy",
  pages     = "6086--6096",
  abstract  = "Recent work on open domain question answering (QA) assumes strong
               supervision of the supporting evidence and/or assumes a blackbox
               information retrieval (IR) system to retrieve evidence
               candidates. We argue that both are suboptimal, since gold
               evidence is not always available, and QA is fundamentally
               different from IR. We show for the first time that it is possible
               to jointly learn the retriever and reader from question-answer
               string pairs and without any IR system. In this setting, evidence
               retrieval from all of Wikipedia is treated as a latent variable.
               Since this is impractical to learn from scratch, we pre-train the
               retriever with an Inverse Cloze Task. We evaluate on open
               versions of five QA datasets. On datasets where the questioner
               already knows the answer, a traditional IR system such as BM25 is
               sufficient. On datasets where a user is genuinely seeking an
               answer, we show that learned retrieval is crucial, outperforming
               BM25 by up to 19 points in exact match.",
  month     =  jul,
  year      =  2019,
  doi       = "10.18653/v1/P19-1612"
}

@INPROCEEDINGS{Mielke2019-jw,
  title     = "What Kind of Language Is Hard to Language-Model?",
  author    = "Mielke, Sabrina J and Cotterell, Ryan and Gorman, Kyle and Roark,
               Brian and Eisner, Jason",
  booktitle = "Proceedings of the 57th Annual Meeting of the Association for
               Computational Linguistics",
  publisher = "Association for Computational Linguistics",
  address   = "Florence, Italy",
  pages     = "4975--4989",
  abstract  = "How language-agnostic are current state-of-the-art NLP tools? Are
               there some types of language that are easier to model with
               current methods? In prior work (Cotterell et al., 2018) we
               attempted to address this question for language modeling, and
               observed that recurrent neural network language models do not
               perform equally well over all the high-resource European
               languages found in the Europarl corpus. We speculated that
               inflectional morphology may be the primary culprit for the
               discrepancy. In this paper, we extend these earlier experiments
               to cover 69 languages from 13 language families using a
               multilingual Bible corpus. Methodologically, we introduce a new
               paired-sample multiplicative mixed-effects model to obtain
               language difficulty coefficients from at-least-pairwise parallel
               corpora. In other words, the model is aware of inter-sentence
               variation and can handle missing data. Exploiting this model, we
               show that ``translationese'' is not any easier to model than
               natively written language in a fair comparison. Trying to answer
               the question of what features difficult languages have in common,
               we try and fail to reproduce our earlier (Cotterell et al., 2018)
               observation about morphological complexity and instead reveal far
               simpler statistics of the data that seem to drive complexity in a
               much larger sample.",
  month     =  jul,
  year      =  2019,
  doi       = "10.18653/v1/P19-1491"
}

@ARTICLE{Turc2019-no,
  title         = "Well-Read Students Learn Better: On the Importance of
                   Pre-training Compact Models",
  author        = "Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and
                   Toutanova, Kristina",
  abstract      = "Recent developments in natural language representations have
                   been accompanied by large and expensive models that leverage
                   vast amounts of general-domain text through self-supervised
                   pre-training. Due to the cost of applying such models to
                   down-stream tasks, several model compression techniques on
                   pre-trained language representations have been proposed (Sun
                   et al., 2019; Sanh, 2019). However, surprisingly, the simple
                   baseline of just pre-training and fine-tuning compact models
                   has been overlooked. In this paper, we first show that
                   pre-training remains important in the context of smaller
                   architectures, and fine-tuning pre-trained compact models can
                   be competitive to more elaborate methods proposed in
                   concurrent work. Starting with pre-trained compact models, we
                   then explore transferring task knowledge from large
                   fine-tuned models through standard knowledge distillation.
                   The resulting simple, yet effective and general algorithm,
                   Pre-trained Distillation, brings further improvements.
                   Through extensive experiments, we more generally explore the
                   interaction between pre-training and distillation under two
                   variables that have been under-studied: model size and
                   properties of unlabeled task data. One surprising observation
                   is that they have a compound effect even when sequentially
                   applied on the same data. To accelerate future research, we
                   will make our 24 pre-trained miniature BERT models publicly
                   available.",
  month         =  aug,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1908.08962"
}

@ARTICLE{Roberts2020-nr,
  title         = "How Much Knowledge Can You Pack Into the Parameters of a
                   Language Model?",
  author        = "Roberts, Adam and Raffel, Colin and Shazeer, Noam",
  abstract      = "It has recently been observed that neural language models
                   trained on unstructured text can implicitly store and
                   retrieve knowledge using natural language queries. In this
                   short paper, we measure the practical utility of this
                   approach by fine-tuning pre-trained models to answer
                   questions without access to any external context or
                   knowledge. We show that this approach scales with model size
                   and performs competitively with open-domain systems that
                   explicitly retrieve answers from an external knowledge source
                   when answering questions. To facilitate reproducibility and
                   future work, we release our code and trained models at
                   https://goo.gle/t5-cbqa.",
  month         =  feb,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2002.08910"
}

@ARTICLE{Lin2020-bb,
  title         = "Pretrained Transformers for Text Ranking: {BERT} and Beyond",
  author        = "Lin, Jimmy and Nogueira, Rodrigo and Yates, Andrew",
  abstract      = "The goal of text ranking is to generate an ordered list of
                   texts retrieved from a corpus in response to a query.
                   Although the most common formulation of text ranking is
                   search, instances of the task can also be found in many
                   natural language processing applications. This survey
                   provides an overview of text ranking with neural network
                   architectures known as transformers, of which BERT is the
                   best-known example. The combination of transformers and
                   self-supervised pretraining has, without exaggeration,
                   revolutionized the fields of natural language processing
                   (NLP), information retrieval (IR), and beyond. In this
                   survey, we provide a synthesis of existing work as a single
                   point of entry for practitioners who wish to gain a better
                   understanding of how to apply transformers to text ranking
                   problems and researchers who wish to pursue work in this
                   area. We cover a wide range of modern techniques, grouped
                   into two high-level categories: transformer models that
                   perform reranking in multi-stage ranking architectures and
                   learned dense representations that attempt to perform ranking
                   directly. There are two themes that pervade our survey:
                   techniques for handling long documents, beyond the typical
                   sentence-by-sentence processing approaches used in NLP, and
                   techniques for addressing the tradeoff between effectiveness
                   (result quality) and efficiency (query latency). Although
                   transformer architectures and pretraining techniques are
                   recent innovations, many aspects of how they are applied to
                   text ranking are relatively well understood and represent
                   mature techniques. However, there remain many open research
                   questions, and thus in addition to laying out the foundations
                   of pretrained transformers for text ranking, this survey also
                   attempts to prognosticate where the field is heading.",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "2010.06467"
}

@ARTICLE{Zhu2020-jl,
  title         = "Don't Parse, Insert: Multilingual Semantic Parsing with
                   Insertion Based Decoding",
  author        = "Zhu, Qile and Khan, Haidar and Soltan, Saleh and Rawls,
                   Stephen and Hamza, Wael",
  abstract      = "Semantic parsing is one of the key components of natural
                   language understanding systems. A successful parse transforms
                   an input utterance to an action that is easily understood by
                   the system. Many algorithms have been proposed to solve this
                   problem, from conventional rulebased or statistical
                   slot-filling systems to shiftreduce based neural parsers. For
                   complex parsing tasks, the state-of-the-art method is based
                   on autoregressive sequence to sequence models to generate the
                   parse directly. This model is slow at inference time,
                   generating parses in O(n) decoding steps (n is the length of
                   the target sequence). In addition, we demonstrate that this
                   method performs poorly in zero-shot cross-lingual transfer
                   learning settings. In this paper, we propose a
                   non-autoregressive parser which is based on the insertion
                   transformer to overcome these two issues. Our approach 1)
                   speeds up decoding by 3x while outperforming the
                   autoregressive model and 2) significantly improves
                   cross-lingual transfer in the low-resource setting by 37\%
                   compared to autoregressive baseline. We test our approach on
                   three well-known monolingual datasets: ATIS, SNIPS and TOP.
                   For cross lingual semantic parsing, we use the MultiATIS++
                   and the multilingual TOP datasets.",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2010.03714"
}

@ARTICLE{Rongali2020-zv,
  title         = "Don't Parse, Generate! A Sequence to Sequence Architecture
                   for Task-Oriented Semantic Parsing",
  author        = "Rongali, Subendhu and Soldaini, Luca and Monti, Emilio and
                   Hamza, Wael",
  abstract      = "Virtual assistants such as Amazon Alexa, Apple Siri, and
                   Google Assistant often rely on a semantic parsing component
                   to understand which action(s) to execute for an utterance
                   spoken by its users. Traditionally, rule-based or statistical
                   slot-filling systems have been used to parse ``simple''
                   queries; that is, queries that contain a single action and
                   can be decomposed into a set of non-overlapping entities.
                   More recently, shift-reduce parsers have been proposed to
                   process more complex utterances. These methods, while
                   powerful, impose specific limitations on the type of queries
                   that can be parsed; namely, they require a query to be
                   representable as a parse tree. In this work, we propose a
                   unified architecture based on Sequence to Sequence models and
                   Pointer Generator Network to handle both simple and complex
                   queries. Unlike other works, our approach does not impose any
                   restriction on the semantic parse schema. Furthermore,
                   experiments show that it achieves state of the art
                   performance on three publicly available datasets (ATIS,
                   SNIPS, Facebook TOP), relatively improving between 3.3\% and
                   7.7\% in exact match accuracy over previous systems. Finally,
                   we show the effectiveness of our approach on two internal
                   datasets.",
  month         =  jan,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2001.11458"
}

@ARTICLE{Du2020-dh,
  title         = "Self-training Improves Pre-training for Natural Language
                   Understanding",
  author        = "Du, Jingfei and Grave, Edouard and Gunel, Beliz and
                   Chaudhary, Vishrav and Celebi, Onur and Auli, Michael and
                   Stoyanov, Ves and Conneau, Alexis",
  abstract      = "Unsupervised pre-training has led to much recent progress in
                   natural language understanding. In this paper, we study
                   self-training as another way to leverage unlabeled data
                   through semi-supervised learning. To obtain additional data
                   for a specific task, we introduce SentAugment, a data
                   augmentation method which computes task-specific query
                   embeddings from labeled data to retrieve sentences from a
                   bank of billions of unlabeled sentences crawled from the web.
                   Unlike previous semi-supervised methods, our approach does
                   not require in-domain unlabeled data and is therefore more
                   generally applicable. Experiments show that self-training is
                   complementary to strong RoBERTa baselines on a variety of
                   tasks. Our augmentation approach leads to scalable and
                   effective self-training with improvements of up to 2.6\% on
                   standard text classification benchmarks. Finally, we also
                   show strong gains on knowledge-distillation and few-shot
                   learning.",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2010.02194"
}

@ARTICLE{Card2020-wg,
  title         = "With Little Power Comes Great Responsibility",
  author        = "Card, Dallas and Henderson, Peter and Khandelwal, Urvashi and
                   Jia, Robin and Mahowald, Kyle and Jurafsky, Dan",
  abstract      = "Despite its importance to experimental design, statistical
                   power (the probability that, given a real effect, an
                   experiment will reject the null hypothesis) has largely been
                   ignored by the NLP community. Underpowered experiments make
                   it more difficult to discern the difference between
                   statistical noise and meaningful model improvements, and
                   increase the chances of exaggerated findings. By
                   meta-analyzing a set of existing NLP papers and datasets, we
                   characterize typical power for a variety of settings and
                   conclude that underpowered experiments are common in the NLP
                   literature. In particular, for several tasks in the popular
                   GLUE benchmark, small test sets mean that most attempted
                   comparisons to state of the art models will not be adequately
                   powered. Similarly, based on reasonable assumptions, we find
                   that the most typical experimental design for human rating
                   studies will be underpowered to detect small model
                   differences, of the sort that are frequently studied. For
                   machine translation, we find that typical test sets of 2000
                   sentences have approximately 75\% power to detect differences
                   of 1 BLEU point. To improve the situation going forward, we
                   give an overview of best practices for power analysis in NLP
                   and release a series of notebooks to assist with future power
                   analyses.",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2010.06595"
}

@ARTICLE{Lewis2020-fy,
  title         = "Retrieval-Augmented Generation for Knowledge-Intensive {NLP}
                   Tasks",
  author        = "Lewis, Patrick and Perez, Ethan and Piktus, Aleksandara and
                   Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and
                   Küttler, Heinrich and Lewis, Mike and Yih, Wen-Tau and
                   Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe",
  abstract      = "Large pre-trained language models have been shown to store
                   factual knowledge in their parameters, and achieve
                   state-of-the-art results when fine-tuned on downstream NLP
                   tasks. However, their ability to access and precisely
                   manipulate knowledge is still limited, and hence on
                   knowledge-intensive tasks, their performance lags behind
                   task-specific architectures. Additionally, providing
                   provenance for their decisions and updating their world
                   knowledge remain open research problems. Pre-trained models
                   with a differentiable access mechanism to explicit
                   non-parametric memory can overcome this issue, but have so
                   far been only investigated for extractive downstream tasks.
                   We explore a general-purpose fine-tuning recipe for
                   retrieval-augmented generation (RAG) -- models which combine
                   pre-trained parametric and non-parametric memory for language
                   generation. We introduce RAG models where the parametric
                   memory is a pre-trained seq2seq model and the non-parametric
                   memory is a dense vector index of Wikipedia, accessed with a
                   pre-trained neural retriever. We compare two RAG
                   formulations, one which conditions on the same retrieved
                   passages across the whole generated sequence, the other can
                   use different passages per token. We fine-tune and evaluate
                   our models on a wide range of knowledge-intensive NLP tasks
                   and set the state-of-the-art on three open domain QA tasks,
                   outperforming parametric seq2seq models and task-specific
                   retrieve-and-extract architectures. For language generation
                   tasks, we find that RAG models generate more specific,
                   diverse and factual language than a state-of-the-art
                   parametric-only seq2seq baseline.",
  month         =  may,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2005.11401"
}

@ARTICLE{Nanda2023-fx,
  title         = "Progress measures for grokking via mechanistic
                   interpretability",
  author        = "Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith,
                   Jess and Steinhardt, Jacob",
  abstract      = "Neural networks often exhibit emergent behavior, where
                   qualitatively new capabilities arise from scaling up the
                   amount of parameters, training data, or training steps. One
                   approach to understanding emergence is to find continuous
                   \textit{progress measures} that underlie the seemingly
                   discontinuous qualitative changes. We argue that progress
                   measures can be found via mechanistic interpretability:
                   reverse-engineering learned behaviors into their individual
                   components. As a case study, we investigate the
                   recently-discovered phenomenon of ``grokking'' exhibited by
                   small transformers trained on modular addition tasks. We
                   fully reverse engineer the algorithm learned by these
                   networks, which uses discrete Fourier transforms and
                   trigonometric identities to convert addition to rotation
                   about a circle. We confirm the algorithm by analyzing the
                   activations and weights and by performing ablations in
                   Fourier space. Based on this understanding, we define
                   progress measures that allow us to study the dynamics of
                   training and split training into three continuous phases:
                   memorization, circuit formation, and cleanup. Our results
                   show that grokking, rather than being a sudden shift, arises
                   from the gradual amplification of structured mechanisms
                   encoded in the weights, followed by the later removal of
                   memorizing components.",
  month         =  jan,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2301.05217"
}

@ARTICLE{Severyn2015-ai,
  title     = "Learning to rank short text pairs with convolutional deep neural
               networks",
  author    = "Severyn, A and Moschitti, A",
  journal   = "Proceedings of the 38th international ACM SIGIR",
  publisher = "dl.acm.org",
  abstract  = "Learning a similarity function between pairs of objects is at the
               core of learning to rank approaches. In information retrieval
               tasks we typically deal with query-document pairs, in question
               answering--question-answer pairs. However, before learning can
               take place, such …",
  year      =  2015
}

@ARTICLE{Webber2010-ct,
  title     = "A similarity measure for indefinite rankings",
  author    = "Webber, William and Moffat, Alistair and Zobel, Justin",
  journal   = "ACM Transactions on Information and System Security",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  28,
  number    =  4,
  pages     = "1--38",
  abstract  = "Ranked lists are encountered in research and daily life and it is
               often of interest to compare these lists even when they are
               incomplete or have only some members in common. An example is
               document rankings returned for the same query by different search
               engines. A measure of the similarity between incomplete rankings
               should handle nonconjointness, weight high ranks more heavily
               than low, and be monotonic with increasing depth of evaluation;
               but no measure satisfying all these criteria currently exists. In
               this article, we propose a new measure having these qualities,
               namely rank-biased overlap (RBO). The RBO measure is based on a
               simple probabilistic user model. It provides monotonicity by
               calculating, at a given depth of evaluation, a base score that is
               non-decreasing with additional evaluation, and a maximum score
               that is nonincreasing. An extrapolated score can be calculated
               between these bounds if a point estimate is required. RBO has a
               parameter which determines the strength of the weighting to top
               ranks. We extend RBO to handle tied ranks and rankings of
               different lengths. Finally, we give examples of the use of the
               measure in comparing the results produced by public search
               engines and in assessing retrieval systems in the laboratory.",
  month     =  nov,
  year      =  2010,
  keywords  = "probabilistic models, Rank correlation, ranking",
  doi       = "10.1145/1852102.1852106",
  issn      = "1094-9224,1046-8188"
}

@ARTICLE{Reimers2020-fj,
  title         = "The Curse of Dense Low-Dimensional Information Retrieval for
                   Large Index Sizes",
  author        = "Reimers, Nils and Gurevych, Iryna",
  abstract      = "Information Retrieval using dense low-dimensional
                   representations recently became popular and showed
                   out-performance to traditional sparse-representations like
                   BM25. However, no previous work investigated how dense
                   representations perform with large index sizes. We show
                   theoretically and empirically that the performance for dense
                   representations decreases quicker than sparse representations
                   for increasing index sizes. In extreme cases, this can even
                   lead to a tipping point where at a certain index size sparse
                   representations outperform dense representations. We show
                   that this behavior is tightly connected to the number of
                   dimensions of the representations: The lower the dimension,
                   the higher the chance for false positives, i.e. returning
                   irrelevant documents.",
  month         =  dec,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "2012.14210"
}

@ARTICLE{Henderson2020-lz,
  title         = "Towards the Systematic Reporting of the Energy and Carbon
                   Footprints of Machine Learning",
  author        = "Henderson, Peter and Hu, Jieru and Romoff, Joshua and
                   Brunskill, Emma and Jurafsky, Dan and Pineau, Joelle",
  abstract      = "Accurate reporting of energy and carbon usage is essential
                   for understanding the potential climate impacts of machine
                   learning research. We introduce a framework that makes this
                   easier by providing a simple interface for tracking realtime
                   energy consumption and carbon emissions, as well as
                   generating standardized online appendices. Utilizing this
                   framework, we create a leaderboard for energy efficient
                   reinforcement learning algorithms to incentivize responsible
                   research in this area as an example for other areas of
                   machine learning. Finally, based on case studies using our
                   framework, we propose strategies for mitigation of carbon
                   emissions and reduction of energy consumption. By making
                   accounting easier, we hope to further the sustainable
                   development of machine learning experiments and spur more
                   research into energy efficient algorithms.",
  month         =  jan,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY",
  eprint        = "2002.05651"
}

@ARTICLE{Kim2021-uk,
  title         = "{I}-{BERT}: Integer-only {BERT} Quantization",
  author        = "Kim, Sehoon and Gholami, Amir and Yao, Zhewei and Mahoney,
                   Michael W and Keutzer, Kurt",
  abstract      = "Transformer based models, like BERT and RoBERTa, have
                   achieved state-of-the-art results in many Natural Language
                   Processing tasks. However, their memory footprint, inference
                   latency, and power consumption are prohibitive for many edge
                   processors, and it has been a challenge to deploy these
                   models for edge applications and devices that have resource
                   constraints. While quantization can be a viable solution to
                   this, previous work on quantizing Transformer based models
                   uses floating-point arithmetic during inference, thus
                   limiting model deployment on many edge processors. In this
                   work, we propose a novel integer-only quantization scheme for
                   Transformer based models that quantizes the entire inference
                   process. In particular, we demonstrate how to approximate
                   nonlinear operations in Transformer architectures, e.g.,
                   GELU, Softmax, and Layer Normalization, with lightweight
                   integer computations. We use those approximations in our
                   method, I-BERT, with an end-to-end integer-only inference,
                   and without any floating point calculation. We test our
                   approach on GLUE downstream tasks using RoBERTa-Base and
                   RoBERTa-Large. For both cases, with an 8-bit integer-only
                   quantization scheme, I-BERT achieves similar accuracy as
                   compared to the full-precision baseline.",
  month         =  jan,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2101.01321"
}

@ARTICLE{Su2021-sh,
  title         = "{CSS}-{LM}: A Contrastive Framework for Semi-supervised
                   Fine-tuning of Pre-trained Language Models",
  author        = "Su, Yusheng and Han, Xu and Lin, Yankai and Zhang, Zhengyan
                   and Liu, Zhiyuan and Li, Peng and Zhou, Jie and Sun, Maosong",
  abstract      = "Fine-tuning pre-trained language models (PLMs) has
                   demonstrated its effectiveness on various downstream NLP
                   tasks recently. However, in many low-resource scenarios, the
                   conventional fine-tuning strategies cannot sufficiently
                   capture the important semantic features for downstream tasks.
                   To address this issue, we introduce a novel framework (named
                   ``CSS-LM'') to improve the fine-tuning phase of PLMs via
                   contrastive semi-supervised learning. Specifically, given a
                   specific task, we retrieve positive and negative instances
                   from large-scale unlabeled corpora according to their
                   domain-level and class-level semantic relatedness to the
                   task. We then perform contrastive semi-supervised learning on
                   both the retrieved unlabeled and original labeled instances
                   to help PLMs capture crucial task-related semantic features.
                   The experimental results show that CSS-LM achieves better
                   results than the conventional fine-tuning strategy on a
                   series of downstream tasks with few-shot settings, and
                   outperforms the latest supervised contrastive fine-tuning
                   strategies. Our datasets and source code will be available to
                   provide more details.",
  month         =  feb,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2102.03752"
}

@ARTICLE{Sinha2021-dz,
  title         = "Masked Language Modeling and the Distributional Hypothesis:
                   Order Word Matters Pre-training for Little",
  author        = "Sinha, Koustuv and Jia, Robin and Hupkes, Dieuwke and Pineau,
                   Joelle and Williams, Adina and Kiela, Douwe",
  abstract      = "A possible explanation for the impressive performance of
                   masked language model (MLM) pre-training is that such models
                   have learned to represent the syntactic structures prevalent
                   in classical NLP pipelines. In this paper, we propose a
                   different explanation: MLMs succeed on downstream tasks
                   almost entirely due to their ability to model higher-order
                   word co-occurrence statistics. To demonstrate this, we
                   pre-train MLMs on sentences with randomly shuffled word
                   order, and show that these models still achieve high accuracy
                   after fine-tuning on many downstream tasks -- including on
                   tasks specifically designed to be challenging for models that
                   ignore word order. Our models perform surprisingly well
                   according to some parametric syntactic probes, indicating
                   possible deficiencies in how we test representations for
                   syntactic information. Overall, our results show that purely
                   distributional information largely explains the success of
                   pre-training, and underscore the importance of curating
                   challenging evaluation datasets that require deeper
                   linguistic knowledge.",
  month         =  apr,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2104.06644"
}

@INPROCEEDINGS{Sakai2007-aj,
  title     = "Alternatives to Bpref",
  author    = "Sakai, Tetsuya",
  booktitle = "Proceedings of the 30th annual international ACM SIGIR conference
               on Research and development in information retrieval",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "71--78",
  abstract  = "Recently, a number of TREC tracks have adopted a retrieval
               effectiveness metric called bpref which has been designed for
               evaluation environments with incomplete relevance data. A
               graded-relevance version of this metric called rpref has also
               been proposed. However, we show that the application of
               Q-measure, normalised Discounted Cumulative Gain (nDCG) or
               Average Precision (AveP)to condensed lists, obtained by ?ltering
               out all unjudged documents from the original ranked lists, is
               actually a better solution to the incompleteness problem than
               bpref. Furthermore, we show that the use of graded relevance
               boosts the robustness of IR evaluation to incompleteness and
               therefore that Q-measure and nDCG based on condensed lists are
               the best choices. To this end, we use four graded-relevance test
               collections from NTCIR to compare ten different IR metrics in
               terms of system ranking stability and pairwise discriminative
               power.",
  series    = "SIGIR '07",
  month     =  jul,
  year      =  2007,
  keywords  = "graded relevance, evaluation metrics, test collection",
  doi       = "10.1145/1277741.1277756",
  isbn      =  9781595935977
}

@ARTICLE{Rogers2021-ot,
  title         = "Changing the World by Changing the Data",
  author        = "Rogers, Anna",
  abstract      = "NLP community is currently investing a lot more research and
                   resources into development of deep learning models than
                   training data. While we have made a lot of progress, it is
                   now clear that our models learn all kinds of spurious
                   patterns, social biases, and annotation artifacts.
                   Algorithmic solutions have so far had limited success. An
                   alternative that is being actively discussed is more careful
                   design of datasets so as to deliver specific signals. This
                   position paper maps out the arguments for and against data
                   curation, and argues that fundamentally the point is moot:
                   curation already is and will be happening, and it is changing
                   the world. The question is only how much thought we want to
                   invest into that process.",
  month         =  may,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2105.13947"
}

@ARTICLE{Fiesler2020-ga,
  title    = "No Robots, Spiders, or Scrapers: Legal and Ethical Regulation of
              Data Collection Methods in Social Media Terms of Service",
  author   = "Fiesler, Casey and Beard, Nathan and Keegan, Brian C",
  journal  = "Proceedings of the International AAAI Conference on Web and Social
              Media",
  volume   =  14,
  pages    = "187--196",
  month    =  may,
  year     =  2020,
  issn     = "2334-0770,2334-0770",
  language = "en"
}

@ARTICLE{Ackerman2019-md,
  title     = "Syntactic and cognitive issues in investigating gendered
               coreference",
  author    = "Ackerman, Lauren",
  journal   = "Glossa a journal of general linguistics",
  publisher = "Ubiquity Press, Ltd.",
  volume    =  4,
  number    =  1,
  month     =  oct,
  year      =  2019,
  doi       = "10.5334/gjgl.721",
  issn      = "2397-1835",
  language  = "en"
}

@ARTICLE{Lee2021-gy,
  title         = "Deduplicating Training Data Makes Language Models Better",
  author        = "Lee, Katherine and Ippolito, Daphne and Nystrom, Andrew and
                   Zhang, Chiyuan and Eck, Douglas and Callison-Burch, Chris and
                   Carlini, Nicholas",
  abstract      = "We find that existing language modeling datasets contain many
                   near-duplicate examples and long repetitive substrings. As a
                   result, over 1\% of the unprompted output of language models
                   trained on these datasets is copied verbatim from the
                   training data. We develop two tools that allow us to
                   deduplicate training datasets -- for example removing from C4
                   a single 61 word English sentence that is repeated over
                   60,000 times. Deduplication allows us to train models that
                   emit memorized text ten times less frequently and require
                   fewer train steps to achieve the same or better accuracy. We
                   can also reduce train-test overlap, which affects over 4\% of
                   the validation set of standard datasets, thus allowing for
                   more accurate evaluation. We release code for reproducing our
                   work and performing dataset deduplication at
                   https://github.com/google-research/deduplicate-text-datasets.",
  month         =  jul,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2107.06499"
}

@ARTICLE{Cao2019-jc,
  title         = "Toward Gender-Inclusive Coreference Resolution",
  author        = "Cao, Yang Trista and Daumé, III, Hal",
  abstract      = "Correctly resolving textual mentions of people fundamentally
                   entails making inferences about those people. Such inferences
                   raise the risk of systemic biases in coreference resolution
                   systems, including biases that can harm binary and non-binary
                   trans and cis stakeholders. To better understand such biases,
                   we foreground nuanced conceptualizations of gender from
                   sociology and sociolinguistics, and develop two new datasets
                   for interrogating bias in crowd annotations and in existing
                   coreference resolution systems. Through these studies,
                   conducted on English text, we confirm that without
                   acknowledging and building systems that recognize the
                   complexity of gender, we build systems that lead to many
                   potential harms.",
  month         =  oct,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1910.13913"
}

@ARTICLE{Blodgett2020-cg,
  title         = "Language (Technology) is Power: A Critical Survey of ``Bias''
                   in {NLP}",
  author        = "Blodgett, Su Lin and Barocas, Solon and Daumé, III, Hal and
                   Wallach, Hanna",
  abstract      = "We survey 146 papers analyzing ``bias'' in NLP systems,
                   finding that their motivations are often vague, inconsistent,
                   and lacking in normative reasoning, despite the fact that
                   analyzing ``bias'' is an inherently normative process. We
                   further find that these papers' proposed quantitative
                   techniques for measuring or mitigating ``bias'' are poorly
                   matched to their motivations and do not engage with the
                   relevant literature outside of NLP. Based on these findings,
                   we describe the beginnings of a path forward by proposing
                   three recommendations that should guide work analyzing
                   ``bias'' in NLP systems. These recommendations rest on a
                   greater recognition of the relationships between language and
                   social hierarchies, encouraging researchers and practitioners
                   to articulate their conceptualizations of ``bias''---i.e.,
                   what kinds of system behaviors are harmful, in what ways, to
                   whom, and why, as well as the normative reasoning underlying
                   these statements---and to center work around the lived
                   experiences of members of communities affected by NLP
                   systems, while interrogating and reimagining the power
                   relations between technologists and such communities.",
  month         =  may,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2005.14050"
}

@ARTICLE{Dehghani2021-rc,
  title         = "The Benchmark Lottery",
  author        = "Dehghani, Mostafa and Tay, Yi and Gritsenko, Alexey A and
                   Zhao, Zhe and Houlsby, Neil and Diaz, Fernando and Metzler,
                   Donald and Vinyals, Oriol",
  abstract      = "The world of empirical machine learning (ML) strongly relies
                   on benchmarks in order to determine the relative
                   effectiveness of different algorithms and methods. This paper
                   proposes the notion of ``a benchmark lottery'' that describes
                   the overall fragility of the ML benchmarking process. The
                   benchmark lottery postulates that many factors, other than
                   fundamental algorithmic superiority, may lead to a method
                   being perceived as superior. On multiple benchmark setups
                   that are prevalent in the ML community, we show that the
                   relative performance of algorithms may be altered
                   significantly simply by choosing different benchmark tasks,
                   highlighting the fragility of the current paradigms and
                   potential fallacious interpretation derived from benchmarking
                   ML methods. Given that every benchmark makes a statement
                   about what it perceives to be important, we argue that this
                   might lead to biased progress in the community. We discuss
                   the implications of the observed phenomena and provide
                   recommendations on mitigating them using multiple machine
                   learning domains and communities as use cases, including
                   natural language processing, computer vision, information
                   retrieval, recommender systems, and reinforcement learning.",
  month         =  jul,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2107.07002"
}

@INPROCEEDINGS{Lo2020-db,
  title     = "{S2ORC}: The semantic scholar open research corpus",
  author    = "Lo, Kyle and Wang, Lucy Lu and Neumann, Mark and Kinney, Rodney
               and Weld, Daniel",
  booktitle = "Proceedings of the 58th Annual Meeting of the Association for
               Computational Linguistics",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  year      =  2020,
  doi       = "10.18653/v1/2020.acl-main.447"
}

@ARTICLE{Bragg2021-vt,
  title     = "Flex: Unifying evaluation for few-shot nlp",
  author    = "Bragg, J and Cohan, A and Lo, K and Beltagy, I",
  journal   = "Thirty-Fifth Conference on Neural",
  publisher = "papers.nips.cc",
  abstract  = "Few-shot NLP research is highly active, yet conducted in disjoint
               research threads with evaluation suites that lack
               challenging-yet-realistic testing setups and fail to employ
               careful experimental design. Consequently, the community does not
               know which techniques perform best or even if they outperform
               simple baselines. In response, we formulate the FLEX Principles,
               a set of requirements and best practices for unified, rigorous,
               valid, and cost-sensitive few-shot NLP evaluation. These
               principles include Sample Size Design, a …",
  year      =  2021
}

@ARTICLE{Piktus2021-ew,
  title         = "The Web Is Your Oyster -- Knowledge-Intensive {NLP} against a
                   Very Large Web Corpus",
  author        = "Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir
                   and Okhonko, Dmytro and Broscheit, Samuel and Izacard,
                   Gautier and Lewis, Patrick and Oğuz, Barlas and Grave,
                   Edouard and Yih, Wen-Tau and Riedel, Sebastian",
  abstract      = "In order to address the increasing demands of real-world
                   applications, the research for knowledge-intensive NLP
                   (KI-NLP) should advance by capturing the challenges of a
                   truly open-domain environment: web scale knowledge, lack of
                   structure, inconsistent quality, and noise. To this end, we
                   propose a new setup for evaluating existing KI-NLP tasks in
                   which we generalize the background corpus to a universal web
                   snapshot. We repurpose KILT, a standard KI-NLP benchmark
                   initially developed for Wikipedia, and ask systems to use a
                   subset of CCNet - the Sphere corpus - as a knowledge source.
                   In contrast to Wikipedia, Sphere is orders of magnitude
                   larger and better reflects the full diversity of knowledge on
                   the Internet. We find that despite potential gaps of
                   coverage, challenges of scale, lack of structure and lower
                   quality, retrieval from Sphere enables a state-of-the-art
                   retrieve-and-read system to match and even outperform
                   Wikipedia-based models on several KILT tasks - even if we
                   aggressively filter content that looks like Wikipedia. We
                   also observe that while a single dense passage index over
                   Wikipedia can outperform a sparse BM25 version, on Sphere
                   this is not yet possible. To facilitate further research into
                   this area, and minimise the community's reliance on
                   proprietary black box search engines, we will share our
                   indices, evaluation metrics and infrastructure.",
  month         =  dec,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2112.09924"
}

@ARTICLE{Mielke2021-et,
  title         = "Between words and characters: A Brief History of
                   Open-Vocabulary Modeling and Tokenization in {NLP}",
  author        = "Mielke, Sabrina J and Alyafeai, Zaid and Salesky, Elizabeth
                   and Raffel, Colin and Dey, Manan and Gallé, Matthias and
                   Raja, Arun and Si, Chenglei and Lee, Wilson Y and Sagot,
                   Benoît and Tan, Samson",
  abstract      = "What are the units of text that we want to model? From bytes
                   to multi-word expressions, text can be analyzed and generated
                   at many granularities. Until recently, most natural language
                   processing (NLP) models operated over words, treating those
                   as discrete and atomic tokens, but starting with byte-pair
                   encoding (BPE), subword-based approaches have become dominant
                   in many areas, enabling small vocabularies while still
                   allowing for fast inference. Is the end of the road
                   character-level model or byte-level processing? In this
                   survey, we connect several lines of work from the pre-neural
                   and neural era, by showing how hybrid approaches of words and
                   characters as well as subword-based approaches based on
                   learned segmentation have been proposed and evaluated. We
                   conclude that there is and likely will never be a silver
                   bullet singular solution for all applications and that
                   thinking seriously about tokenization remains important for
                   many applications.",
  month         =  dec,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2112.10508"
}

@ARTICLE{Lin2021-ht,
  title         = "Few-shot Learning with Multilingual Language Models",
  author        = "Lin, Xi Victoria and Mihaylov, Todor and Artetxe, Mikel and
                   Wang, Tianlu and Chen, Shuohui and Simig, Daniel and Ott,
                   Myle and Goyal, Naman and Bhosale, Shruti and Du, Jingfei and
                   Pasunuru, Ramakanth and Shleifer, Sam and Koura, Punit Singh
                   and Chaudhary, Vishrav and O'Horo, Brian and Wang, Jeff and
                   Zettlemoyer, Luke and Kozareva, Zornitsa and Diab, Mona and
                   Stoyanov, Veselin and Li, Xian",
  abstract      = "Large-scale autoregressive language models such as GPT-3 are
                   few-shot learners that can perform a wide range of language
                   tasks without fine-tuning. While these models are known to be
                   able to jointly represent many different languages, their
                   training data is dominated by English, potentially limiting
                   their cross-lingual generalization. In this work, we train
                   multilingual autoregressive language models on a balanced
                   corpus covering a diverse set of languages, and study their
                   few- and zero-shot learning capabilities in a wide range of
                   tasks. Our largest model with 7.5 billion parameters sets new
                   state of the art in few-shot learning in more than 20
                   representative languages, outperforming GPT-3 of comparable
                   size in multilingual commonsense reasoning (with +7.4\%
                   absolute accuracy improvement in 0-shot settings and +9.4\%
                   in 4-shot settings) and natural language inference (+5.4\% in
                   each of 0-shot and 4-shot settings). On the FLORES-101
                   machine translation benchmark, our model outperforms GPT-3 on
                   171 out of 182 translation directions with 32 training
                   examples, while surpassing the official supervised baseline
                   in 45 directions. We present a detailed analysis of where the
                   model succeeds and fails, showing in particular that it
                   enables cross-lingual in-context learning on some tasks,
                   while there is still room for improvement on surface form
                   robustness and adaptation to tasks that do not have a natural
                   cloze form. Finally, we evaluate our models in social value
                   tasks such as hate speech detection in five languages and
                   find it has limitations similar to comparable sized GPT-3
                   models.",
  month         =  dec,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2112.10668"
}

@ARTICLE{Choi2021-ww,
  title     = "Decontextualization: Making sentences stand-alone",
  author    = "Choi, Eunsol and Palomaki, Jennimaria and Lamm, Matthew and
               Kwiatkowski, Tom and Das, Dipanjan and Collins, Michael",
  journal   = "Transactions of the Association for Computational Linguistics",
  publisher = "MIT Press - Journals",
  volume    =  9,
  pages     = "447--461",
  abstract  = "Abstract Models for question answering, dialogue agents, and
               summarization often interpret the meaning of a sentence in a rich
               context and use that meaning in a new context. Taking excerpts of
               text can be problematic, as key pieces may not be explicit in a
               local window. We isolate and define the problem of sentence
               decontextualization: taking a sentence together with its context
               and rewriting it to be interpretable out of context, while
               preserving its meaning. We describe an annotation procedure,
               collect data on the Wikipedia corpus, and use the data to train
               models to automatically decontextualize sentences. We present
               preliminary studies that show the value of sentence
               decontextualization in a user-facing task, and as preprocessing
               for systems that perform document understanding. We argue that
               decontextualization is an important subtask in many downstream
               applications, and that the definitions and resources provided can
               benefit tasks that operate on sentences that occur in a richer
               context.",
  month     =  apr,
  year      =  2021,
  doi       = "10.1162/tacl\_a\_00377",
  issn      = "2307-387X",
  language  = "en"
}

@ARTICLE{Dasigi2017-al,
  title         = "Experiment Segmentation in Scientific Discourse as
                   Clause-level Structured Prediction using Recurrent Neural
                   Networks",
  author        = "Dasigi, Pradeep and Burns, Gully A P and Hovy, Eduard and de
                   Waard, Anita",
  abstract      = "We propose a deep learning model for identifying structure
                   within experiment narratives in scientific literature. We
                   take a sequence labeling approach to this problem, and label
                   clauses within experiment narratives to identify the
                   different parts of the experiment. Our dataset consists of
                   paragraphs taken from open access PubMed papers labeled with
                   rhetorical information as a result of our pilot annotation.
                   Our model is a Recurrent Neural Network (RNN) with Long
                   Short-Term Memory (LSTM) cells that labels clauses. The
                   clause representations are computed by combining word
                   representations using a novel attention mechanism that
                   involves a separate RNN. We compare this model against LSTMs
                   where the input layer has simple or no attention and a
                   feature rich CRF model. Furthermore, we describe how our work
                   could be useful for information extraction from scientific
                   literature.",
  month         =  feb,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1702.05398"
}

@ARTICLE{Brack2021-ca,
  title    = "Sequential Sentence Classification in Research Papers using
              Cross-Domain Multi-Task Learning",
  author   = "Brack, Arthur and Hoppe, Anett and Buschermöhle, Pascal and
              Ewerth, R",
  journal  = "undefined",
  abstract = "It is demonstrated that models, which are trained on datasets from
              different scientific domains, benefit from one another when using
              the proposed multi-task learning architecture, and the approach
              outperforms the state of the art on three benchmark datasets. The
              task of sequential sentence classification enables the semantic
              structuring of research papers. This can enhance academic search
              engines to support researchers in finding and exploring research
              literature more effectively. However, previous work has not
              investigated the potential of transfer learning with datasets from
              different scientific domains for this task yet. We propose a
              uniform deep learning architecture and multi-task learning to
              improve sequential sentence classification in scientific texts
              across domains by exploiting training data from multiple domains.
              Our contributions can be summarised as follows: (1) We tailor two
              common transfer learning methods, sequential transfer learning and
              multi-task learning, and evaluate their performance for sequential
              sentence classification; (2) The presented multi-task model is
              able to recognise semantically related classes from different
              datasets and thus supports manual comparison and assessment of
              different annotation schemes; (3) The unified approach is capable
              of handling datasets that contain either only abstracts or full
              papers without further feature engineering. We demonstrate that
              models, which are trained on datasets from different scientific
              domains, benefit from one another when using the proposed
              multi-task learning architecture. Our approach outperforms the
              state of the art on three benchmark datasets. Arthur Brack E-mail:
              arthur.brack@tib.eu Anett Hoppe E-mail: anett.hoppe@tib.eu Pascal
              Buschermöhle E-mail: pascal.buschermoehle@gmail.com Ralph Ewerth
              E-mail: ralph.ewerth@tib.eu 1TIB – Leibniz Information Centre for
              Science and Technology, Hannover, Germany 2L3S Research Center,
              Leibniz University, Hannover, Germany ar X iv :2 10 2. 06 00 8v 1
              [ cs .C L ] 1 1 Fe b 20 21",
  year     =  2021,
  eprint   = "2102.06008",
  language = "en"
}

@ARTICLE{Dernoncourt2017-sw,
  title         = "{PubMed} {200k} {RCT}: a Dataset for Sequential Sentence
                   Classification in Medical Abstracts",
  author        = "Dernoncourt, Franck and Lee, Ji Young",
  abstract      = "We present PubMed 200k RCT, a new dataset based on PubMed for
                   sequential sentence classification. The dataset consists of
                   approximately 200,000 abstracts of randomized controlled
                   trials, totaling 2.3 million sentences. Each sentence of each
                   abstract is labeled with their role in the abstract using one
                   of the following classes: background, objective, method,
                   result, or conclusion. The purpose of releasing this dataset
                   is twofold. First, the majority of datasets for sequential
                   short-text classification (i.e., classification of short
                   texts that appear in sequences) are small: we hope that
                   releasing a new large dataset will help develop more accurate
                   algorithms for this task. Second, from an application
                   perspective, researchers need better tools to efficiently
                   skim through the literature. Automatically classifying each
                   sentence in an abstract would help researchers read abstracts
                   more efficiently, especially in fields where abstracts may be
                   long, such as the medical field.",
  month         =  oct,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "1710.06071"
}

@ARTICLE{Sciavolino2021-xf,
  title         = "Simple Entity-Centric Questions Challenge Dense Retrievers",
  author        = "Sciavolino, Christopher and Zhong, Zexuan and Lee, Jinhyuk
                   and Chen, Danqi",
  abstract      = "Open-domain question answering has exploded in popularity
                   recently due to the success of dense retrieval models, which
                   have surpassed sparse models using only a few supervised
                   training examples. However, in this paper, we demonstrate
                   current dense models are not yet the holy grail of retrieval.
                   We first construct EntityQuestions, a set of simple,
                   entity-rich questions based on facts from Wikidata (e.g.,
                   ``Where was Arve Furset born?''), and observe that dense
                   retrievers drastically underperform sparse methods. We
                   investigate this issue and uncover that dense retrievers can
                   only generalize to common entities unless the question
                   pattern is explicitly observed during training. We discuss
                   two simple solutions towards addressing this critical
                   problem. First, we demonstrate that data augmentation is
                   unable to fix the generalization problem. Second, we argue a
                   more robust passage encoder helps facilitate better question
                   adaptation using specialized question encoders. We hope our
                   work can shed light on the challenges in creating a robust,
                   universal dense retriever that works well across different
                   input distributions.",
  month         =  sep,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2109.08535"
}

@INPROCEEDINGS{Kanakarajan2021-hf,
  title     = "{BioELECTRA}:Pretrained Biomedical text Encoder using
               Discriminators",
  author    = "Kanakarajan, Kamal Raj and Kundumani, Bhuvana and Sankarasubbu,
               Malaikannan",
  booktitle = "Proceedings of the 20th Workshop on Biomedical Language
               Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "143--154",
  abstract  = "Kamal raj Kanakarajan, Bhuvana Kundumani, Malaikannan
               Sankarasubbu. Proceedings of the 20th Workshop on Biomedical
               Language Processing. 2021.",
  year      =  2021,
  doi       = "10.18653/v1/2021.bionlp-1.16"
}

@INPROCEEDINGS{Mahajan2021-ia,
  title     = "{IBMResearch} at {MEDIQA} {2021}: Toward improving factual
               correctness of radiology report abstractive summarization",
  author    = "Mahajan, Diwakar and Tsou, Ching-Huei and Liang, Jennifer J",
  booktitle = "Proceedings of the 20th Workshop on Biomedical Language
               Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "302--310",
  abstract  = "Diwakar Mahajan, Ching-Huei Tsou, Jennifer J Liang. Proceedings
               of the 20th Workshop on Biomedical Language Processing. 2021.",
  year      =  2021,
  doi       = "10.18653/v1/2021.bionlp-1.35"
}

@ARTICLE{Li2022-un,
  title         = "{CORWA}: A Citation-Oriented Related Work Annotation dataset",
  author        = "Li, Xiangci and Mandal, Biswadip and Ouyang, Jessica",
  abstract      = "Academic research is an exploratory activity to discover new
                   solutions to problems. By this nature, academic research
                   works perform literature reviews to distinguish their
                   novelties from prior work. In natural language processing,
                   this literature review is usually conducted under the
                   ``Related Work'' section. The task of related work generation
                   aims to automatically generate the related work section given
                   the rest of the research paper and a list of papers to cite.
                   Prior work on this task has focused on the sentence as the
                   basic unit of generation, neglecting the fact that related
                   work sections consist of variable length text fragments
                   derived from different information sources. As a first step
                   toward a linguistically-motivated related work generation
                   framework, we present a Citation Oriented Related Work
                   Annotation (CORWA) dataset that labels different types of
                   citation text fragments from different information sources.
                   We train a strong baseline model that automatically tags the
                   CORWA labels on massive unlabeled related work section texts.
                   We further suggest a novel framework for human-in-the-loop,
                   iterative, abstractive related work generation.",
  month         =  may,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2205.03512"
}

@ARTICLE{Lewis2020-jy,
  title         = "Question and answer test-train overlap in open-Domain
                   Question Answering datasets",
  author        = "Lewis, Patrick and Stenetorp, Pontus and Riedel, Sebastian",
  abstract      = "Ideally Open-Domain Question Answering models should exhibit
                   a number of competencies, ranging from simply memorizing
                   questions seen at training time, to answering novel question
                   formulations with answers seen during training, to
                   generalizing to completely novel questions with novel
                   answers. However, single aggregated test set scores do not
                   show the full picture of what capabilities models truly have.
                   In this work, we perform a detailed study of the test sets of
                   three popular open-domain benchmark datasets with respect to
                   these competencies. We find that 60-70\% of test-time answers
                   are also present somewhere in the training sets. We also find
                   that 30\% of test-set questions have a near-duplicate
                   paraphrase in their corresponding training sets. Using these
                   findings, we evaluate a variety of popular open-domain models
                   to obtain greater insight into what extent they can actually
                   generalize, and what drives their overall performance. We
                   find that all models perform dramatically worse on questions
                   that cannot be memorized from training sets, with a mean
                   absolute performance difference of 63\% between repeated and
                   non-repeated data. Finally we show that simple
                   nearest-neighbor models out-perform a BART closed-book QA
                   model, further highlighting the role that training set
                   memorization plays in these benchmarks",
  month         =  aug,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2008.02637"
}

@ARTICLE{Wahle2022-mi,
  title         = "{D3}: A massive dataset of scholarly metadata for analyzing
                   the state of computer science research",
  author        = "Wahle, Jan Philip and Ruas, Terry and Mohammad, Saif M and
                   Gipp, Bela",
  abstract      = "DBLP is the largest open-access repository of scientific
                   articles on computer science and provides metadata associated
                   with publications, authors, and venues. We retrieved more
                   than 6 million publications from DBLP and extracted pertinent
                   metadata (e.g., abstracts, author affiliations, citations)
                   from the publication texts to create the DBLP Discovery
                   Dataset (D3). D3 can be used to identify trends in research
                   activity, productivity, focus, bias, accessibility, and
                   impact of computer science research. We present an initial
                   analysis focused on the volume of computer science research
                   (e.g., number of papers, authors, research activity), trends
                   in topics of interest, and citation patterns. Our findings
                   show that computer science is a growing research field
                   (approx. 15\% annually), with an active and collaborative
                   researcher community. While papers in recent years present
                   more bibliographical entries in comparison to previous
                   decades, the average number of citations has been declining.
                   Investigating papers' abstracts reveals that recent topic
                   trends are clearly reflected in D3. Finally, we list further
                   applications of D3 and pose supplemental research questions.
                   The D3 dataset, our findings, and source code are publicly
                   available for research purposes.",
  month         =  apr,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.DL",
  eprint        = "2204.13384"
}

@ARTICLE{Wang2022-gh,
  title         = "{KECP}: Knowledge enhanced contrastive prompting for few-shot
                   extractive Question Answering",
  author        = "Wang, Jianing and Wang, Chengyu and Qiu, Minghui and Shi,
                   Qiuhui and Wang, Hongbin and Huang, Jun and Gao, Ming",
  abstract      = "Extractive Question Answering (EQA) is one of the most
                   important tasks in Machine Reading Comprehension (MRC), which
                   can be solved by fine-tuning the span selecting heads of
                   Pre-trained Language Models (PLMs). However, most existing
                   approaches for MRC may perform poorly in the few-shot
                   learning scenario. To solve this issue, we propose a novel
                   framework named Knowledge Enhanced Contrastive Prompt-tuning
                   (KECP). Instead of adding pointer heads to PLMs, we introduce
                   a seminal paradigm for EQA that transform the task into a
                   non-autoregressive Masked Language Modeling (MLM) generation
                   problem. Simultaneously, rich semantics from the external
                   knowledge base (KB) and the passage context are support for
                   enhancing the representations of the query. In addition, to
                   boost the performance of PLMs, we jointly train the model by
                   the MLM and contrastive learning objectives. Experiments on
                   multiple benchmarks demonstrate that our method consistently
                   outperforms state-of-the-art approaches in few-shot settings
                   by a large margin.",
  month         =  may,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2205.03071"
}

@ARTICLE{Rubin2021-zr,
  title         = "Learning to retrieve prompts for in-context learning",
  author        = "Rubin, Ohad and Herzig, Jonathan and Berant, Jonathan",
  abstract      = "In-context learning is a recent paradigm in natural language
                   understanding, where a large pre-trained language model (LM)
                   observes a test instance and a few training examples as its
                   input, and directly decodes the output without any update to
                   its parameters. However, performance has been shown to
                   strongly depend on the selected training examples (termed
                   prompt). In this work, we propose an efficient method for
                   retrieving prompts for in-context learning using annotated
                   data and a LM. Given an input-output pair, we estimate the
                   probability of the output given the input and a candidate
                   training example as the prompt, and label training examples
                   as positive or negative based on this probability. We then
                   train an efficient dense retriever from this data, which is
                   used to retrieve training examples as prompts at test time.
                   We evaluate our approach on three sequence-to-sequence tasks
                   where language utterances are mapped to meaning
                   representations, and find that it substantially outperforms
                   prior work and multiple baselines across the board.",
  month         =  dec,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2112.08633"
}

@ARTICLE{An2021-dg,
  title         = "{RetrievalSum}: A Retrieval Enhanced Framework for
                   Abstractive Summarization",
  author        = "An, Chenxin and Zhong, Ming and Geng, Zhichao and Yang,
                   Jianqiang and Qiu, Xipeng",
  abstract      = "Existing summarization systems mostly generate summaries
                   purely relying on the content of the source document.
                   However, even for humans, we usually need some references or
                   exemplars to help us fully understand the source document and
                   write summaries in a particular format. But how to find the
                   high-quality exemplars and incorporate them into
                   summarization systems is still challenging and worth
                   exploring. In this paper, we propose RetrievalSum, a novel
                   retrieval enhanced abstractive summarization framework
                   consisting of a dense Retriever and a Summarizer. At first,
                   several closely related exemplars are retrieved as
                   supplementary input to help the generation model understand
                   the text more comprehensively. Furthermore, retrieved
                   exemplars can also play a role in guiding the model to
                   capture the writing style of a specific corpus. We validate
                   our method on a wide range of summarization datasets across
                   multiple domains and two backbone models: BERT and BART.
                   Results show that our framework obtains significant
                   improvement by 1.38~4.66 in ROUGE-1 score when compared with
                   the powerful pre-trained models, and achieve new
                   state-of-the-art on BillSum. Human evaluation demonstrates
                   that our retrieval enhanced model can better capture the
                   domain-specific writing style.",
  month         =  sep,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2109.07943"
}

@ARTICLE{Kulkarni2020-nh,
  title         = "{AQuaMuSe}: Automatically Generating Datasets for Query-Based
                   Multi-Document Summarization",
  author        = "Kulkarni, Sayali and Chammas, Sheide and Zhu, Wan and Sha,
                   Fei and Ie, Eugene",
  abstract      = "Summarization is the task of compressing source document(s)
                   into coherent and succinct passages. This is a valuable tool
                   to present users with concise and accurate sketch of the top
                   ranked documents related to their queries. Query-based
                   multi-document summarization (qMDS) addresses this pervasive
                   need, but the research is severely limited due to lack of
                   training and evaluation datasets as existing single-document
                   and multi-document summarization datasets are inadequate in
                   form and scale. We propose a scalable approach called
                   AQuaMuSe to automatically mine qMDS examples from question
                   answering datasets and large document corpora. Our approach
                   is unique in the sense that it can general a dual dataset --
                   for extractive and abstractive summaries both. We publicly
                   release a specific instance of an AQuaMuSe dataset with 5,519
                   query-based summaries, each associated with an average of 6
                   input documents selected from an index of 355M documents from
                   Common Crawl. Extensive evaluation of the dataset along with
                   baseline summarization model experiments are provided.",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2010.12694"
}

@ARTICLE{Wang2022-up,
  title         = "Training {datA} is more valuable than you think: A simple and
                   effective method by retrieving from {traINing} {datA}",
  author        = "Wang, Shuohang and Xu, Yichong and Fang, Yuwei and Liu, Yang
                   and Sun, Siqi and Xu, Ruochen and Zhu, Chenguang and Zeng,
                   Michael",
  abstract      = "Retrieval-based methods have been shown to be effective in
                   NLP tasks via introducing external knowledge. However, the
                   indexing and retrieving of large-scale corpora bring
                   considerable computational cost. Surprisingly, we found that
                   REtrieving from the traINing datA (REINA) only can lead to
                   significant gains on multiple NLG and NLU tasks. We retrieve
                   the labeled training instances most similar to the input text
                   and then concatenate them with the input to feed into the
                   model to generate the output. Experimental results show that
                   this simple method can achieve significantly better
                   performance on a variety of NLU and NLG tasks, including
                   summarization, machine translation, language modeling, and
                   question answering tasks. For instance, our proposed method
                   achieved state-of-the-art results on XSum, BigPatent, and
                   CommonsenseQA. Our code is released,
                   https://github.com/microsoft/REINA .",
  month         =  mar,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2203.08773"
}

@ARTICLE{Cao2020-es,
  title         = "{DeFormer}: Decomposing Pre-trained Transformers for Faster
                   Question Answering",
  author        = "Cao, Qingqing and Trivedi, Harsh and Balasubramanian, Aruna
                   and Balasubramanian, Niranjan",
  abstract      = "Transformer-based QA models use input-wide self-attention --
                   i.e. across both the question and the input passage -- at all
                   layers, causing them to be slow and memory-intensive. It
                   turns out that we can get by without input-wide
                   self-attention at all layers, especially in the lower layers.
                   We introduce DeFormer, a decomposed transformer, which
                   substitutes the full self-attention with question-wide and
                   passage-wide self-attentions in the lower layers. This allows
                   for question-independent processing of the input text
                   representations, which in turn enables pre-computing passage
                   representations reducing runtime compute drastically.
                   Furthermore, because DeFormer is largely similar to the
                   original model, we can initialize DeFormer with the
                   pre-training weights of a standard transformer, and directly
                   fine-tune on the target QA dataset. We show DeFormer versions
                   of BERT and XLNet can be used to speed up QA by over 4.3x and
                   with simple distillation-based losses they incur only a 1\%
                   drop in accuracy. We open source the code at
                   https://github.com/StonyBrookNLP/deformer.",
  month         =  may,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2005.00697"
}

@ARTICLE{Mavi2022-ku,
  title         = "A survey on multi-hop Question Answering and generation",
  author        = "Mavi, Vaibhav and Jangra, Anubhav and Jatowt, Adam",
  journal       = "arXiv [cs.CL]",
  publisher     = "arXiv",
  abstract      = "The problem of Question Answering (QA) has attracted
                   significant research interest for long. Its relevance to
                   language understanding and knowledge retrieval tasks, along
                   with the simple setting makes the task of QA crucial for
                   strong AI systems. Recent success on simple QA tasks has
                   shifted the focus to more complex settings. Among these,
                   Multi-Hop QA (MHQA) is one of the most researched tasks over
                   the recent years. The ability to answer multi-hop questions
                   and perform multi step reasoning can significantly improve
                   the utility of NLP systems. Consequently, the field has seen
                   a sudden surge with high quality datasets, models and
                   evaluation strategies. The notion of `multiple hops' is
                   somewhat abstract which results in a large variety of tasks
                   that require multi-hop reasoning. This implies that different
                   datasets and models differ significantly which makes the
                   field challenging to generalize and survey. This work aims to
                   provide a general and formal definition of MHQA task, and
                   organize and summarize existing MHQA frameworks. We also
                   outline the best methods to create MHQA datasets. The paper
                   provides a systematic and thorough introduction as well as
                   the structuring of the existing attempts to this highly
                   interesting, yet quite challenging task.",
  month         =  apr,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2204.09140",
  doi           = "10.48550/ARXIV.2204.09140"
}

@MISC{noauthor_undated-kc,
  title        = "No title",
  howpublished = "\url{https://academic.oup.com/ser/article/21/2/1217/7030814}",
  note         = "Accessed: 2023-6-13"
}

@MISC{noauthor_undated-mz,
  howpublished = "\url{https://aclanthology.org/2022.amta-research.9}",
  note         = "Accessed: 2023-6-11"
}

@ARTICLE{Stengel-Eskin2022-uh,
  title         = "When more data hurts: A troubling quirk in developing
                   broad-coverage natural language understanding systems",
  author        = "Stengel-Eskin, Elias and Platanios, Emmanouil Antonios and
                   Pauls, Adam and Thomson, Sam and Fang, Hao and Van Durme,
                   Benjamin and Eisner, Jason and Su, Yu",
  abstract      = "In natural language understanding (NLU) production systems,
                   users' evolving needs necessitate the addition of new
                   features over time, indexed by new symbols added to the
                   meaning representation space. This requires additional
                   training data and results in ever-growing datasets. We
                   present the first systematic investigation of this
                   incremental symbol learning scenario. Our analysis reveals a
                   troubling quirk in building broad-coverage NLU systems: as
                   the training dataset grows, performance on the new symbol
                   often decreases if we do not accordingly increase its
                   training data. This suggests that it becomes more difficult
                   to learn new symbols with a larger training dataset. We show
                   that this trend holds for multiple mainstream models on two
                   common NLU tasks: intent recognition and semantic parsing.
                   Rejecting class imbalance as the sole culprit, we reveal that
                   the trend is closely associated with an effect we call source
                   signal dilution, where strong lexical cues for the new symbol
                   become diluted as the training dataset grows. Selectively
                   dropping training examples to prevent dilution often reverses
                   the trend, showing the over-reliance of mainstream neural NLU
                   models on simple lexical cues. Code, models, and data are
                   available at https://aka.ms/nlu-incremental-symbol-learning",
  month         =  may,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2205.12228"
}

@ARTICLE{Zeng2022-jc,
  title         = "{GLM}-{130B}: An Open Bilingual Pre-trained Model",
  author        = "Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan
                   and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan
                   and Zheng, Wendi and Xia, Xiao and Tam, Weng Lam and Ma,
                   Zixuan and Xue, Yufei and Zhai, Jidong and Chen, Wenguang and
                   Zhang, Peng and Dong, Yuxiao and Tang, Jie",
  abstract      = "We introduce GLM-130B, a bilingual (English and Chinese)
                   pre-trained language model with 130 billion parameters. It is
                   an attempt to open-source a 100B-scale model at least as good
                   as GPT-3 and unveil how models of such a scale can be
                   successfully pre-trained. Over the course of this effort, we
                   face numerous unexpected technical and engineering
                   challenges, particularly on loss spikes and disconvergence.
                   In this paper, we introduce the training process of GLM-130B
                   including its design choices, training strategies for both
                   efficiency and stability, and engineering efforts. The
                   resultant GLM-130B model offers significant outperformance
                   over GPT-3 175B on a wide range of popular English benchmarks
                   while the performance advantage is not observed in OPT-175B
                   and BLOOM-176B. It also consistently and significantly
                   outperforms ERNIE TITAN 3.0 260B -- the largest Chinese
                   language model -- across related benchmarks. Finally, we
                   leverage a unique scaling property of GLM-130B to reach INT4
                   quantization, without quantization aware training and with
                   almost no performance loss, making it the first among
                   100B-scale models. More importantly, the property allows its
                   effective inference on 4$\times$RTX 3090 (24G) or
                   8$\times$RTX 2080 Ti (11G) GPUs, the most ever affordable
                   GPUs required for using 100B-scale models. The GLM-130B model
                   weights are publicly accessible and its code, training logs,
                   related toolkit, and lessons learned are open-sourced at
                   https://github.com/THUDM/GLM-130B .",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2210.02414"
}

@INPROCEEDINGS{Caciularu2022-du,
  title     = "Long Context Question Answering via Supervised Contrastive
               Learning",
  author    = "Caciularu, Avi and Dagan, Ido and Goldberger, Jacob and Cohan,
               Arman",
  booktitle = "Proceedings of the 2022 Conference of the North American Chapter
               of the Association for Computational Linguistics: Human Language
               Technologies",
  publisher = "Association for Computational Linguistics",
  address   = "Seattle, United States",
  pages     = "2872--2879",
  abstract  = "Long-context question answering (QA) tasks require reasoning over
               a long document or multiple documents. Addressing these tasks
               often benefits from identifying a set of evidence spans (e.g.,
               sentences), which provide supporting evidence for answering the
               question.In this work, we propose a novel method for equipping
               long-context QA models with an additional sequence-level
               objective for better identification of the supporting evidence.We
               achieve this via an additional contrastive supervision signal in
               finetuning, where the model is encouraged to explicitly
               discriminate supporting evidence sentences from negative ones by
               maximizing question-evidence similarity. The proposed additional
               loss exhibits consistent improvements on three different strong
               long-context transformer models, across two challenging question
               answering benchmarks -- HotpotQA and QAsper.",
  month     =  jul,
  year      =  2022,
  doi       = "10.18653/v1/2022.naacl-main.207"
}

@INPROCEEDINGS{Jain2020-jn,
  title     = "{SciREX}: A Challenge Dataset for Document-Level Information
               Extraction",
  author    = "Jain, Sarthak and van Zuylen, Madeleine and Hajishirzi, Hannaneh
               and Beltagy, Iz",
  booktitle = "Proceedings of the 58th Annual Meeting of the Association for
               Computational Linguistics",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  year      =  2020,
  doi       = "10.18653/v1/2020.acl-main.670"
}

@ARTICLE{Groeneveld2020-qz,
  title         = "A Simple Yet Strong Pipeline for {HotpotQA}",
  author        = "Groeneveld, Dirk and Khot, Tushar and {Mausam} and Sabharwal,
                   Ashish",
  abstract      = "State-of-the-art models for multi-hop question answering
                   typically augment large-scale language models like BERT with
                   additional, intuitively useful capabilities such as named
                   entity recognition, graph-based reasoning, and question
                   decomposition. However, does their strong performance on
                   popular multi-hop datasets really justify this added design
                   complexity? Our results suggest that the answer may be no,
                   because even our simple pipeline based on BERT, named Quark,
                   performs surprisingly well. Specifically, on HotpotQA, Quark
                   outperforms these models on both question answering and
                   support identification (and achieves performance very close
                   to a RoBERTa model). Our pipeline has three steps: 1) use
                   BERT to identify potentially relevant sentences independently
                   of each other; 2) feed the set of selected sentences as
                   context into a standard BERT span prediction model to choose
                   an answer; and 3) use the sentence selection model, now with
                   the chosen answer, to produce supporting sentences. The
                   strong performance of Quark resurfaces the importance of
                   carefully exploring simple model designs before using popular
                   benchmarks to justify the value of complex techniques.",
  month         =  apr,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2004.06753"
}

@ARTICLE{Liu2021-pp,
  title         = "Bridging Subword Gaps in Pretrain-Finetune Paradigm for
                   Natural Language Generation",
  author        = "Liu, Xin and Yang, Baosong and Liu, Dayiheng and Zhang, Haibo
                   and Luo, Weihua and Zhang, Min and Zhang, Haiying and Su,
                   Jinsong",
  abstract      = "A well-known limitation in pretrain-finetune paradigm lies in
                   its inflexibility caused by the one-size-fits-all vocabulary.
                   This potentially weakens the effect when applying pretrained
                   models into natural language generation (NLG) tasks,
                   especially for the subword distributions between upstream and
                   downstream tasks with significant discrepancy. Towards
                   approaching this problem, we extend the vanilla
                   pretrain-finetune pipeline with an extra embedding transfer
                   step. Specifically, a plug-and-play embedding generator is
                   introduced to produce the representation of any input token,
                   according to pre-trained embeddings of its morphologically
                   similar ones. Thus, embeddings of mismatch tokens in
                   downstream tasks can also be efficiently initialized. We
                   conduct experiments on a variety of NLG tasks under the
                   pretrain-finetune fashion. Experimental results and extensive
                   analyses show that the proposed strategy offers us
                   opportunities to feel free to transfer the vocabulary,
                   leading to more efficient and better performed downstream NLG
                   models.",
  month         =  jun,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2106.06125v1"
}

@INPROCEEDINGS{Wadden2022-zm,
  title     = "{MultiVerS}: Improving scientific claim verification with weak
               supervision and full-document context",
  author    = "Wadden, David and Lo, Kyle and Wang, Lucy and Cohan, Arman and
               Beltagy, Iz and Hajishirzi, Hannaneh",
  booktitle = "Findings of the Association for Computational Linguistics: NAACL
               2022",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "61--76",
  abstract  = "The scientific claim verification task requires an NLP system to
               label scientific documents which Support or Refute an input
               claim, and to select evidentiary sentences (or rationales)
               justifying each predicted label. In this work, we present
               MultiVerS, which predicts a fact-checking label and identifies
               rationales in a multitask fashion based on a shared encoding of
               the claim and full document context. This approach accomplishes
               two key modeling goals. First, it ensures that all relevant
               contextual information is incorporated into each labeling
               decision. Second, it enables the model to learn from instances
               annotated with a document-level fact-checking label, but lacking
               sentence-level rationales. This allows MultiVerS to perform
               weakly-supervised domain adaptation by training on scientific
               documents labeled using high-precision heuristics. Our approach
               outperforms two competitive baselines on three scientific claim
               verification datasets, with particularly strong performance in
               zero / few-shot domain adaptation experiments. Our code and data
               are available at https://github.com/dwadden/multivers.",
  month     =  jul,
  year      =  2022,
  doi       = "10.18653/v1/2022.findings-naacl.6"
}

@ARTICLE{Birhane2021-xl,
  title         = "The Values Encoded in Machine Learning Research",
  author        = "Birhane, Abeba and Kalluri, Pratyusha and Card, Dallas and
                   Agnew, William and Dotan, Ravit and Bao, Michelle",
  abstract      = "Machine learning currently exerts an outsized influence on
                   the world, increasingly affecting institutional practices and
                   impacted communities. It is therefore critical that we
                   question vague conceptions of the field as value-neutral or
                   universally beneficial, and investigate what specific values
                   the field is advancing. In this paper, we first introduce a
                   method and annotation scheme for studying the values encoded
                   in documents such as research papers. Applying the scheme, we
                   analyze 100 highly cited machine learning papers published at
                   premier machine learning conferences, ICML and NeurIPS. We
                   annotate key features of papers which reveal their values:
                   their justification for their choice of project, which
                   attributes of their project they uplift, their consideration
                   of potential negative consequences, and their institutional
                   affiliations and funding sources. We find that few of the
                   papers justify how their project connects to a societal need
                   (15\%) and far fewer discuss negative potential (1\%).
                   Through line-by-line content analysis, we identify 59 values
                   that are uplifted in ML research, and, of these, we find that
                   the papers most frequently justify and assess themselves
                   based on Performance, Generalization, Quantitative evidence,
                   Efficiency, Building on past work, and Novelty. We present
                   extensive textual evidence and identify key themes in the
                   definitions and operationalization of these values. Notably,
                   we find systematic textual evidence that these top values are
                   being defined and applied with assumptions and implications
                   generally supporting the centralization of power.Finally, we
                   find increasingly close ties between these highly cited
                   papers and tech companies and elite universities.",
  month         =  jun,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2106.15590"
}

@ARTICLE{Ko2021-ii,
  title    = "Discourse Comprehension: A Question Answering Framework to
              Represent Sentence Connections",
  author   = "Ko, Wei-Jen and Dalton, Cutter and Simmons, M and Fisher, Eliza
              and Durrett, Greg and Li, Junyi Jessy",
  journal  = "ArXiv",
  abstract = "A novel paradigm that enables scalable data collection targeting
              the comprehension of news documents, viewing these questions
              through the lens of discourse, and shows that DCQA provides
              valuable supervision for answering open-ended questions. While
              there has been substantial progress in text comprehension through
              simple factoid question answering, more holistic comprehension of
              a discourse still presents a major challenge (Dunietz et al.,
              2020). Someone critically reﬂecting on a text as they read it will
              pose curiosity-driven, often open-ended questions, which reﬂect
              deep understanding of the content and require complex reasoning to
              answer (Ko et al., 2020; Westera et al., 2020). A key challenge in
              building and evaluating models for this type of discourse
              comprehension is the lack of annotated data, especially since
              ﬁnding answers to such questions (which may not be answered at
              all) requires high cognitive load for annotators over long
              documents. This paper presents a novel paradigm that enables
              scalable data collection targeting the comprehension of news
              documents, viewing these questions through the lens of discourse.
              The resulting corpus, DCQA ( D iscourse C omprehension by Q
              uestion A nswering), consists of 22,394 question-answer pairs
              across 606 English documents. DCQA captures both discourse and
              semantic links between sentences in the form of free-form,
              open-ended questions. On an evaluation set that we annotated on
              questions from Ko et al. (2020), we show that DCQA provides
              valuable supervision for answering open-ended questions. We
              additionally design pre-training methods utilizing existing
              question-answering resources, and use synthetic data to
              accommodate unanswerable questions. We release DCQA",
  year     =  2021,
  eprint   = "2111.00701",
  language = "en"
}

@INPROCEEDINGS{King2020-rx,
  title     = "High-Precision Extraction of Emerging Concepts from Scientific
               Literature",
  author    = "King, Daniel and Downey, Doug and Weld, Daniel S",
  booktitle = "Proceedings of the 43rd International ACM SIGIR Conference on
               Research and Development in Information Retrieval",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1549--1552",
  abstract  = "Identification of new concepts in scientific literature can help
               power faceted search, scientific trend analysis, knowledge-base
               construction, and more, but current methods are lacking. Manual
               identification can't keep up with the torrent of new
               publications, while the precision of existing automatic
               techniques is too low for many applications. We present an
               unsupervised concept extraction method for scientific literature
               that achieves much higher precision than previous work. Our
               approach relies on a simple but novel intuition: each scientific
               concept is likely to be introduced or popularized by a single
               paper that is disproportionately cited by subsequent papers
               mentioning the concept. From a corpus of computer science papers
               on arXiv, we find that our method achieves a Precision@1000 of
               99\%, compared to 86\% for prior work, and a substantially better
               precision-yield trade-off across the top 15,000 extractions. To
               stimulate research in this area, we release our code and data.",
  series    = "SIGIR '20",
  month     =  jul,
  year      =  2020,
  keywords  = "concept extraction, scientific literature, citation graph",
  doi       = "10.1145/3397271.3401235",
  isbn      =  9781450380164
}

@MISC{Lucy1991-uf,
  title   = "The Ernst \& Young approach to facilities management",
  author  = "Lucy, J de and de Lucy, J",
  journal = "Property Management",
  volume  =  9,
  number  =  1,
  pages   = "24--31",
  year    =  1991,
  doi     = "10.1108/02637479110029793"
}

@ARTICLE{Ushio2022-kq,
  title         = "Generative Language Models for Paragraph-Level Question
                   Generation",
  author        = "Ushio, Asahi and Alva-Manchego, Fernando and
                   Camacho-Collados, Jose",
  abstract      = "Powerful generative models have led to recent progress in
                   question generation (QG). However, it is difficult to measure
                   advances in QG research since there are no standardized
                   resources that allow a uniform comparison among approaches.
                   In this paper, we introduce QG-Bench, a multilingual and
                   multidomain benchmark for QG that unifies existing question
                   answering datasets by converting them to a standard QG
                   setting. It includes general-purpose datasets such as SQuAD
                   for English, datasets from ten domains and two styles, as
                   well as datasets in eight different languages. Using QG-Bench
                   as a reference, we perform an extensive analysis of the
                   capabilities of language models for the task. First, we
                   propose robust QG baselines based on fine-tuning generative
                   language models. Then, we complement automatic evaluation
                   based on standard metrics with an extensive manual
                   evaluation, which in turn sheds light on the difficulty of
                   evaluating QG models. Finally, we analyse both the domain
                   adaptability of these models as well as the effectiveness of
                   multilingual models in languages other than English. QG-Bench
                   is released along with the fine-tuned models presented in the
                   paper https://github.com/asahi417/lm-question-generation,
                   which are also available as a demo https://autoqg.net/.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2210.03992"
}

@ARTICLE{Dasigi2021-tv,
  title         = "A Dataset of Information-Seeking Questions and Answers
                   Anchored in Research Papers",
  author        = "Dasigi, Pradeep and Lo, Kyle and Beltagy, Iz and Cohan, Arman
                   and Smith, Noah A and Gardner, Matt",
  abstract      = "Readers of academic research papers often read with the goal
                   of answering specific questions. Question Answering systems
                   that can answer those questions can make consumption of the
                   content much more efficient. However, building such tools
                   requires data that reflect the difficulty of the task arising
                   from complex reasoning about claims made in multiple parts of
                   a paper. In contrast, existing information-seeking question
                   answering datasets usually contain questions about generic
                   factoid-type information. We therefore present QASPER, a
                   dataset of 5,049 questions over 1,585 Natural Language
                   Processing papers. Each question is written by an NLP
                   practitioner who read only the title and abstract of the
                   corresponding paper, and the question seeks information
                   present in the full text. The questions are then answered by
                   a separate set of NLP practitioners who also provide
                   supporting evidence to answers. We find that existing models
                   that do well on other QA tasks do not perform well on
                   answering these questions, underperforming humans by at least
                   27 F1 points when answering them from entire papers,
                   motivating further research in document-grounded,
                   information-seeking QA, which our dataset is designed to
                   facilitate.",
  month         =  may,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2105.03011"
}

@ARTICLE{DeYoung2021-ft,
  title         = "{MS2}: Multi-Document Summarization of Medical Studies",
  author        = "DeYoung, Jay and Beltagy, Iz and van Zuylen, Madeleine and
                   Kuehl, Bailey and Wang, Lucy Lu",
  abstract      = "To assess the effectiveness of any medical intervention,
                   researchers must conduct a time-intensive and highly manual
                   literature review. NLP systems can help to automate or assist
                   in parts of this expensive process. In support of this goal,
                   we release MS\textasciicircum2 (Multi-Document Summarization
                   of Medical Studies), a dataset of over 470k documents and 20k
                   summaries derived from the scientific literature. This
                   dataset facilitates the development of systems that can
                   assess and aggregate contradictory evidence across multiple
                   studies, and is the first large-scale, publicly available
                   multi-document summarization dataset in the biomedical
                   domain. We experiment with a summarization system based on
                   BART, with promising early results. We formulate our
                   summarization inputs and targets in both free text and
                   structured forms and modify a recently proposed metric to
                   assess the quality of our system's generated summaries. Data
                   and models are available at https://github.com/allenai/ms2",
  month         =  apr,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2104.06486"
}

@ARTICLE{Chen2022-mh,
  title         = "Augmenting Pre-trained Language Models with {QA}-Memory for
                   Open-Domain Question Answering",
  author        = "Chen, Wenhu and Verga, Pat and de Jong, Michiel and Wieting,
                   John and Cohen, William",
  abstract      = "Retrieval augmented language models have recently become the
                   standard for knowledge intensive tasks. Rather than relying
                   purely on latent semantics within the parameters of large
                   neural models, these methods enlist a semi-parametric memory
                   to encode an index of knowledge for the model to retrieve
                   over. Most prior work has employed text passages as the unit
                   of knowledge, which has high coverage at the cost of
                   interpretability, controllability, and efficiency. The
                   opposite properties arise in other methods which have instead
                   relied on knowledge base (KB) facts. At the same time, more
                   recent work has demonstrated the effectiveness of storing and
                   retrieving from an index of Q-A pairs derived from text
                   \citep{lewis2021paq}. This approach yields a high coverage
                   knowledge representation that maintains KB-like properties
                   due to its representations being more atomic units of
                   information. In this work we push this line of research
                   further by proposing a question-answer augmented
                   encoder-decoder model and accompanying pretraining strategy.
                   This yields an end-to-end system that not only outperforms
                   prior QA retrieval methods on single-hop QA tasks but also
                   enables compositional reasoning, as demonstrated by strong
                   performance on two multi-hop QA datasets. Together, these
                   methods improve the ability to interpret and control the
                   model while narrowing the performance gap with passage
                   retrieval systems.",
  month         =  apr,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2204.04581"
}

@ARTICLE{Chen2022-hg,
  title         = "{MuRAG}: Multimodal Retrieval-Augmented Generator for Open
                   Question Answering over Images and Text",
  author        = "Chen, Wenhu and Hu, Hexiang and Chen, Xi and Verga, Pat and
                   Cohen, William W",
  abstract      = "While language Models store a massive amount of world
                   knowledge implicitly in their parameters, even very large
                   models often fail to encode information about rare entities
                   and events, while incurring huge computational costs.
                   Recently, retrieval-augmented models, such as REALM, RAG, and
                   RETRO, have incorporated world knowledge into language
                   generation by leveraging an external non-parametric index and
                   have demonstrated impressive performance with constrained
                   model sizes. However, these methods are restricted to
                   retrieving only textual knowledge, neglecting the ubiquitous
                   amount of knowledge in other modalities like images -- much
                   of which contains information not covered by any text. To
                   address this limitation, we propose the first Multimodal
                   Retrieval-Augmented Transformer (MuRAG), which accesses an
                   external non-parametric multimodal memory to augment language
                   generation. MuRAG is pre-trained with a mixture of
                   large-scale image-text and text-only corpora using a joint
                   contrastive and generative loss. We perform experiments on
                   two different datasets that require retrieving and reasoning
                   over both images and text to answer a given query: WebQA, and
                   MultimodalQA. Our results show that MuRAG achieves
                   state-of-the-art accuracy, outperforming existing models by
                   10-20\% absolute on both datasets and under both distractor
                   and full-wiki settings.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2210.02928"
}

@ARTICLE{Wu2022-qu,
  title         = "An Efficient Memory-Augmented Transformer for
                   Knowledge-Intensive {NLP} Tasks",
  author        = "Wu, Yuxiang and Zhao, Yu and Hu, Baotian and Minervini,
                   Pasquale and Stenetorp, Pontus and Riedel, Sebastian",
  abstract      = "Access to external knowledge is essential for many natural
                   language processing tasks, such as question answering and
                   dialogue. Existing methods often rely on a parametric model
                   that stores knowledge in its parameters, or use a
                   retrieval-augmented model that has access to an external
                   knowledge source. Parametric and retrieval-augmented models
                   have complementary strengths in terms of computational
                   efficiency and predictive accuracy. To combine the strength
                   of both approaches, we propose the Efficient Memory-Augmented
                   Transformer (EMAT) -- it encodes external knowledge into a
                   key-value memory and exploits the fast maximum inner product
                   search for memory querying. We also introduce pre-training
                   tasks that allow EMAT to encode informative key-value
                   representations, and to learn an implicit strategy to
                   integrate multiple memory slots into the transformer.
                   Experiments on various knowledge-intensive tasks such as
                   question answering and dialogue datasets show that, simply
                   augmenting parametric models (T5-base) using our method
                   produces more accurate results (e.g., 25.8 -> 44.3 EM on NQ)
                   while retaining a high throughput (e.g., 1000 queries/s on
                   NQ). Compared to retrieval-augmented models, EMAT runs
                   substantially faster across the board and produces more
                   accurate results on WoW and ELI5. Our code and datasets are
                   available at https://github. com/uclnlp/EMAT.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2210.16773"
}

@ARTICLE{Lewis2021-mc,
  title     = "Paq: 65 million probably-asked questions and what you can do with
               them",
  author    = "Lewis, Patrick and Wu, Yuxiang and Liu, Linqing and Minervini,
               Pasquale and Küttler, Heinrich and Piktus, Aleksandra and
               Stenetorp, Pontus and Riedel, Sebastian",
  journal   = "Transactions of the Association for Computational Linguistics",
  publisher = "MIT Press",
  volume    =  9,
  pages     = "1098--1115",
  year      =  2021
}

@ARTICLE{Zhao2022-lc,
  title         = "On Measuring the Intrinsic Few-Shot Hardness of Datasets",
  author        = "Zhao, Xinran and Murty, Shikhar and Manning, Christopher D",
  abstract      = "While advances in pre-training have led to dramatic
                   improvements in few-shot learning of NLP tasks, there is
                   limited understanding of what drives successful few-shot
                   adaptation in datasets. In particular, given a new dataset
                   and a pre-trained model, what properties of the dataset make
                   it \emph{few-shot learnable} and are these properties
                   independent of the specific adaptation techniques used? We
                   consider an extensive set of recent few-shot learning
                   methods, and show that their performance across a large
                   number of datasets is highly correlated, showing that
                   few-shot hardness may be intrinsic to datasets, for a given
                   pre-trained model. To estimate intrinsic few-shot hardness,
                   we then propose a simple and lightweight metric called
                   ``Spread'' that captures the intuition that few-shot learning
                   is made possible by exploiting feature-space invariances
                   between training and test samples. Our metric better accounts
                   for few-shot hardness compared to existing notions of
                   hardness, and is ~8-100x faster to compute.",
  month         =  nov,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2211.09113"
}

@ARTICLE{Boytsov2023-gs,
  title         = "{InPars}-light: Cost-effective unsupervised training of
                   efficient rankers",
  author        = "Boytsov, Leonid and Patel, Preksha and Sourabh, Vivek and
                   Nisar, Riddhi and Kundu, Sayani and Ramanathan, Ramya and
                   Nyberg, Eric",
  journal       = "arXiv [cs.IR]",
  publisher     = "arXiv",
  abstract      = "We carried out a reproducibility study of InPars recipe for
                   unsupervised training of neural rankers. As a by-product of
                   this study, we developed a simple-yet-effective modification
                   of InPars, which we called InPars-light. Unlike InPars,
                   InPars-light uses only a freely available language model
                   BLOOM and 7x-100x smaller ranking models. On all five English
                   retrieval collections (used in the original InPars study) we
                   obtained substantial (7-30\%) and statistically significant
                   improvements over BM25 in nDCG or MRR using only a 30M
                   parameter six-layer MiniLM ranker. In contrast, in the InPars
                   study only a 100x larger MonoT5-3B model consistently
                   outperformed BM25, whereas their smaller MonoT5-220M model
                   (which is still 7x larger than our MiniLM ranker),
                   outperformed BM25 only on MS MARCO and TREC DL 2020. In a
                   purely unsupervised setting, our 435M parameter DeBERTA v3
                   ranker was roughly at par with the 7x larger MonoT5-3B: In
                   fact, on three out of five datasets, it slightly outperformed
                   MonoT5-3B. Finally, these good results were achieved by
                   re-ranking only 100 candidate documents compared to 1000 used
                   in InPars. We believe that InPars-light is the first truly
                   cost-effective prompt-based unsupervised recipe to train and
                   deploy neural ranking models that outperform BM25.",
  month         =  jan,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "2301.02998",
  doi           = "10.48550/ARXIV.2301.02998"
}

@ARTICLE{Bonifacio2022-wa,
  title         = "{InPars}: Data augmentation for information retrieval using
                   large language models",
  author        = "Bonifacio, Luiz and Abonizio, Hugo and Fadaee, Marzieh and
                   Nogueira, Rodrigo",
  journal       = "arXiv [cs.CL]",
  publisher     = "arXiv",
  abstract      = "The information retrieval community has recently witnessed a
                   revolution due to large pretrained transformer models.
                   Another key ingredient for this revolution was the MS MARCO
                   dataset, whose scale and diversity has enabled zero-shot
                   transfer learning to various tasks. However, not all IR tasks
                   and domains can benefit from one single dataset equally.
                   Extensive research in various NLP tasks has shown that using
                   domain-specific training data, as opposed to a
                   general-purpose one, improves the performance of neural
                   models. In this work, we harness the few-shot capabilities of
                   large pretrained language models as synthetic data generators
                   for IR tasks. We show that models finetuned solely on our
                   unsupervised dataset outperform strong baselines such as BM25
                   as well as recently proposed self-supervised dense retrieval
                   methods. Furthermore, retrievers finetuned on both supervised
                   and our synthetic data achieve better zero-shot transfer than
                   models finetuned only on supervised data. Code, models, and
                   data are available at
                   https://github.com/zetaalphavector/inpars .",
  month         =  feb,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2202.05144",
  doi           = "10.48550/ARXIV.2202.05144"
}

@ARTICLE{Wei2022-lo,
  title         = "Chain-of-Thought Prompting Elicits Reasoning in Large
                   Language Models",
  author        = "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma,
                   Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le,
                   Quoc and Zhou, Denny",
  abstract      = "We explore how generating a chain of thought -- a series of
                   intermediate reasoning steps -- significantly improves the
                   ability of large language models to perform complex
                   reasoning. In particular, we show how such reasoning
                   abilities emerge naturally in sufficiently large language
                   models via a simple method called chain of thought prompting,
                   where a few chain of thought demonstrations are provided as
                   exemplars in prompting. Experiments on three large language
                   models show that chain of thought prompting improves
                   performance on a range of arithmetic, commonsense, and
                   symbolic reasoning tasks. The empirical gains can be
                   striking. For instance, prompting a 540B-parameter language
                   model with just eight chain of thought exemplars achieves
                   state of the art accuracy on the GSM8K benchmark of math word
                   problems, surpassing even finetuned GPT-3 with a verifier.",
  month         =  jan,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2201.11903"
}

@INPROCEEDINGS{Yang2015-kp,
  title     = "Leveraging Procedural Knowledge for Task-oriented Search",
  author    = "Yang, Zi and Nyberg, Eric",
  booktitle = "Proceedings of the 38th International ACM SIGIR Conference on
               Research and Development in Information Retrieval",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "513--522",
  abstract  = "Many search engine users attempt to satisfy an information need
               by issuing multiple queries, with the expectation that each
               result will contribute some portion of the required information.
               Previous research has shown that structured or semi-structured
               descriptive knowledge bases (such as Wikipedia) can be used to
               improve search quality and experience for general or
               entity-centric queries. However, such resources do not have
               sufficient coverage of procedural knowledge, i.e. what actions
               should be performed and what factors should be considered to
               achieve some goal; such procedural knowledge is crucial when
               responding to task-oriented search queries. This paper provides a
               first attempt to bridge the gap between two evolving research
               areas: development of procedural knowledge bases (such as
               wikiHow) and task-oriented search. We investigate whether
               task-oriented search can benefit from existing procedural
               knowledge (search task suggestion) and whether automatic
               procedural knowledge construction can benefit from users' search
               activities (automatic procedural knowledge base construction). We
               propose to create a three-way parallel corpus of queries, query
               contexts, and task descriptions, and reduce both problems to
               sequence labeling tasks. We propose a set of textual features and
               structural features to identify key search phrases from task
               descriptions, and then adapt similar features to extract
               wikiHow-style procedural knowledge descriptions from search
               queries and relevant text snippets. We compare our proposed
               solution with baseline algorithms, commercial search engines, and
               the (manually-curated) wikiHow procedural knowledge; experimental
               results show an improvement of +0.28 to +0.41 in terms of
               Precision@8 and mean average precision (MAP).",
  series    = "SIGIR '15",
  month     =  aug,
  year      =  2015,
  keywords  = "search intent, query suggestion, wikihow, search log, procedural
               knowledge base",
  doi       = "10.1145/2766462.2767744",
  isbn      =  9781450336215
}

@INPROCEEDINGS{Rajagopal2020-js,
  title     = "What-if {I} ask you to explain: Explaining the effects of
               perturbations in procedural text",
  author    = "Rajagopal, Dheeraj and Tandon, Niket and Clark, Peter and Dalvi,
               Bhavana and Hovy, Eduard",
  booktitle = "Findings of the Association for Computational Linguistics: EMNLP
               2020",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  abstract  = "QUARTET, a system that constructs explanations from the sentences
               in the procedural text, achieves ~18 points better on explanation
               accuracy compared to several strong baselines on a recent process
               comprehension benchmark, and shows a surprising finding that good
               explanations do not have to come at the expense of end task
               performance. Our goal is to explain the effects of perturbations
               in procedural text, e.g., given a passage describing a rabbit’s
               life cycle, explain why illness (the perturbation) may reduce the
               rabbit population (the effect). Although modern systems are able
               to solve the original prediction task well (e.g., illness results
               in less rabbits), the explanation task - identifying the causal
               chain of events from perturbation to effect - remains largely
               unaddressed, and is the goal of this research. We present
               QUARTET, a system that constructs such explanations from
               paragraphs, by modeling the explanation task as a multitask
               learning problem. QUARTET constructs explanations from the
               sentences in the procedural text, achieving ~18 points better on
               explanation accuracy compared to several strong baselines on a
               recent process comprehension benchmark. On an end task on this
               benchmark, we show a surprising finding that good explanations do
               not have to come at the expense of end task performance, in fact
               leading to a 7\% F1 improvement over SOTA.",
  year      =  2020,
  doi       = "10.18653/v1/2020.findings-emnlp.300",
  language  = "en"
}

@ARTICLE{Gao2023-oz,
  title         = "Enabling large language models to generate text with
                   citations",
  author        = "Gao, Tianyu and Yen, Howard and Yu, Jiatong and Chen, Danqi",
  abstract      = "Large language models (LLMs) have emerged as a widely-used
                   tool for information seeking, but their generated outputs are
                   prone to hallucination. In this work, we aim to enable LLMs
                   to generate text with citations, improving their factual
                   correctness and verifiability. Existing work mainly relies on
                   commercial search engines and human evaluation, making it
                   challenging to reproduce and compare with different modeling
                   approaches. We propose ALCE, the first benchmark for
                   Automatic LLMs' Citation Evaluation. ALCE collects a diverse
                   set of questions and retrieval corpora and requires building
                   end-to-end systems to retrieve supporting evidence and
                   generate answers with citations. We build automatic metrics
                   along three dimensions -- fluency, correctness, and citation
                   quality -- and demonstrate their strong correlation with
                   human judgements. Our experiments with state-of-the-art LLMs
                   and novel prompting strategies show that current systems have
                   considerable room for improvements -- for example, on the
                   ELI5 dataset, even the best model has 49\% of its generations
                   lacking complete citation support. Our extensive analyses
                   further highlight promising future directions, including
                   developing better retrievers, advancing long-context LLMs,
                   and improving the ability to synthesize information from
                   multiple sources.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.14627"
}

@ARTICLE{Gao2022-jc,
  title         = "{RARR}: Researching and revising what language models say,
                   using language models",
  author        = "Gao, Luyu and Dai, Zhuyun and Pasupat, Panupong and Chen,
                   Anthony and Chaganty, Arun Tejasvi and Fan, Yicheng and Zhao,
                   Vincent Y and Lao, Ni and Lee, Hongrae and Juan, Da-Cheng and
                   Guu, Kelvin",
  abstract      = "Language models (LMs) now excel at many tasks such as
                   few-shot learning, question answering, reasoning, and dialog.
                   However, they sometimes generate unsupported or misleading
                   content. A user cannot easily determine whether their outputs
                   are trustworthy or not, because most LMs do not have any
                   built-in mechanism for attribution to external evidence. To
                   enable attribution while still preserving all the powerful
                   advantages of recent generation models, we propose RARR
                   (Retrofit Attribution using Research and Revision), a system
                   that 1) automatically finds attribution for the output of any
                   text generation model and 2) post-edits the output to fix
                   unsupported content while preserving the original output as
                   much as possible. When applied to the output of several
                   state-of-the-art LMs on a diverse set of generation tasks, we
                   find that RARR significantly improves attribution while
                   otherwise preserving the original input to a much greater
                   degree than previously explored edit models. Furthermore, the
                   implementation of RARR requires only a handful of training
                   examples, a large language model, and standard web search.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2210.08726"
}

@ARTICLE{Ravfogel2023-bj,
  title         = "Retrieving Texts based on Abstract Descriptions",
  author        = "Ravfogel, Shauli and Pyatkin, Valentina and Cohen, Amir D N
                   and Manevich, Avshalom and Goldberg, Yoav",
  abstract      = "In this work, we aim to connect two research areas:
                   instruction models and retrieval-based models. While
                   instruction-tuned Large Language Models (LLMs) excel at
                   extracting information from text, they are not suitable for
                   semantic retrieval. Similarity search over embedding vectors
                   allows to index and query vectors, but the similarity
                   reflected in the embedding is sub-optimal for many use cases.
                   We identify the task of retrieving sentences based on
                   abstract descriptions of their content. We demonstrate the
                   inadequacy of current text embeddings and propose an
                   alternative model that significantly improves when used in
                   standard nearest neighbor search. The model is trained using
                   positive and negative pairs sourced through prompting an a
                   large language model (LLM). While it is easy to source the
                   training material from an LLM, the retrieval task cannot be
                   performed by the LLM directly. This demonstrates that data
                   from LLMs can be used not only for distilling more efficient
                   specialized models than the original LLM, but also for
                   creating new capabilities not immediately possible using the
                   original model.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.12517"
}

@ARTICLE{Gupta2023-ky,
  title         = "Instruction Tuned Models are Quick Learners",
  author        = "Gupta, Himanshu and Sawant, Saurabh Arjun and Mishra, Swaroop
                   and Nakamura, Mutsumi and Mitra, Arindam and Mashetty,
                   Santosh and Baral, Chitta",
  abstract      = "Instruction tuning of language models has demonstrated the
                   ability to enhance model generalization to unseen tasks via
                   in-context learning using a few examples. However, typical
                   supervised learning still requires a plethora of downstream
                   training data for finetuning. Often in real-world situations,
                   there is a scarcity of data available for finetuning, falling
                   somewhere between few shot inference and fully supervised
                   finetuning. In this work, we demonstrate the sample
                   efficiency of instruction tuned models over various tasks by
                   estimating the minimal downstream training data required by
                   them to perform transfer learning and match the performance
                   of state-of-the-art (SOTA) supervised models. We conduct
                   experiments on 119 tasks from Super Natural Instructions
                   (SuperNI) in both the single task learning (STL) and multi
                   task learning (MTL) settings. Our findings reveal that, in
                   the STL setting, instruction tuned models equipped with 25\%
                   of the downstream train data surpass the SOTA performance on
                   the downstream tasks. In the MTL setting, an instruction
                   tuned model trained on only 6\% of downstream training data
                   achieve SOTA, while using 100\% of the training data results
                   in a 3.69\% points improvement (ROUGE-L 74.68) over the
                   previous SOTA. We conduct an analysis on T5 vs Tk-Instruct by
                   developing several baselines to demonstrate that instruction
                   tuning aids in increasing both sample efficiency and transfer
                   learning. Additionally, we observe a consistent ~4\%
                   performance increase in both settings when pre-finetuning is
                   performed with instructions. Finally, we conduct a
                   categorical study and find that contrary to previous results,
                   tasks in the question rewriting and title generation
                   categories suffer from instruction tuning.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2306.05539"
}

@ARTICLE{Gudibande2023-op,
  title         = "The false promise of imitating proprietary {LLMs}",
  author        = "Gudibande, Arnav and Wallace, Eric and Snell, Charlie and
                   Geng, Xinyang and Liu, Hao and Abbeel, Pieter and Levine,
                   Sergey and Song, Dawn",
  abstract      = "An emerging method to cheaply improve a weaker language model
                   is to finetune it on outputs from a stronger model, such as a
                   proprietary system like ChatGPT (e.g., Alpaca, Self-Instruct,
                   and others). This approach looks to cheaply imitate the
                   proprietary model's capabilities using a weaker open-source
                   model. In this work, we critically analyze this approach. We
                   first finetune a series of LMs that imitate ChatGPT using
                   varying base model sizes (1.5B--13B), data sources, and
                   imitation data amounts (0.3M--150M tokens). We then evaluate
                   the models using crowd raters and canonical NLP benchmarks.
                   Initially, we were surprised by the output quality of our
                   imitation models -- they appear far better at following
                   instructions, and crowd workers rate their outputs as
                   competitive with ChatGPT. However, when conducting more
                   targeted automatic evaluations, we find that imitation models
                   close little to none of the gap from the base LM to ChatGPT
                   on tasks that are not heavily supported in the imitation
                   data. We show that these performance discrepancies may slip
                   past human raters because imitation models are adept at
                   mimicking ChatGPT's style but not its factuality. Overall, we
                   conclude that model imitation is a false promise: there
                   exists a substantial capabilities gap between open and closed
                   LMs that, with current methods, can only be bridged using an
                   unwieldy amount of imitation data or by using more capable
                   base LMs. In turn, we argue that the highest leverage action
                   for improving open-source models is to tackle the difficult
                   challenge of developing better base LMs, rather than taking
                   the shortcut of imitating proprietary systems.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.15717"
}

@ARTICLE{Saad-Falcon2023-gg,
  title         = "{UDAPDR}: Unsupervised domain adaptation via {LLM} prompting
                   and distillation of rerankers",
  author        = "Saad-Falcon, Jon and Khattab, Omar and Santhanam, Keshav and
                   Florian, Radu and Franz, Martin and Roukos, Salim and Sil,
                   Avirup and Sultan, Md Arafat and Potts, Christopher",
  abstract      = "Many information retrieval tasks require large labeled
                   datasets for fine-tuning. However, such datasets are often
                   unavailable, and their utility for real-world applications
                   can diminish quickly due to domain shifts. To address this
                   challenge, we develop and motivate a method for using large
                   language models (LLMs) to generate large numbers of synthetic
                   queries cheaply. The method begins by generating a small
                   number of synthetic queries using an expensive LLM. After
                   that, a much less expensive one is used to create large
                   numbers of synthetic queries, which are used to fine-tune a
                   family of reranker models. These rerankers are then distilled
                   into a single efficient retriever for use in the target
                   domain. We show that this technique boosts zero-shot accuracy
                   in long-tail domains, even where only 2K synthetic queries
                   are used for fine-tuning, and that it achieves substantially
                   lower latency than standard reranking methods. We make our
                   end-to-end approach, including our synthetic datasets and
                   replication code, publicly available on Github:
                   https://github.com/primeqa/primeqa.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "2303.00807"
}

@ARTICLE{Widder2022-sn,
  title         = "Dislocated accountabilities in the {AI} supply chain:
                   Modularity and developers' notions of responsibility",
  author        = "Widder, David Gray and Nafus, Dawn",
  abstract      = "Responsible AI guidelines often ask engineers to consider how
                   their systems might harm. However, contemporary AI systems
                   are built by composing many preexisting software modules that
                   pass through many hands before becoming a finished product or
                   service. How does this shape responsible AI practice? In
                   interviews with 27 AI engineers across industry, open source,
                   and academia, our participants often did not see the
                   questions posed in responsible AI guidelines to be within
                   their agency, capability, or responsibility to address. We
                   use Lucy Suchman's notion of located accountability to show
                   how responsible AI labor is currently organized, and to
                   explore how it could be done differently. We identify
                   cross-cutting social logics, like modularizability, scale,
                   reputation, and customer orientation, that organize which
                   responsible AI actions do take place, and which are relegated
                   to low status staff or believed to be the work of the next or
                   previous person in the chain. We argue that current
                   responsible AI interventions, like ethics checklists and
                   guidelines that assume panoptical knowledge and control over
                   systems, could improve by taking a located accountability
                   approach, where relations and obligations intertwine and
                   incrementally add value in the process. This would constitute
                   a shift from ``supply chain' thinking to ''value chain``
                   thinking.",
  month         =  sep,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY",
  eprint        = "2209.09780"
}

@ARTICLE{Hedderich2020-bt,
  title         = "A survey on recent approaches for natural language processing
                   in low-resource scenarios",
  author        = "Hedderich, Michael A and Lange, Lukas and Adel, Heike and
                   Strötgen, Jannik and Klakow, Dietrich",
  abstract      = "Current developments in natural language processing offer
                   challenges and opportunities for low-resource languages and
                   domains. Deep neural networks are known for requiring large
                   amounts of training data which might not be available in
                   resource-lean scenarios. However, there is also a growing
                   body of works to improve the performance in low-resource
                   settings. Motivated by fundamental changes towards neural
                   models and the currently popular pre-train and fine-tune
                   paradigm, we give an overview of promising approaches for
                   low-resource natural language processing. After a discussion
                   about the definition of low-resource scenarios and the
                   different dimensions of data availability, we then examine
                   methods that enable learning when training data is sparse.
                   This includes mechanisms to create additional labeled data
                   like data augmentation and distant supervision as well as
                   transfer learning settings that reduce the need for target
                   supervision. The survey closes with a brief look into methods
                   suggested in non-NLP machine learning communities, which
                   might be beneficial for NLP in low-resource scenarios",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2010.12309"
}

@ARTICLE{Agrawal2023-xk,
  title         = "Do language models know when they're hallucinating
                   references?",
  author        = "Agrawal, Ayush and Mackey, Lester and Kalai, Adam Tauman",
  abstract      = "Current state-of-the-art language models (LMs) are notorious
                   for generating text with ``hallucinations,'' a primary
                   example being book and paper references that lack any solid
                   basis in their training data. However, we find that many of
                   these fabrications can be identified using the same LM, using
                   only black-box queries without consulting any external
                   resources. Consistency checks done with direct queries about
                   whether the generated reference title is real (inspired by
                   Kadavath et al. 2022, Lin et al. 2022, Manakul et al. 2023)
                   are compared to consistency checks with indirect queries
                   which ask for ancillary details such as the authors of the
                   work. These consistency checks are found to be partially
                   reliable indicators of whether or not the reference is a
                   hallucination. In particular, we find that LMs in the
                   GPT-series will hallucinate differing authors of hallucinated
                   references when queried in independent sessions, while it
                   will consistently identify authors of real references. This
                   suggests that the hallucination may be more a result of
                   generation techniques than the underlying representation.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.18248"
}

@ARTICLE{Saikh2022-tu,
  title     = "{ScienceQA}: a novel resource for question answering on scholarly
               articles",
  author    = "Saikh, Tanik and Ghosal, Tirthankar and Mittal, Amish and Ekbal,
               Asif and Bhattacharyya, Pushpak",
  journal   = "International journal on digital libraries",
  publisher = "Springer Science and Business Media LLC",
  volume    =  23,
  number    =  3,
  pages     = "289--301",
  abstract  = "Machine Reading Comprehension (MRC) of a document is a
               challenging problem that requires discourse-level understanding.
               Information extraction from scholarly articles nowadays is a
               critical use case for researchers to understand the underlying
               research quickly and move forward, especially in this age of
               infodemic. MRC on research articles can also provide helpful
               information to the reviewers and editors. However, the main
               bottleneck in building such models is the availability of
               human-annotated data. In this paper, firstly, we introduce a
               dataset to facilitate question answering (QA) on scientific
               articles. We prepare the dataset in a semi-automated fashion
               having more than 100k human-annotated context-question-answer
               triples. Secondly, we implement one baseline QA model based on
               Bidirectional Encoder Representations from Transformers (BERT).
               Additionally, we implement two models: the first one is based on
               Science BERT (SciBERT), and the second is the combination of
               SciBERT and Bi-Directional Attention Flow (Bi-DAF). The best
               model (i.e., SciBERT) obtains an F1 score of 75.46\%. Our dataset
               is novel, and our work opens up a new avenue for scholarly
               document processing research by providing a benchmark QA dataset
               and standard baseline. We make our dataset and codes available
               here at
               https://github.com/TanikSaikh/Scientific-Question-Answering.",
  month     =  jul,
  year      =  2022,
  keywords  = "Automatic article review system; BERT; BiDAF; Machine reading
               comprehension; Question answering; Scholarly articles; SciBERT",
  doi       = "10.1007/s00799-022-00329-y",
  pmc       = "PMC9297303",
  pmid      =  35873651,
  issn      = "1432-5012,1432-1300",
  language  = "en"
}

@ARTICLE{Krishna2023-do,
  title         = "{USB}: A unified summarization benchmark across tasks and
                   domains",
  author        = "Krishna, Kundan and Gupta, Prakhar and Ramprasad, Sanjana and
                   Wallace, Byron C and Bigham, Jeffrey P and Lipton, Zachary C",
  abstract      = "An abundance of datasets exist for training and evaluating
                   models on the task of summary generation.However, these
                   datasets are often derived heuristically, and lack sufficient
                   annotations to support research into all aspects of
                   summarization, such as evidence extraction and controllable
                   summarization. We introduce a benchmark comprising 8 tasks
                   that require multi-dimensional understanding of
                   summarization, e.g., surfacing evidence for a summary,
                   assessing its correctness, and gauging its relevance to
                   different topics. We compare various methods on this
                   benchmark and discover that on multiple tasks,
                   moderately-sized fine-tuned models consistently outperform
                   much larger few-shot prompted language models. For factuality
                   related tasks, we also evaluate existing heuristics to create
                   training data and find that training on them performs worse
                   than training on $20\times$ less human-labeled data. Our
                   benchmark consists of data from 6 different domains, allowing
                   us to study cross-domain performance of trained models. We
                   find that for some tasks, the amount of training data matters
                   more than the domain where it comes from, while for other
                   tasks training specifically on data from the target domain,
                   even if limited, is more beneficial. Our work fulfills the
                   need for a well-annotated summarization benchmark with
                   diverse tasks, and provides useful insights about the impact
                   of the quality, size and domain of training data.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.14296"
}

@ARTICLE{Ni2021-ku,
  title         = "Large Dual Encoders Are Generalizable Retrievers",
  author        = "Ni, Jianmo and Qu, Chen and Lu, Jing and Dai, Zhuyun and
                   Ábrego, Gustavo Hernández and Ma, Ji and Zhao, Vincent Y and
                   Luan, Yi and Hall, Keith B and Chang, Ming-Wei and Yang,
                   Yinfei",
  abstract      = "It has been shown that dual encoders trained on one domain
                   often fail to generalize to other domains for retrieval
                   tasks. One widespread belief is that the bottleneck layer of
                   a dual encoder, where the final score is simply a dot-product
                   between a query vector and a passage vector, is too limited
                   to make dual encoders an effective retrieval model for
                   out-of-domain generalization. In this paper, we challenge
                   this belief by scaling up the size of the dual encoder model
                   {\em while keeping the bottleneck embedding size fixed.} With
                   multi-stage training, surprisingly, scaling up the model size
                   brings significant improvement on a variety of retrieval
                   tasks, especially for out-of-domain generalization.
                   Experimental results show that our dual encoders,
                   \textbf{G}eneralizable \textbf{T}5-based dense
                   \textbf{R}etrievers (GTR), outperform
                   \%ColBERT~\cite{khattab2020colbert} and existing sparse and
                   dense retrievers on the BEIR dataset~\cite{thakur2021beir}
                   significantly. Most surprisingly, our ablation study finds
                   that GTR is very data efficient, as it only needs 10\% of MS
                   Marco supervised data to achieve the best out-of-domain
                   performance. All the GTR models are released at
                   https://tfhub.dev/google/collections/gtr/1.",
  month         =  dec,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR",
  eprint        = "2112.07899"
}

@ARTICLE{Fu2023-ts,
  title         = "Specializing smaller language models towards multi-step
                   reasoning",
  author        = "Fu, Yao and Peng, Hao and Ou, Litu and Sabharwal, Ashish and
                   Khot, Tushar",
  abstract      = "The surprising ability of Large Language Models (LLMs) to
                   perform well on complex reasoning with only few-shot
                   chain-of-thought prompts is believed to emerge only in very
                   large-scale models (100+ billion parameters). We show that
                   such abilities can, in fact, be distilled down from GPT-3.5
                   ($\ge$ 175B) to T5 variants ($\le$ 11B). We propose model
                   specialization, to specialize the model's ability towards a
                   target task. The hypothesis is that large models (commonly
                   viewed as larger than 100B) have strong modeling power, but
                   are spread on a large spectrum of tasks. Small models
                   (commonly viewed as smaller than 10B) have limited model
                   capacity, but if we concentrate their capacity on a specific
                   target task, the model can achieve a decent improved
                   performance. We use multi-step math reasoning as our testbed
                   because it is a very typical emergent ability. We show two
                   important aspects of model abilities: (1). there exists a
                   very complex balance/ tradeoff between language models'
                   multi-dimensional abilities; (2). by paying the price of
                   decreased generic ability, we can clearly lift up the scaling
                   curve of models smaller than 10B towards a specialized
                   multi-step math reasoning ability. We further give
                   comprehensive discussions about important design choices for
                   better generalization, including the tuning data format, the
                   start model checkpoint, and a new model selection method. We
                   hope our practice and discoveries can serve as an important
                   attempt towards specialized smaller models in the new
                   research paradigm set by LLMs.",
  month         =  jan,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2301.12726"
}

@INPROCEEDINGS{Choshen2018-ps,
  title     = "Inherent biases in reference-based evaluation for grammatical
               error correction",
  author    = "Choshen, Leshem and Abend, Omri",
  booktitle = "Proceedings of the 56th Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  abstract  = "It is shown that overcoming LCB in Grammatical Error Correction
               (GEC) evaluation cannot be attained by re-scaling or by
               increasing the number of references in any feasible range,
               contrary to previous suggestions. The prevalent use of too few
               references for evaluating text-to-text generation is known to
               bias estimates of their quality ({\it low coverage bias} or LCB).
               This paper shows that overcoming LCB in Grammatical Error
               Correction (GEC) evaluation cannot be attained by re-scaling or
               by increasing the number of references in any feasible range,
               contrary to previous suggestions. This is due to the long-tailed
               distribution of valid corrections for a sentence. Concretely, we
               show that LCB incentivizes GEC systems to avoid correcting even
               when they can generate a valid correction. Consequently, existing
               systems obtain comparable or superior performance compared to
               humans, by making few but targeted changes to the input. Similar
               effects on Text Simplification further support our claims.",
  year      =  2018,
  doi       = "10.18653/v1/p18-1059",
  language  = "en"
}

@ARTICLE{Eldan2023-vp,
  title         = "{TinyStories}: How small can language models be and still
                   speak coherent English?",
  author        = "Eldan, Ronen and Li, Yuanzhi",
  abstract      = "Language models (LMs) are powerful tools for natural language
                   processing, but they often struggle to produce coherent and
                   fluent text when they are small. Models with around 125M
                   parameters such as GPT-Neo (small) or GPT-2 (small) can
                   rarely generate coherent and consistent English text beyond a
                   few words even after extensive training. This raises the
                   question of whether the emergence of the ability to produce
                   coherent English text only occurs at larger scales (with
                   hundreds of millions of parameters or more) and complex
                   architectures (with many layers of global attention). In this
                   work, we introduce TinyStories, a synthetic dataset of short
                   stories that only contain words that a typical 3 to
                   4-year-olds usually understand, generated by GPT-3.5 and
                   GPT-4. We show that TinyStories can be used to train and
                   evaluate LMs that are much smaller than the state-of-the-art
                   models (below 10 million total parameters), or have much
                   simpler architectures (with only one transformer block), yet
                   still produce fluent and consistent stories with several
                   paragraphs that are diverse and have almost perfect grammar,
                   and demonstrate reasoning capabilities. We also introduce a
                   new paradigm for the evaluation of language models: We
                   suggest a framework which uses GPT-4 to grade the content
                   generated by these models as if those were stories written by
                   students and graded by a (human) teacher. This new paradigm
                   overcomes the flaws of standard benchmarks which often
                   requires the model's output to be very structures, and
                   moreover provides a multidimensional score for the model,
                   providing scores for different capabilities such as grammar,
                   creativity and consistency. We hope that TinyStories can
                   facilitate the development, analysis and research of LMs,
                   especially for low-resource or specialized domains, and shed
                   light on the emergence of language capabilities in LMs.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.07759"
}

@ARTICLE{Jung2023-yy,
  title         = "Impossible Distillation: From low-quality model to
                   high-quality dataset \& model for summarization and
                   paraphrasing",
  author        = "Jung, Jaehun and West, Peter and Jiang, Liwei and Brahman,
                   Faeze and Lu, Ximing and Fisher, Jillian and Sorensen, Taylor
                   and Choi, Yejin",
  abstract      = "It is commonly perceived that the strongest language models
                   (LMs) rely on a combination of massive scale, instruction
                   data, and human feedback to perform specialized tasks -- e.g.
                   summarization and paraphrasing, without supervision. In this
                   paper, we propose that language models can learn to summarize
                   and paraphrase sentences, with none of these 3 factors. We
                   present Impossible Distillation, a framework that distills a
                   task-specific dataset directly from an off-the-shelf LM, even
                   when it is impossible for the LM itself to reliably solve the
                   task. By training a student model on the generated dataset
                   and amplifying its capability through self-distillation, our
                   method yields a high-quality model and dataset from a
                   low-quality teacher model, without the need for scale or
                   supervision. Using Impossible Distillation, we are able to
                   distill an order of magnitude smaller model (with only 770M
                   parameters) that outperforms 175B parameter GPT-3, in both
                   quality and controllability, as confirmed by automatic and
                   human evaluations. Furthermore, as a useful byproduct of our
                   approach, we obtain DIMSUM+, a high-quality dataset with 3.4M
                   sentence summaries and paraphrases. Our analyses show that
                   this dataset, as a purely LM-generated corpus, is more
                   diverse and more effective for generalization to unseen
                   domains than all human-authored datasets -- including
                   Gigaword with 4M samples.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2305.16635"
}

@ARTICLE{DiPaolo2022-wu,
  title     = "What's wrong with epistemic trespassing?",
  author    = "DiPaolo, Joshua",
  journal   = "Philosophical studies",
  publisher = "Springer Science and Business Media LLC",
  volume    =  179,
  number    =  1,
  pages     = "223--243",
  abstract  = "Epistemic trespassers are experts who pass judgment on questions
               in fields where they lack expertise. What's wrong with epistemic
               trespassing? I identify several limitations with a seminal
               analysis to isolate three desiderata on an answer to this
               question and motivate my own answer. An answer (i) should explain
               what's wrong in the cases that motivate inquiry into epistemic
               trespassing, (ii) should explain what's wrong with epistemic
               trespassing even if trespassers do not acknowledge their
               trespassing, and (iii) these explanations should not be
               independent of the fact that epistemic trespassing involves
               expertise. I also independently motivate a fourth desideratum:
               (iv) this account should explain the evaluative difference
               between different kinds of trespassing. To satisfy these
               desiderata, I develop a social analysis: epistemic trespassing is
               wrong because it is an abuse of expert authority that neglects
               novice vulnerabilities.",
  year      =  2022,
  keywords  = "Assertion; Epistemic dependence; Epistemic trespassing;
               Expertise; Higher-order evidence",
  doi       = "10.1007/s11098-021-01657-6",
  pmc       = "PMC8131877",
  pmid      =  34024942,
  issn      = "0031-8116,1573-0883",
  language  = "en"
}

@MISC{McCoy2023-iz,
  title        = "When language models produce text, is the text novel or copied
                  from the training set?For answers, come to our poster today at
                  \#acl2023nlp! Session 1 posters, 11:00 - 12:30 {todayCritics*}
                  are calling the work ``monumental''Link to paper:
                  https://t.co/{s6kDlGA3IH1}/2 pic.twitter.com/{qw1cKUMZvk}",
  author       = "McCoy, Tom",
  booktitle    = "Twitter",
  month        =  jul,
  year         =  2023,
  howpublished = "\url{https://twitter.com/rtommccoy/status/1678378978925506562?s=12\&t=jr8t7woBh6WnXaXmdrUbIw}",
  note         = "Accessed: 2023-7-11",
  language     = "en"
}

@ARTICLE{McCoy2023-pk,
  title     = "How much do language models copy from their training data?
               Evaluating linguistic novelty in text generation using {RAVEN}",
  author    = "McCoy, R Thomas and Smolensky, Paul and Linzen, Tal and Gao,
               Jianfeng and Celikyilmaz, Asli",
  journal   = "Transactions of the Association for Computational Linguistics",
  publisher = "MIT Press",
  volume    =  11,
  pages     = "652--670",
  abstract  = "Abstract Current language models can generate high-quality text.
               Are they simply copying text they have seen before, or have they
               learned generalizable linguistic abstractions? To tease apart
               these possibilities, we introduce RAVEN, a suite of analyses for
               assessing the novelty of generated text, focusing on sequential
               structure (n-grams) and syntactic structure. We apply these
               analyses to four neural language models trained on English (an
               LSTM, a Transformer, Transformer-XL, and GPT-2). For local
               structure—e.g., individual dependencies—text generated with a
               standard sampling scheme is substantially less novel than our
               baseline of human-generated text from each model’s test set. For
               larger-scale structure—e.g., overall sentence
               structure—model-generated text is as novel or even more novel
               than the human-generated baseline, but models still sometimes
               copy substantially, in some cases duplicating passages over 1,000
               words long from the training set. We also perform extensive
               manual analysis, finding evidence that GPT-2 uses both
               compositional and analogical generalization mechanisms and
               showing that GPT-2’s novel text is usually well-formed
               morphologically and syntactically but has reasonably frequent
               semantic issues (e.g., being self-contradictory).",
  month     =  jun,
  year      =  2023,
  doi       = "10.1162/tacl\_a\_00567",
  issn      = "2307-387X",
  language  = "en"
}

@ARTICLE{Yao2022-kh,
  title         = "{ReAct}: Synergizing Reasoning and Acting in Language Models",
  author        = "Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and
                   Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan",
  abstract      = "While large language models (LLMs) have demonstrated
                   impressive capabilities across tasks in language
                   understanding and interactive decision making, their
                   abilities for reasoning (e.g. chain-of-thought prompting) and
                   acting (e.g. action plan generation) have primarily been
                   studied as separate topics. In this paper, we explore the use
                   of LLMs to generate both reasoning traces and task-specific
                   actions in an interleaved manner, allowing for greater
                   synergy between the two: reasoning traces help the model
                   induce, track, and update action plans as well as handle
                   exceptions, while actions allow it to interface with external
                   sources, such as knowledge bases or environments, to gather
                   additional information. We apply our approach, named ReAct,
                   to a diverse set of language and decision making tasks and
                   demonstrate its effectiveness over state-of-the-art
                   baselines, as well as improved human interpretability and
                   trustworthiness over methods without reasoning or acting
                   components. Concretely, on question answering (HotpotQA) and
                   fact verification (Fever), ReAct overcomes issues of
                   hallucination and error propagation prevalent in
                   chain-of-thought reasoning by interacting with a simple
                   Wikipedia API, and generates human-like task-solving
                   trajectories that are more interpretable than baselines
                   without reasoning traces. On two interactive decision making
                   benchmarks (ALFWorld and WebShop), ReAct outperforms
                   imitation and reinforcement learning methods by an absolute
                   success rate of 34\% and 10\% respectively, while being
                   prompted with only one or two in-context examples. Project
                   site with code: https://react-lm.github.io",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2210.03629"
}
